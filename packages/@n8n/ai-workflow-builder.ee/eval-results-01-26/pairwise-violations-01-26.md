# Pairwise Evaluation Violations (Jan 26, 2026)

**Evaluation Type:** Pairwise
**Examples:** 71
**Pass Rate:** 85.9% (61 passed, 9 failed, 1 error)
**Average Score:** 82.13%

---

### example-001-7255af79
**Status:** pass | **Score:** 79.8%

**LLM-Judge Violations:**
- [critical] Schedule trigger configured incorrectly - uses 6-hour intervals starting from an arbitrary time instead of three specific times (morning, afternoon, night) as explicitly requested. User requested 'three different times daily' with specific distribution (morning/afternoon/night), but workflow triggers every 6 hours which doesn't guarantee the requested time slots.; [minor] Split In Batches node processes array incorrectly - the 'videos' array is stored as a JSON string in arrayValue, but Split In Batches expects to receive multiple items or needs proper array parsing. The workflow would likely fail to iterate over the 3 video topics as intended.
- [major] Batch Complete node (output 0 of Process 3 Videos) is a dead-end branch with no outgoing connections. The completion message is created but never used, logged, or integrated back into the workflow.; [major] Loop-back connection from Upload to YouTube to Process 3 Videos creates potential for unclear termination logic. While Split In Batches supports loops, the workflow lacks explicit handling to ensure the batch completes cleanly after 3 videos without unintended re-execution.; [minor] Batch Complete branch adds unnecessary complexity without functional value. The node processes data but has no downstream consumer, making it redundant in the current workflow design.
- [critical] In 'Define Video Topics' node: The arrayValue field contains a JSON string instead of a proper expression. Should use `={{ [...] }}` format for dynamic arrays, not a static JSON string.; [major] In 'Batch Complete' node: Expression `={{ "Successfully uploaded 3 videos for " + $json.timeSlot + " time slot" }}` references $json.timeSlot, but this field comes from 'Define Video Topics' node and may not be available after the loop. Should use `={{ $('Define Video Topics').item.json.timeSlot }}`; [minor] In 'Upload to YouTube' node: Expression `={{ $json.topic + " - " + $json.timeSlot + " Short Story" }}` references $json.timeSlot which may not exist in the current item context after video generation. The topic field should be preserved through the workflow or referenced from the original node.
- [major] Schedule Trigger configured for every 6 hours (4 times daily) instead of 3 specific times (morning, afternoon, night) as explicitly requested by user
- [major] Inefficient scheduling mechanism: Uses a single 6-hour interval trigger instead of three specific time-based triggers (morning, afternoon, night). This creates imprecise timing and doesn't match the requirement for specific scheduled times.; [minor] Hardcoded video topics in Set node: All three topics are defined statically in the workflow, which means the same 3 stories repeat daily. This could be optimized with dynamic topic generation or a topic database.; [minor] Unnecessary 'Batch Complete' Set node: This node only creates a completion message but doesn't contribute to the workflow's core functionality. It could be removed or consolidated with error handling.
- [critical] Data structure mismatch in Split In Batches node: 'Define Video Topics' outputs a single item with an array in the 'videos' field, but 'Process 3 Videos' expects multiple items to batch. The arrayValue is stored as a string, not parsed as JSON, causing the batching logic to fail.; [critical] Missing data extraction from videos array: The 'Story Generator Agent' node references $json.topic and $json.duration, but these fields don't exist at the item level after Split In Batches. The node receives the entire item structure with the unparsed 'videos' string, not individual video objects.; [major] Type conversion error: The 'videos' field is set as a string (arrayValue with JSON string) rather than being properly parsed into an array structure. This prevents proper iteration and field access in downstream nodes.; [major] Lost context data in loop: The 'timeSlot' field from 'Define Video Topics' is not properly carried through the Split In Batches loop to be available for the YouTube upload title and completion message.; [major] Incorrect field reference in YouTube upload: The node references $json.topic and $json.timeSlot which are not available in the data structure at that point. The Sora node outputs binary video data and the 'output' field, but not the original topic metadata.
- [minor] Node 'Batch Complete' has a generic purpose - could be more descriptive about what happens after batch completion (e.g., 'Log Batch Upload Summary'); [minor] The workflow organization could benefit from clearer separation between story generation and video processing stages - related nodes are somewhat scattered vertically; [minor] The 'OpenAI GPT Model' node is positioned away from its connected 'Story Generator Agent', making the AI connection less obvious in the visual layout; [minor] Schedule trigger name 'Daily Schedule (3x)' could be more specific about the actual timing (e.g., 'Morning/Afternoon/Night Schedule Trigger')
- [major] Schedule Trigger Configuration Issue: The user explicitly requested 3 different times daily (morning, afternoon, night) to distribute 9 videos (3 per time slot), but the workflow uses a single Schedule Trigger with 6-hour intervals. This creates 4 triggers per day (midnight, 6am, noon, 6pm) instead of the requested 3 specific times. Best practice for scheduling requires matching the trigger configuration to the user's specific timing requirements. This violates the core requirement and will not execute as the user intended.; [minor] Static Topic Definition: The 'Define Video Topics' Set node contains hardcoded topics that will repeat for every execution. Best practice for content generation workflows is to ensure variety and freshness in generated content. For a daily automated workflow creating 9 videos per day, using the same 3 topics repeatedly will result in duplicate content. The workflow should either generate dynamic topics or rotate through a larger topic pool to maintain content quality.; [minor] Missing Error Handling for Long-Running Operations: The best practices documentation specifically warns about async processing for video generation: 'For long-running content generation tasks (especially video), implement proper wait/polling mechanisms.' While a Wait node is mentioned as recommended for video processing, the workflow lacks error handling around the Sora video generation which can fail or timeout. Given this is an automated daily workflow, failures could break the entire batch without recovery.

**Programmatic Violations:**
- None

### example-002-8fc9092d
**Status:** fail | **Score:** 62.1%

**LLM-Judge Violations:**
- [critical] Schedule Trigger does not connect to the RSS Feed nodes. The trigger connects directly to the Merge node, but the RSS Feed nodes have no incoming connections, meaning they will never execute. The workflow structure is fundamentally broken - the RSS nodes need to be triggered by the Schedule Trigger.
- [critical] Schedule Trigger 'Daily at 8 AM' connects directly to 'Combine News Sources' Merge node at index 0, while 'AI News Source 1' also connects to index 0, creating a conflict. The three RSS Feed nodes (AI News Source 1, 2, 3) have no incoming connections and are orphaned from the execution flow. The Schedule Trigger should fan out to trigger all three RSS Feed nodes first, which then feed into the Merge node.; [critical] Conversation Memory node is missing required ai_memory connection to 'Telegram Chat Trigger'. The user requirements explicitly state the memory must connect to BOTH the Telegram Trigger and the AI Agent to share conversation context. Currently it only connects to the AI Agent.
- [critical] In 'Summarize Top 5 Stories' node: Using `{{ $json }}` to pass entire article data is incorrect. $json refers to a single item's JSON, not all merged articles. Should use proper data access like `{{ JSON.stringify($input.all()) }}` or iterate through items properly.
- [major] Schedule Trigger node 'Daily at 8 AM' is missing the connection trigger from the three RSS Feed nodes. The Schedule Trigger should trigger the RSS Feed nodes, but currently the RSS Feed nodes have no incoming connections to activate them.
- [minor] The Schedule Trigger connects to the Merge node but doesn't trigger the RSS Feed nodes - the RSS nodes appear disconnected from the trigger, which would prevent the workflow from executing properly; [minor] Two separate OpenAI GPT models are used (one for summarization, one for chat) when a single credential/model configuration could potentially be reused, though the separation is reasonable for different contexts; [minor] The image generation uses Google Gemini with a static prompt that doesn't incorporate the actual news content, missing an opportunity to generate contextually relevant images
- [critical] Schedule Trigger connects directly to Merge node without triggering RSS Feed nodes - the three RSS Feed nodes (AI News Source 1, 2, 3) are orphaned and will never execute. The Schedule Trigger should connect to each RSS Feed node, not directly to the Merge node.; [major] Incorrect data transformation in 'Summarize Top 5 Stories' node - using {{ $json }} to pass all merged articles will not properly format the data for LLM processing. Should iterate through items or use proper expression to extract relevant fields (title, link, content) from RSS feed structure.; [major] Image generation node output not properly integrated with Telegram message - the 'Send News to Telegram' node only sends text from the summarization, but doesn't include the generated image. The binary data from 'newsImage' property needs to be sent as a photo attachment.; [major] Chat agent has no access to the generated news content - the AI News Chat Agent is isolated from the daily news generation flow. There's no mechanism (vector store, tool, or shared context) for the chat agent to access the news summaries that were generated, making it impossible to answer questions about the actual news content.; [minor] Missing data validation between nodes - no error handling or checks to ensure RSS feeds return data before merging, which could result in empty or malformed data being passed to the LLM.
- [major] CRITICAL: Chatbot disconnected from scheduled news data. The user explicitly requested that 'This same Window Buffer Memory node must be connected to both the Telegram Trigger node and the AI Agent' and that 'users can query about the news content that was generated.' However, the Conversation Memory node is ONLY connected to the AI News Chat Agent - it has no connection to the scheduled news gathering workflow. The chatbot has no access to the daily news summaries generated by the scheduled workflow. Per chatbot best practices: 'When a workflow has multiple triggers (e.g., scheduled data collection + chatbot interaction), the chatbot MUST have access to the data generated by the workflow. Connect the chatbot through shared memory, vector stores, data storage, or direct data flow connections.' This violates the explicit user requirement and best practice guidance.; [minor] Schedule Trigger not connected to RSS Feed nodes. The workflow shows 'Daily at 8 AM' connecting directly to 'Combine News Sources' (Merge node), but the three RSS Feed nodes (AI News Source 1, 2, 3) have no incoming connections from the trigger. This means the RSS feeds won't be triggered by the schedule. The RSS nodes should be connected from the Schedule Trigger to ensure they execute when the workflow runs at 8 AM.; [minor] Missing n8n attribution disabling for Telegram nodes. Per chatbot best practices: 'n8n chatbots often attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for nodes that support it.' The two Telegram nodes ('Send News to Telegram' and 'Send Chat Response') do not have this setting configured to false, which may result in unwanted attribution in messages.

**Programmatic Violations:**
- [critical] Node AI News Source 1 (n8n-nodes-base.rssFeedRead) is missing required input of type main; [critical] Node AI News Source 2 (n8n-nodes-base.rssFeedRead) is missing required input of type main; [critical] Node AI News Source 3 (n8n-nodes-base.rssFeedRead) is missing required input of type main
- [major] Node 'AI News Source 1' is not connected to any input. It will not receive data.; [major] Node 'AI News Source 2' is not connected to any input. It will not receive data.; [major] Node 'AI News Source 3' is not connected to any input. It will not receive data.; [major] Merge node "Combine News Sources" has only 0 input connection(s). Merge nodes require at least 2 inputs.

### example-003-79d6182e
**Status:** pass | **Score:** 87.3%

**LLM-Judge Violations:**
- [major] Gmail trigger data field mapping issue: AI Agent references '$json.text' but Gmail trigger with 'simple: false' outputs structured data with fields like 'snippet', 'payload.body.data', 'payload.parts', etc. The 'text' field may not exist or contain the full email body content needed for delivery information extraction.; [minor] Missing explicit data transformation to extract and format email body content from Gmail's complex payload structure before passing to AI Agent. With 'simple: false', the email body is base64 encoded in nested payload structures requiring decoding.; [minor] No validation or fallback handling if email body extraction fails or returns empty content, which could lead to AI Agent receiving incomplete data for delivery information extraction.
- [minor] Gmail trigger uses 'simple: false' which is correct per user requirements, but the workflow doesn't explicitly handle the email body extraction. The AI Agent references '{{ $json.text }}' which may not contain the full email body with attachments available. Best practice for data extraction suggests ensuring binary data (attachments) is properly referenced and preserved through the workflow pipeline.

**Programmatic Violations:**
- None

### example-004-c9bac036
**Status:** pass | **Score:** 84.5%

**LLM-Judge Violations:**
- [major] Missing SerpAPI or web search tool connection to AI Agent. The agent is configured to research investors but has no capability to actually search the web or access external data sources. It only has a language model connected, which cannot retrieve real-time investor information.
- [major] AI Investor Researcher agent lacks research tools. The agent is tasked with researching investors and finding specific information (addresses, emails, capital amounts, crypto investment preferences) but has no tools connected (no search tool, web scraping, or API access). Without tools, the agent can only generate responses from training data, leading to hallucinated or outdated information rather than actual research. For a research workflow requiring accurate, current data, the agent needs at minimum a web search tool or similar capability.
- [major] In 'AI Investor Researcher' node: The expression uses `$json.existingOrganizations` without proper node reference. Should be `$('Extract Existing Organizations').item.json.existingOrganizations` to explicitly reference the previous node's output.
- [minor] Google Sheets nodes use placeholder values for documentId and sheetName, but this is expected since user didn't provide specific Google Sheet ID or sheet name. These are valid configuration points for user to fill in.; [minor] AI Agent node has maxIterations set to 15, which may be higher than necessary for a straightforward research task. A lower value (5-10) would be more efficient, though this won't break functionality.; [minor] The prompt in AI Investor Researcher could be more robust in handling edge cases where existingOrganizations might be empty on first run, though the current implementation will still work.
- [minor] The 'No New Investors Found' NoOp node serves no functional purpose and could be replaced with workflow completion or error handling; [minor] The 'Extract Existing Organizations' Code node could potentially be combined with the AI prompt construction to reduce one node; [minor] The 'Has New Investors?' IF node could be avoided by using Google Sheets' built-in handling of empty data or by structuring the Parse node to always return valid data
- [major] Field name mismatch between user requirements and workflow implementation: User requested 'Name' for organization but workflow uses same 'Name' field without distinguishing between organization name and representative name, creating potential confusion; [major] Inconsistent field naming in AI prompt vs. validation: AI prompt requests 'OpenToCrypto' but user specified 'open to Crypto?' - the validation code correctly maps this but the column name transformation may not match Google Sheets headers exactly; [minor] JSON parsing logic uses regex extraction which could fail on valid JSON with different formatting, potentially causing data loss if AI returns valid JSON in unexpected format; [minor] Validation filter removes investors missing any of Name, Representative, Type, or Contact but doesn't log or report which investors were filtered out, causing silent data loss without user awareness
- [major] AI Agent lacks required tools for research functionality. Best practices state 'AI agents can autonomously gather information from websites' and recommend SerpAPI or Perplexity tools for research workflows. The agent has no search tools connected, making it impossible to actually research investors - it can only generate fictional data based on its training data, not perform real research as requested.; [minor] Missing rate limiting and batching safeguards. Best practices emphasize 'Batch requests and introduce delays to avoid hitting API rate limits' and 'Use Wait nodes and batching options'. While the user requested processing 3 investors at a time, there's no Wait node or rate limiting for the Google Sheets API calls, which could cause 429 errors with repeated executions.; [minor] No error handling for AI response parsing failures. The Code node 'Parse and Validate Results' returns an empty array on JSON parse errors without any notification or fallback mechanism. Best practices recommend 'Handle errors gracefully' to prevent data loss, which is relevant since the user plans to loop this workflow.

**Programmatic Violations:**
- None

### example-005-51825960
**Status:** pass | **Score:** 87.4%

**LLM-Judge Violations:**
- [critical] Google Calendar Tool is implemented as a Code Tool with placeholder/simulated behavior that does not actually interact with Google Calendar API. The tool explicitly returns messages stating 'Please configure Google Calendar API credentials to enable actual calendar operations' and does not perform the core requested functionality of searching or adding events to Google Calendar.
- [major] In 'Delivery Calendar Agent' node: Complex string concatenation expression uses incorrect syntax. Should use template literal format with backticks or proper string concatenation with + operator inside expression brackets. The expression attempts to concatenate strings with + outside of proper JavaScript context.; [minor] In 'Delivery Calendar Agent' node: The expression uses $json.textPlain and $json.textHtml without checking which field actually exists. While this works with || operator, it could be more robust with explicit field checking.; [minor] In 'Google Calendar Tool' node: The jsCode references 'query' variable which should be available in the Code Tool context, but this is not explicitly validated. This is a minor concern as it's the correct variable name for Code Tool input.; [minor] In 'Delivery Calendar Agent' node: The long multi-line string concatenation could be more maintainable using template literals with backticks instead of string concatenation with + operators.
- [minor] Gmail Trigger node uses 'simple' mode which may limit access to full email content. Setting 'simple: false' would provide more complete email data for the AI agent to analyze.; [minor] Tool Code node contains placeholder implementation with simulated behavior rather than actual Google Calendar API integration. While the structure is correct, the jsCode returns mock responses instead of performing real calendar operations.
- [major] Google Calendar Tool uses placeholder code instead of actual Google Calendar node - this creates a non-functional implementation that would require complete replacement in production; [minor] The AI agent is tasked with both extraction and calendar management logic when a simpler approach could use the agent only for extraction, then use native Google Calendar nodes for search/add operations
- [major] Email body data extraction uses fallback pattern ($json.textPlain || $json.textHtml) without proper handling of HTML content. If textPlain is empty string (falsy), it will fall back to textHtml which may contain HTML tags that could confuse the AI agent.; [minor] No explicit data validation or error handling for missing email fields (subject, from, body). If these fields are undefined, the prompt will contain 'undefined' strings which could lead to poor AI responses.
- [major] Google Calendar Tool uses placeholder Code Tool instead of actual Google Calendar node. The toolCode node contains only simulated behavior with comments like 'Note: In a real implementation, you would use Google Calendar API' and 'Please configure Google Calendar API credentials to enable actual calendar operations'. This violates the user's explicit requirement to 'search the Google Calendar' and 'add it to the calendar'. The workflow will not actually interact with Google Calendar as requested.; [major] Missing error handling for Gmail trigger failures. The best practices documentation for data extraction emphasizes proper handling of binary data and data structure management. If the Gmail trigger fails to retrieve email content (textPlain or textHtml), the agent prompt construction will fail. The expression '{{ $json.textPlain || $json.textHtml }}' may return undefined, breaking the core functionality requested by the user.; [minor] AI Agent temperature set to 0.7 when best practices for triage workflows recommend low temperature (0-0.2) for consistency in classification and decision-making tasks. The agent is performing triage (deciding whether to add calendar events) and should use consistent logic rather than creative responses.; [minor] No fallback or default path for handling cases where the AI agent fails to process the email or encounters errors. Best practices for triage workflows emphasize 'CRITICAL: Always include a default/fallback path to catch unclassified items. Never allow data to drop silently.' While this workflow has a single path, there's no error handling if the agent fails.

**Programmatic Violations:**
- None

### example-006-39ad6fe6
**Status:** pass | **Score:** 88.6%

**LLM-Judge Violations:**
- [major] Agent node is missing a required tool/capability to actually crawl/search the web. The agent has a language model and output parser connected, but no web search tool (like SerpAPI, Google Search, or similar) is connected via ai_tool port. Without this, the agent cannot perform the core function of 'crawling the web to find startups'.
- [minor] Unnecessary expression wrapper in 'News Scraping Agent' text parameter - the entire string is static text and doesn't need ={{ }} wrapper
- [major] Agent node missing required web search tool connection - the agent is configured to search the web but has no tool (like SerpAPI, Google Search, or Web Scraper) connected via ai_tool to actually perform web searches
- [major] Redundant filtering: The AI agent is already instructed to 'Focus on finding startups that raised over $1 million' in its prompt, but a separate Filter node is used to filter the same condition. This creates unnecessary duplication of filtering logic.; [minor] The structured output parser could include the filtering logic in its schema validation, eliminating the need for a separate Filter node. The schema could be designed to only accept entries with fundingAmount > 1000000.; [minor] The agent's maxIterations is set to 15, which may be excessive for a simple news scraping task. This could lead to unnecessary API calls and longer execution times. A value of 5-8 would likely be sufficient.
- [critical] Missing data transformation between Agent output and Filter node - the Structured Output Parser returns nested array structure {startups: [{...}]} but Filter node expects flat items with direct access to $json.fundingAmount. The agent outputs a single item with nested startups array, but Filter operates on individual items.; [major] No array flattening/splitting logic - the structured output schema defines 'startups' as an array, but there's no node to split this array into individual items before filtering. Filter node will receive one item with an array property instead of multiple items to filter.
- [major] Missing web search tool for the AI Agent. The user requested a 'news-scraping agent that crawls the web' but the agent has no tool to actually search the web. Best practices documentation recommends SerpAPI (@n8n/n8n-nodes-langchain.toolSerpApi) or Perplexity (n8n-nodes-base.perplexityTool) for giving agents the ability to search for research materials. Without a search tool, the agent cannot fulfill its core purpose of finding startup funding news from the web.; [major] Missing Split Out node before Filter. The Structured Output Parser returns a single item containing an array of startups in the 'startups' field (as shown in the schema example). The Filter node expects to process individual items with 'fundingAmount' at the root level, but will receive a single item with a nested array. Best practices for data_extraction state: 'If retrieving a JSON array, this will return a single item containing that array. If you wish to use a Loop Over Items node then you will need to split out the array into items before looping over it.' A Split Out node is required between the agent and filter to convert the single item with array into multiple items.; [minor] No rate limiting or batching implemented. Best practices for scraping_and_research state: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers. Use Wait nodes and batching options.' While the user didn't explicitly request production-ready features, web scraping workflows that search multiple news sources should implement basic rate limiting to avoid 429 errors and respect target servers.; [minor] maxIterations set to 15 may be excessive without proper safeguards. Best practices warn about resource management in scraping workflows. While the user requested thorough searching, 15 iterations without batching or delays could lead to rate limiting issues or excessive API costs. A more conservative value (5-10) with proper error handling would be more appropriate.; [minor] Data Table node has placeholder value for dataTableId. While this is expected for a template, best practices for data_persistence recommend using ResourceLocator mode 'list' to allow users to select from existing tables. The current configuration will fail on execution without user intervention.

**Programmatic Violations:**
- None

### example-007-32894f57
**Status:** fail | **Score:** 66.4%

**LLM-Judge Violations:**
- [major] The Set nodes create array data incorrectly. They use 'arrayValue' with a JSON string, but the splitInBatches nodes expect individual items. The workflow should use 'manual' mode with multiple value assignments or use a Code node to properly create an array that splitInBatches can iterate over.; [minor] The workflow lacks error handling for API failures. Given the complexity of the pipeline (ChatGPT API, Sora API, video download, YouTube upload), there's no mechanism to handle failures gracefully, which could cause the entire batch to fail if one video generation fails.
- [critical] Invalid JSON syntax in 'Generate Story Script' nodes: Using string concatenation with + operator inside JSON body without proper escaping. The expression '={"model": "gpt-4", "messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "Write a compelling 60-second short story about " + $json.theme + ". Make it visual..."}], "max_tokens": 500}' will fail because you cannot concatenate strings with + inside a JSON string literal. Should use template literals or proper expression syntax.; [major] Missing = prefix in 'Create 3 Morning/Afternoon/Night Story Prompts' nodes: The arrayValue parameter contains JSON string '[{"id": 1, "theme": "adventure"}, ...]' without the = prefix, which should be '=[{"id": 1, "theme": "adventure"}, ...]' to be treated as an expression that can be parsed.
- [minor] Set nodes use 'arrayValue' with JSON string instead of proper array structure. While functional, the JSON should be parsed or the array should be split into separate items for cleaner processing.; [minor] Wait nodes use fixed 30-second intervals which may be insufficient for Sora video generation. Video generation typically takes longer and may require polling or webhook-based completion detection.; [minor] HTTP Request nodes for Sora API use hypothetical endpoint 'https://api.openai.com/v1/video/generations' which may not match actual Sora API structure when available. However, this is acceptable as a placeholder for future API.
- [critical] Entire workflow logic is duplicated 3 times (morning/afternoon/night) - identical processing chains with only different trigger times and story themes. This creates 24 nodes where 8 would suffice.; [critical] Three separate schedule triggers (8 AM, 2 PM, 8 PM) when a single trigger with a loop could handle all 9 videos with proper timing logic; [major] Story generation, video generation, download, and upload logic is copy-pasted 3 times with identical parameters and structure - clear violation of DRY principle; [major] Three separate Set nodes creating story prompts when one node could create all 9 stories with time-based filtering or a single data structure; [minor] Wait nodes with fixed 30-second delays may be inefficient - should poll for completion status or use webhooks if available
- [critical] Array data loss in Set nodes: The 'stories' array is stored as a JSON string in arrayValue field, but splitInBatches expects individual items. The Set node outputs a single item with a 'stories' field containing a string, not 3 separate items. This will cause splitInBatches to process only 1 batch instead of 3.; [critical] Missing data extraction from Set node: After the Set node creates the stories array, there's no Code/Item Lists node to parse the JSON string and split it into individual items. The splitInBatches node receives 1 item with a 'stories' string field instead of 3 items with 'id' and 'theme' fields.; [major] Field reference error in Generate Story Script nodes: The expression '$json.theme' assumes the theme field exists at the root level, but after the Set node, the data structure would be $json.stories (as a string), not $json.theme. This will result in undefined values being passed to the API.; [major] Field reference error in Upload to YouTube nodes: The expression '$json.id' and '$json.theme' reference fields that won't exist in the data at that point. After video download, the data structure changes and these original fields from the Set node are lost in the transformation chain.; [major] Incorrect API response structure assumption: The 'Generate Video with Sora' nodes reference '$json.data[0].url' but the actual Sora API response structure is unknown/hypothetical. This could cause the Download Video nodes to fail if the response structure differs.
- [critical] Massive code duplication - identical processing logic repeated three times for morning/afternoon/night schedules instead of using a single reusable flow; [major] Poor modularity - the workflow has 24 nodes doing essentially the same operations three times, making it extremely difficult to maintain and update; [major] No error handling or logging nodes - if any step fails in the video generation or upload process, there's no way to track or recover; [minor] Hardcoded story themes in Set nodes - themes should be dynamically generated or stored in a database for easier content variation; [minor] No validation or status checking after video generation - workflow assumes Sora API always succeeds after 30 second wait
- [major] Using HTTP Request nodes for OpenAI API calls instead of the built-in OpenAI node (@n8n/n8n-nodes-langchain.openAi). The best practices explicitly state: 'Always prefer built-in n8n nodes over HTTP Request nodes when a dedicated node exists for the service.' The OpenAI node provides pre-configured authentication, optimized data structures, better error handling, and supports DALL-E, GPT, TTS, and SORA video generation. This is particularly critical since the user specifically mentioned 'ChatGPT Sora' in their request.; [major] Missing error handling for async video generation. The workflow uses a fixed 30-second wait for video generation, but the best practices warn: 'For long-running content generation tasks (especially video), implement proper wait/polling mechanisms. Don't assume instant completion - many AI services process requests asynchronously.' Video generation can take much longer than 30 seconds, and without proper polling or error handling, the workflow will fail when videos aren't ready, preventing the core functionality from working.; [minor] Massive code duplication across three identical workflow branches (morning, afternoon, night). While this technically works, it violates workflow design best practices for modularity and maintainability. A better approach would use a single scheduled trigger with parameters or a loop to handle all three time periods, making the workflow easier to troubleshoot and modify.; [minor] No data persistence or logging of generated videos. While not explicitly requested by the user, the best practices recommend storing workflow results for audit trails and tracking. Given this is an automated content generation workflow posting to YouTube daily, having a record of what was generated and posted would be valuable for tracking and preventing duplicates.

**Programmatic Violations:**
- None

### example-008-3513d2f9
**Status:** pass | **Score:** 87.1%

**LLM-Judge Violations:**
- [minor] User requested 'SerpAPI or Perplexity' as search options, but workflow only implements SerpAPI without providing Perplexity as an alternative. While SerpAPI is functional and meets the core requirement, the user explicitly mentioned both options.; [minor] Google Sheets column mapping uses 'Representative' as the column name, but user specified the columns should be 'Name, Address, Name, Type, Country, Open to Crypto, Contact (email), and Capital' - which appears to have 'Name' twice (likely meaning firm name and representative name). The workflow interprets this reasonably but deviates slightly from the literal specification.; [minor] The workflow creates hardcoded search queries rather than accepting dynamic input for investor types or criteria. While this works for the immediate use case, the user mentioned 'I plan to loop this process later' which suggests they may want to vary search parameters across iterations.
- [minor] In 'Search Investors via SerpAPI' node, the URL uses encodeURIComponent() which is valid JavaScript but n8n provides built-in URL encoding. However, this is functionally correct and will work.
- [minor] Google Sheets node uses 'autoMapInputData' for dataMode but also defines explicit column mappings in 'columns' parameter. While this works, it's redundant - should use 'defineBelow' mode consistently or rely solely on autoMap.; [minor] Google Sheets 'columnToMatchOn' is set to 'Name' for appendOrUpdate operation, but user didn't explicitly request update/upsert behavior - they only mentioned storing data. This adds unnecessary complexity for a simple append operation.
- [minor] Google Sheets node uses both 'autoMapInputData' and manual column mapping, which is redundant. Should use one approach consistently.; [minor] The Code node creates hardcoded search queries that could be more flexible. Consider parameterizing search terms for better reusability.; [minor] Information Extractor batching configuration (batchSize: 3) may be unnecessary given the workflow already processes 3 queries. This adds complexity without clear benefit.
- [major] Field mapping inconsistency: User requested 'Name' column twice (for investor/firm name and representative name), but workflow maps 'Representative' field to a 'Representative' column instead of the second 'Name' column. This creates a mismatch with the user's specified schema.; [minor] Data structure assumption risk: The workflow passes the entire SerpAPI response JSON to the Information Extractor without pre-processing. SerpAPI returns complex nested structures (organic_results, knowledge_graph, etc.), and stringifying the entire response may include irrelevant data that could confuse the AI extraction process.; [minor] Potential data loss from batching configuration: The Information Extractor has batching set to 3 items with 1 second delay, but this processes items in batches rather than ensuring 3 distinct investors are found. If SerpAPI returns 10 results per query (30 total items), the batching may not align with the user's intent to find '3 investors at a time'.
- [minor] Missing rate limiting protection for SerpAPI requests. The workflow makes 3 consecutive API calls without delays or batching configuration. Best practice states: 'Batch requests and introduce delays to avoid hitting API rate limits' and 'Use Wait nodes and batching options in HTTP Request nodes.' While the user didn't explicitly request production-grade rate limiting, this is a documented pitfall for scraping workflows that could cause 429 errors.; [minor] Information Extractor node has batching configured (batchSize: 3, delayBetweenBatches: 1000ms) but this is applied to the wrong operation. The batching should be on the SerpAPI HTTP requests to prevent rate limiting, not on the AI extraction which processes results after API calls are complete. This misplacement doesn't align with the best practice of batching API requests to external services.; [minor] Google Sheets node uses 'appendOrUpdate' operation with 'columnToMatchOn: Name' which could cause issues if investor names are similar but not identical (e.g., 'Sequoia Capital' vs 'Sequoia Capital LLC'). Best practice for data persistence states: 'Use unique identifiers or upsert operations when appropriate' and warns about 'Not Handling Duplicates' as a common pitfall. A more robust approach would validate exact matches or use compound keys.; [minor] Missing error handling for HTTP Request node failures. Best practice warns: 'Refresh expired tokens, verify API keys, and ensure correct permissions to avoid authentication failures' and 'Double-check URL formatting, query parameters, and ensure all required fields are present to avoid bad request errors.' While the user didn't request error handling, this is a documented pitfall that could cause workflow failures with invalid API responses.

**Programmatic Violations:**
- [critical] Node type n8n-nodes-base.removeDuplicates not found for node Remove Duplicate Organizations

### example-009-bd2ffceb
**Status:** pass | **Score:** 88.5%

**LLM-Judge Violations:**
- [major] Memory connection is reversed: Chat Memory connects TO Chat Interface via ai_memory, but the correct pattern is Chat Interface should connect TO Chat Memory. The user explicitly requested 'connect it to the Chat Trigger via the ai_memory connection point' which means the Chat Trigger should have an outgoing ai_memory connection to the memory node.
- [minor] OpenAI Chat Model node uses 'gpt-5-mini' which is not a valid OpenAI model. The user requested 'gpt-5-mini' but this model doesn't exist in OpenAI's lineup. Valid models would be 'gpt-4o-mini', 'gpt-4', 'gpt-3.5-turbo', etc. However, this appears to be following the user's explicit (though incorrect) specification.
- [minor] Session ID extraction uses $json.sessionId without validation - if sessionId is missing or malformed, memory functionality could fail silently; [minor] Chat input text mapping uses $json.chatInput without fallback handling - should verify the field exists in the trigger output
- [minor] Chat Trigger node does not have 'Append n8n Attribution' disabled. Best practices state: 'n8n chatbots often attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for nodes that support it, add this setting and set it to false.' While this doesn't break functionality, it affects the professional appearance of the chat interface.

**Programmatic Violations:**
- [critical] Node Chat Interface (@n8n/n8n-nodes-langchain.chatTrigger) received unsupported connection type ai_memory; [critical] Node type n8n-nodes-base.googleSheetsTool not found for node Google Sheets Data Retriever

### example-010-4dec43c4
**Status:** fail | **Score:** 61.0%

**LLM-Judge Violations:**
- [critical] Schedule trigger does not connect to the three news fetching HTTP Request nodes. The workflow has 'Daily at 8 AM' connecting only to 'Combine All News', but the three HTTP Request nodes (Fetch AI News - Source 1, 2, 3) have no incoming connections, meaning they will never execute when the schedule triggers.; [major] Chat Memory is configured to store only 20 conversation turns (contextWindowLength: 20), which equals 40 messages total (20 user + 20 assistant). However, the user explicitly requested 'at least 40 last messages should be stored', which should be interpreted as 40 messages from each side (80 total messages, requiring contextWindowLength: 40).; [major] The AI News Summarizer agent node is missing the required language model connection. Agent nodes require an LLM to be connected via the ai_languageModel port, but the OpenAI GPT-4 model is only connected to the Chat Agent, not to the AI News Summarizer.; [major] The generated image is not actually sent with the Telegram message. The 'Generate AI News Image' node creates a binary image output, but the 'Send to Telegram' node only sends text message without including the image attachment. The user explicitly requested to 'send everything as a structured Telegram message' including the generated image.
- [critical] Schedule Trigger does not connect to HTTP Request nodes. 'Daily at 8 AM' connects to 'Combine All News' but the three 'Fetch AI News' HTTP Request nodes are never triggered. They must receive the trigger signal via main connections from the Schedule Trigger to execute.; [critical] AI News Summarizer agent missing required language model capability. The 'AI News Summarizer' AI Agent node has no ai_languageModel connection. 'OpenAI GPT-4' only connects to 'Chat Agent', leaving the summarizer unable to function.; [major] Memory not connected to AI News Summarizer. User requested 40 messages stored for chatting about news, but 'Chat Memory (40 messages)' only connects to 'Chat Agent' via ai_memory, not to 'AI News Summarizer'. The summarizer cannot reference conversation history.
- [major] In 'AI News Summarizer' node: Using `{{ $json }}` to pass entire article data is problematic. This references only the current item's json, but after 'Normalize & Rank Articles' outputs 5 items, each item contains a single article. The prompt should iterate over all items or use `$input.all()` to access all 5 articles.; [minor] In 'Format Telegram Message' node: The code attempts to access articles with `$input.all().slice(0, 5)` but the actual article data comes from a previous node ('Normalize & Rank Articles'), not from the AI News Summarizer output. The logic assumes articles are available in the current input, which may not match the actual data flow.
- [major] Chat Memory (40 messages) node has contextWindowLength set to 20, but user explicitly requested 'at least 40 last messages should be stored'. This should be 40 to match the requirement.; [major] AI News Summarizer agent node is missing required ai_languageModel connection configuration. The node has no LLM model connected to perform the summarization task.
- [major] The 'Daily at 8 AM' trigger node doesn't connect to the three HTTP Request nodes (Source 1, 2, 3). These nodes appear disconnected from the trigger, meaning they won't execute when the schedule fires. This is a critical path inefficiency.; [minor] The 'AI News Summarizer' agent node is used but doesn't have its LLM sub-node connected. The OpenAI GPT-4 node only connects to the Chat Agent, not the summarizer, which may cause execution issues.; [minor] The image generation uses Google Gemini node but the generated image isn't actually attached to the Telegram message. The Format Telegram Message node doesn't reference the binary data, making the image generation step redundant.
- [critical] Missing LLM connection to AI News Summarizer agent - the agent node has no language model connected, which will cause it to fail completely; [critical] Schedule trigger not connected to news fetch nodes - Daily at 8 AM connects to Combine All News but the three HTTP fetch nodes have no incoming connections, so they will never execute; [major] Format Telegram Message node attempts to access both summary and articles data incorrectly - uses $input.first() for summary but the AI News Summarizer output structure is not properly accessed, and tries to slice articles from all inputs when articles are not in the data flow at this point; [major] Chat Memory configured for 20 messages (contextWindowLength: 20) but requirement specifies 40 messages - this stores only 20 messages (10 exchanges) instead of 40; [major] Generated image from Google Gemini node is not passed to Telegram - the image binary data is generated but the Send to Telegram node only sends text message without the image attachment; [minor] Normalize & Rank Articles passes entire article objects to AI News Summarizer using {{ $json }} which will serialize all 5 articles as a single string rather than structured data the LLM can properly parse
- [minor] Node 'Combine All News' could be more descriptive - consider 'Merge News from All Sources' to better indicate the merging operation; [minor] The 'Chat Memory (40 messages)' node is configured for 20 messages (contextWindowLength: 20) but named for 40, creating confusion. The parameter should be 40 to match requirements.; [minor] Missing connection from 'OpenAI GPT-4' to 'AI News Summarizer' - the LLM model should be connected to the agent that uses it for summarization
- [major] CRITICAL: Chatbot is completely disconnected from scheduled news workflow. The user explicitly requested 'I should be able to chat about the news' - but the Chat Agent has no access to the news data collected by the scheduled workflow. The two workflows share NO connection through memory, vector stores, or data storage. The Chat Memory node is only connected to the Chat Agent, not to the AI News Summarizer agent. Best practice states: 'When a workflow has multiple triggers (e.g., scheduled data collection + chatbot interaction), the chatbot MUST have access to the data generated by the workflow. Connect the chatbot through shared memory, vector stores, data storage, or direct data flow connections.'; [major] Memory configuration violates user requirement. User requested 'at least 40 last messages should be stored' but Chat Memory is configured with contextWindowLength: 20, which stores only 20 messages (not 40). This directly fails to meet the stated requirement.; [minor] Missing rate limiting/batching for multiple HTTP requests. Best practices state: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers.' Three simultaneous HTTP requests to news APIs without any Wait nodes or batching could trigger rate limits (429 errors). While not critical for basic functionality, this is a documented pitfall.; [minor] AI News Summarizer agent missing connection to Chat Model. The agent node has no ai_languageModel connection to the OpenAI GPT-4 node, meaning it cannot function properly. The OpenAI GPT-4 node is only connected to the Chat Agent, not the AI News Summarizer.

**Programmatic Violations:**
- [critical] Node Fetch AI News - Source 1 (n8n-nodes-base.httpRequest) is missing required input of type main; [critical] Node Fetch AI News - Source 2 (n8n-nodes-base.httpRequest) is missing required input of type main; [critical] Node Fetch AI News - Source 3 (n8n-nodes-base.httpRequest) is missing required input of type main; [critical] Node AI News Summarizer (@n8n/n8n-nodes-langchain.agent) is missing required input of type ai_languageModel
- [major] Agent node "AI News Summarizer" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context
- [major] Node 'Fetch AI News - Source 1' is not connected to any input. It will not receive data.; [major] Node 'Fetch AI News - Source 2' is not connected to any input. It will not receive data.; [major] Node 'Fetch AI News - Source 3' is not connected to any input. It will not receive data.; [major] Merge node "Combine All News" has only 0 input connection(s). Merge nodes require at least 2 inputs.; [minor] Agent node "AI News Summarizer" has no expression in its prompt. When working with a chat trigger node, use { promptType: 'auto', text: '={{ $json.chatInput }}' }. Or use { promptType: 'define', text: '={{ ... }}' } if any other trigger

### example-011-5914b846
**Status:** pass | **Score:** 79.8%

**LLM-Judge Violations:**
- [major] The workflow has a critical structural flaw in the loop logic. The 'Post to Twitter' node connects back to 'Process Each Article' (Split in Batches), but the 'Collect All Tweets' aggregate node is positioned to receive data from the first output of Split in Batches. This creates a race condition where the aggregate may complete before all tweets are posted, resulting in an incomplete summary report being sent to the admin.; [minor] The 'Search Drone News' tool uses hardcoded/simulated news data instead of actually searching for real drone news articles. While the user requested the workflow to 'search & collect latest 5 drone news articles', the implementation returns static placeholder data rather than performing an actual search via News API, RSS feeds, or web scraping.
- [critical] Data flow break in batch processing loop: Collect All Tweets aggregates from Process Each Article's completion output (output 0), which contains original article data, not the generated tweet content from the Create Tweet  Post to Twitter loop. The summary report will receive malformed data as it expects tweet outputs ($json.tweets.map(t => t.output)) but will get article objects instead.
- [major] In 'Parse Articles' node: regex pattern `[.*]` is incorrect - should be `\[.*\]` to match JSON arrays. The unescaped brackets create an invalid character class.; [minor] In 'Send Summary Report' node: accessing `$json.tweets.length` assumes tweets is always an array, but the aggregate node structure may not guarantee this field exists or is an array.
- [minor] Gmail 'Send Summary Report' node has placeholder value for 'sendTo' parameter. While this is expected when user doesn't provide admin email, it should be noted for runtime configuration.
- [major] Redundant AI processing: Uses AI Agent with search tool to collect articles, then uses another AI Chain to format tweets. The AI Agent could potentially handle both collection and formatting in one pass.; [minor] Parse Articles node includes fallback logic that duplicates article creation, which could be simplified or moved to error handling.; [minor] Two separate OpenAI model nodes (OpenAI Model and Summary Model) with similar configurations could potentially be consolidated into one reusable model node.
- [critical] Parse Articles node has incorrect regex pattern - uses '[.*]' instead of '\[.*\]' which will fail to match JSON arrays, causing the fallback to always execute; [major] Email summary report references 'tweets' array but attempts to access 't.output' field - however, the Aggregate node collects items with 'output' field from Create Tweet, but Post to Twitter node doesn't preserve this structure in its output; [minor] Parse Articles fallback creates articles without proper error handling context - silently fails without logging what went wrong with the AI response parsing; [minor] No validation that exactly 5 articles are returned before processing - could result in fewer items being processed without notification
- [major] Using Code Tool instead of recommended scraping/research nodes: The workflow uses a custom Code Tool ('Search Drone News') with hardcoded sample data instead of using recommended nodes like SerpAPI, Perplexity, or HTTP Request to actually fetch real drone news. Best practices explicitly state to use SerpAPI for research materials or HTTP Request for fetching web pages/API data. This prevents the workflow from fulfilling its core purpose of collecting 'latest 5 drone news articles'.; [minor] Missing rate limiting protection for Twitter API: The workflow posts 5 tweets in rapid succession without any Wait nodes or batching delays between posts. Best practices for scraping/research workflows emphasize 'Batch requests and introduce delays to avoid hitting API rate limits' and 'Use Wait nodes and batching options'. While Twitter's rate limits may accommodate 5 posts, this is a documented pitfall that should be addressed.

**Programmatic Violations:**
- [major] Agent node "AI News Collector" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-012-6c6f9262
**Status:** pass | **Score:** 74.6%

**LLM-Judge Violations:**
- [major] Critical logic error in IF node connections: The 'true' branch has a self-loop connection back to 'Is About Portugal?' node itself (index 0 appears twice in the true branch array). This creates an infinite loop that would prevent the workflow from functioning correctly.
- [critical] IF node 'Is About Portugal?' has a self-loop connection in its true branch (output 0), creating an infinite execution cycle. The node connects back to itself alongside the connection to 'Translate to Portuguese', which will cause the workflow to deadlock or loop infinitely when the condition is true.
- [major] In 'Check if About Portugal' node: Expression uses string concatenation with + operator instead of proper n8n template literal syntax. Should be: `="Analyze this news message and determine if it is about Portugal (the country). Reply with only YES or NO.
- 
- News: {{ $json.message.text }}"`
- [minor] Telegram Trigger node has empty credentials object - while credentials are configured at runtime, the node is properly configured to listen for both 'message' and 'channel_post' updates as needed for receiving news from Telegram channels; [minor] Send to My Channel node uses a placeholder for chatId which is expected since user didn't provide their specific channel ID - this is a valid configuration point for user to fill in; [minor] Is About Portugal? node has a self-referencing connection in the 'true' branch output which appears to be a workflow design issue, but from a parameter configuration perspective, the IF conditions are correctly configured with proper leftValue, operator, and rightValue
- [critical] Circular reference in 'Is About Portugal?' node - output 0 connects back to itself, creating an infinite loop that would cause workflow failure; [major] Data field extraction assumes 'message.text' exists but doesn't handle 'channel_post' updates which have different structure (channel_post.text instead of message.text); [minor] No validation or error handling for empty or missing text content from Telegram messages before passing to AI nodes
- [major] Using Chat node (@n8n/n8n-nodes-langchain.chat) instead of AI Agent for AI operations. Best practices state: 'Unless user asks for a node by name, always use the AI Agent node over provider-specific nodes or use-case-specific AI nodes (like Message a model) for chatbot workflows.' While this is a triage workflow, the same principle applies - AI Agent provides better orchestration and is the recommended approach for AI-powered classification tasks.; [major] Missing structured output format for AI classification. Best practices for triage workflows state: 'When using AI with structured output, always add reasoning field alongside category or score to aid debugging' and 'CRITICAL: Always use structured output format (JSON schema)'. The current implementation relies on plain text 'YES/NO' responses which are fragile and harder to parse reliably.; [minor] No temperature configuration for AI models. Best practices state: 'Set low temperature parameter of the model (0-0.2) for consistency' for classification tasks. This is important for getting consistent YES/NO responses from the AI.; [minor] Inefficient workflow structure with duplicate AI operations. The workflow uses two separate Chat nodes with two separate OpenAI models for classification and translation. Best practices recommend using AI Agent for better orchestration, which could handle both operations more efficiently.; [minor] Self-referencing connection in 'Is About Portugal?' IF node. The connections show the node connects to itself, which appears to be a workflow generation error and could cause execution issues.

**Programmatic Violations:**
- [critical] Node Check if About Portugal (@n8n/n8n-nodes-langchain.chat) received unsupported connection type ai_languageModel; [critical] Node Translate to Portuguese (@n8n/n8n-nodes-langchain.chat) received unsupported connection type ai_languageModel

### example-013-13e39f15
**Status:** pass | **Score:** 76.4%

**LLM-Judge Violations:**
- [major] In 'Generate Fun Weather Report' node: Missing '=' prefix for expression in 'text' parameter. The multi-line string contains expressions like '{{ $json.name }}' but the parameter value doesn't start with '='
- [minor] HTTP Request node uses placeholder syntax '<__PLACEHOLDER_VALUE__...>' in URL which may not be the most user-friendly format, though functionally acceptable as it clearly indicates where user input is needed
- [major] Email message field uses HTML type but receives plain text from LLM - the LLM output is plain text but the email is configured as 'html' type, which could cause formatting issues or the text to be interpreted as HTML; [minor] Timestamp conversions in prompt use JavaScript Date methods which may not format consistently across timezones - sunrise/sunset times use toLocaleTimeString() without timezone specification
- [minor] Missing error handling for API failures: The workflow lacks error handling mechanisms (Continue on Fail, IF nodes for error checking) for the OpenWeather API and OpenAI API calls. While not explicitly requested by the user, the best practices documentation for notification workflows recommends 'error handling paths with Continue on Fail settings for redundancy.' However, since the user requested a simple automation without mentioning reliability or error handling, this is a minor issue rather than critical.

**Programmatic Violations:**
- None

### example-014-22dc4b8e
**Status:** pass | **Score:** 97.3%

**LLM-Judge Violations:**
- [minor] The expression '($json.textPlain || $json.textHtml)' uses logical OR which may not handle all edge cases optimally. If textPlain is an empty string (falsy), it will fall back to textHtml even if textPlain exists. A more explicit null/undefined check would be more robust.; [minor] No explicit error handling for cases where both textPlain and textHtml might be missing or undefined, which could result in 'undefined' being concatenated to the subject string.
- [minor] The workflow uses Text Classifier for email classification but doesn't configure the 'When No Clear Match' option to handle items that don't clearly match any category. Best practice states: 'In Text Classifier node, set "When No Clear Match" to "Output on Extra, Other Branch" to capture unmatched items.' However, since the user didn't request handling of edge cases or unmatched items, this is a minor omission rather than critical.

**Programmatic Violations:**
- None

### example-015-7df0bade
**Status:** pass | **Score:** 84.5%

**LLM-Judge Violations:**
- [critical] The 'Move File to Folder' Google Drive node is configured with operation 'update' but does not specify the parent folder to move files to. The node has 'changeFileContent: false' and only requests fields 'id' and 'name', but lacks the critical 'updateDetails' or 'parentId' configuration needed to actually move files to the target folder. Files will not be moved to their categorized folders.; [minor] The workflow creates folders every time it runs (twice daily) without checking if folders already exist. This will cause errors or duplicate folder creation attempts on subsequent runs, though the core functionality would work on first execution.
- [major] In 'Categorize Files with AI' node: String concatenation using + operator instead of proper n8n expression syntax. Should use template literals or proper string interpolation: `={{ "prompt text...
- 
- Input files: " + JSON.stringify($json.files) }}` mixes string literal with concatenation.; [minor] In 'Move File to Folder' node: The Google Drive update operation is configured but missing the actual folder move parameters (addParents/removeParents). While the expression syntax for fileId is correct (`={{ $json.fileId }}`), the node configuration won't actually move the file to the target folder.
- [major] Group Files by Category node: Missing 'mergeByFields' parameter required for aggregating by category. The node needs to specify which field to group by (category) to properly separate files into their respective categories.
- [major] Folders are created every time the workflow runs (twice daily) instead of checking if they already exist, causing unnecessary API calls and potential duplicates; [major] The 'Extract File Info' Set node is redundant - the Google Drive node already returns id, name, and mimeType in the exact format needed; [minor] The 'Parse AI Response' Code node could be eliminated if the Structured Output Parser is configured correctly to output the array directly; [minor] The 'Create Category Folders' Code node hardcodes categories that could be derived from the actual categorization results or stored as workflow variables
- [critical] Move File to Folder node missing critical 'addParents' parameter - files will not actually be moved to target folders. The Google Drive update operation requires 'addParents' to specify the destination folder ID, but this is completely absent from the configuration.; [major] Aggregate node 'Group Files by Category' incorrectly configured - it aggregates by all fields including 'category', which will create separate groups for each unique combination rather than grouping by category alone. Missing 'mergeByFields' configuration to specify category as the grouping key.; [minor] Inefficient data transformation in 'Prepare File Moves' - accesses category as array with [0] index (item.json.category[0]) when it should be a string value. This suggests misunderstanding of the aggregate output structure and could cause issues if the data structure varies.
- [minor] Code nodes use generic names ('Parse AI Response', 'Create Category Folders', 'Prepare File Moves') instead of more descriptive names that indicate their specific transformation logic; [minor] The workflow could benefit from better separation between the folder creation phase and file organization phase - these are currently tightly coupled in the linear flow
- [major] Missing default/fallback path in file organization workflow. The workflow lacks error handling for files that fail to be categorized by the AI Agent. If the AI Agent fails or returns unexpected output, files could be dropped silently, violating the user's requirement that 'no files remain ungroupedall files must be organized into folders.' Best practice states: 'CRITICAL: Always include a default/fallback path to catch unclassified items. Never allow data to drop silently.'; [minor] The 'Move File to Folder' node (Google Drive update operation) is missing the critical 'updateFields' parameter to specify the new parent folder. The node configuration shows 'changeFileContent: false' but doesn't include the necessary 'updateFields.parents' configuration to actually move the file to the target folder. This means files won't actually be moved to their categorized folders, breaking the core functionality.; [minor] No handling for existing folders. The workflow attempts to create category folders every time it runs (twice daily) without checking if they already exist. This will cause errors on subsequent runs when folders already exist. Best practice for data persistence states to 'Avoid duplicates: Use unique identifiers or upsert operations when appropriate.'

**Programmatic Violations:**
- None

### example-016-56815c36
**Status:** pass | **Score:** 84.1%

**LLM-Judge Violations:**
- [critical] The final HighLevel node uses 'task' resource with 'create' operation instead of the explicitly requested 'Create Note' functionality. The user specifically requested to 'use another HighLevel node to "Create Note" on that specific contact record', but the workflow creates a task instead of a note.
- [critical] Node 'Process Summary with OpenAI': Using outdated syntax $node["Process Summary with OpenAI"] instead of modern $('Process Summary with OpenAI') in the 'Create Note on Contact' node; [major] Node 'Create Note on Contact': Referencing $json.id from 'Search Contact by Email' node, but the actual contact ID field path may differ depending on HighLevel API response structure. Should verify the correct field path (commonly contacts[0].id or similar)
- [major] Create Note on Contact node uses wrong resource/operation: configured as 'task/create' instead of 'note/create' as explicitly requested by user. This creates a task instead of a note on the contact record.
- [minor] The 'Create Note on Contact' node uses resource 'task' instead of a proper note/comment resource, which may not be the most semantically correct approach for creating notes; [minor] Missing error handling for cases where contact search returns no results, which could cause workflow failure
- [critical] Wrong node type used for OpenAI - '@n8n/n8n-nodes-langchain.openAi' is the LangChain Chat Model node, not a standalone processing node. This node is meant to be connected to AI Agent/Chain nodes, not used directly in the main flow. The output structure will not have a 'message' field as expected.; [major] Incorrect field reference in 'Create Note on Contact' node - references '$node["Process Summary with OpenAI"].json.message' but the OpenAI LangChain node's output structure is different. Should reference the correct output field from the actual node response.; [major] Wrong HighLevel operation used - 'Create Note on Contact' node uses 'task' resource with 'create' operation instead of the correct 'note' resource. This creates a task, not a note as requested, causing incorrect data structure and functionality.; [major] Potential data loss in contact search - uses 'limit: 1' without proper error handling if no contact is found. The workflow will fail silently or error if the search returns no results, and the contactId reference will be undefined.; [minor] Inefficient data extraction in Search Contact - uses fallback pattern '{{ $json.payload.object.participant.email || $json.payload.object.host_email }}' but doesn't validate which field actually exists in the Zoom webhook payload structure, potentially passing undefined values.; [minor] Suboptimal data transformation in OpenAI prompt - uses JSON.stringify($json) which will pass the entire webhook payload as a string, including unnecessary metadata. Should extract only the relevant summary fields from the Zoom payload.
- [major] Incorrect node type used for OpenAI processing. The workflow uses '@n8n/n8n-nodes-langchain.openAi' which is a Chat Model sub-node meant for AI agent connections, not a standalone processing node. According to best practices, for processing and reformatting text, the workflow should use a proper AI chain node or the standard OpenAI node that can execute independently. Chat Model nodes are designed to be connected to AI agents via [ai_model] connections, not used directly in data flow pipelines.; [minor] The 'Create Note on Contact' node uses resource 'task' with operation 'create' instead of the more appropriate note-related operation. While this may work as a workaround (creating a completed task with a body), it doesn't align with the user's explicit request to 'Create Note' on the contact. This represents a less optimal pattern that deviates from the stated requirement.

**Programmatic Violations:**
- None

### example-017-04253a14
**Status:** pass | **Score:** 89.8%

**LLM-Judge Violations:**
- [minor] In 'Scrape LinkedIn Profile' node, the expression `={{ $json.linkedin_url }}` correctly references the form trigger output, but the node configuration shows `credentials: {}` which may cause runtime issues if credentials are not properly configured (though this is a configuration issue, not expression syntax); [minor] In 'AI Lead Analyzer & Qualifier' node, the text parameter uses string concatenation with `={{ "..." + JSON.stringify($json) }}`. While this works, it could be more readable using template literals or the modern string format `="Text {{ $json }}"`, though the current syntax is functionally correct
- [minor] PhantomBuster node uses a placeholder agentId value '<__PLACEHOLDER_VALUE__Your Phantombuster Agent ID for LinkedIn Profile Scraper__>' which is expected since the user didn't provide a specific agent ID, but the placeholder format is slightly verbose; [minor] AI Agent prompt could be more concise - the prompt construction uses string concatenation with JSON.stringify which is functional but could be cleaner with template formatting
- [major] PhantomBuster node uses 'resolveData: true' but the scraped data structure is not validated or transformed before passing to AI Agent. The raw output format from PhantomBuster may not match expected structure, potentially causing the AI to receive malformed data.; [minor] The AI Agent prompt embeds the entire scraped JSON using JSON.stringify($json) without any data validation or preprocessing. If PhantomBuster returns nested result containers or metadata, the AI will receive unnecessary wrapper data.
- [major] Missing raw form data storage: The workflow uses a Form Trigger but does not store the raw form response to any persistent storage destination (Google Sheets, Airtable, Data Tables, etc.). According to best practices: 'ALWAYS store raw form responses to a persistent data storage destination even if the primary purpose of the workflow is to trigger another action.' The workflow should include a storage node immediately after the form trigger to persist the LinkedIn URL input for administration and monitoring purposes.; [minor] n8n attribution not disabled: The Form Trigger node does not have the 'Append n8n Attribution' setting disabled. Best practices state: 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for the n8n form nodes, add this setting and set it to false.' This affects the professional appearance of the form.; [minor] No error handling for scraping failures: The workflow lacks error handling between the PhantomBuster scraping node and the AI Agent. If the scraping fails or returns no data, the AI Agent will receive invalid input. While error handling wasn't explicitly requested, this is a common pitfall in scraping workflows that could cause the core functionality to fail silently.

**Programmatic Violations:**
- [critical] Node Lead Data Parser (@n8n/n8n-nodes-langchain.outputParserStructured) is missing required input of type ai_languageModel

### example-018-8fa3def3
**Status:** pass | **Score:** 92.9%

**LLM-Judge Violations:**
- [minor] Chat history is hardcoded with mock data instead of actually retrieving it based on the session ID. The workflow accepts a session ID parameter but doesn't use it to fetch real chat history from any storage mechanism (database, static data, memory, etc.). While the code includes a comment acknowledging this limitation, the user explicitly requested functionality 'based on the session id', implying the session ID should determine which chat history is retrieved.
- [major] Session ID extraction uses optional chaining but doesn't validate if sessionId is actually provided - defaults to 'unknown' which may mask missing parameter errors; [major] Chat history is hardcoded with mock data instead of actually retrieving data based on sessionId - the sessionId parameter is extracted but never used to fetch actual data; [minor] respondWith parameter set to 'json' but actually returning HTML content - should be 'text' or 'allEntries' for proper HTML response handling
- [minor] The workflow uses hardcoded mock chat history data instead of retrieving actual chat history from persistent storage. The best practices documentation for data_persistence emphasizes using storage nodes (Data Table, Google Sheets, Airtable, or databases) to retrieve stored data. The Code node contains a comment 'In a real scenario, you'd fetch from a database or n8n's static data' but doesn't implement actual retrieval. For a webhook that 'returns the chat history', the workflow should query a storage system where chat history is actually persisted.

**Programmatic Violations:**
- None

### example-019-7786623b
**Status:** pass | **Score:** 78.6%

**LLM-Judge Violations:**
- [major] Gmail Trigger uses polling (gmailTrigger with pollTimes) instead of webhook as explicitly requested. User specifically asked for 'Gmail Webhook node as the trigger', but workflow uses a polling trigger that checks every minute instead of real-time webhook notifications.
- [major] Data context loss through Switch node: Action nodes (Create Sales Task, Update Contact Status, Send to Slack) reference email data fields ($json.from, $json.subject, $json.snippet) that originate from Gmail Trigger, but the OpenAI classification node outputs only the classification result. The Switch node will pass only the classification data, not the original email context, breaking the data dependencies for downstream nodes.
- [critical] Node 'Create Sales Task': Uses outdated and incorrect syntax `$node["Find Contact in HubSpot"].json.id` instead of modern syntax `$('Find Contact in HubSpot').item.json.id`; [minor] Node 'Create Sales Task': Missing = prefix in string expressions with embedded variables. Should use `="Follow up with interested lead: {{ $json.from }}"` format instead of `"Follow up with interested lead: {{ $json.from }}"`
- [minor] Gmail Trigger node: User requested 'Gmail Webhook node' but workflow uses Gmail Trigger with polling (everyMinute). While functionally similar, this is not a true webhook trigger as requested.; [minor] Classify Email Intent node: Uses 'snippet' field (={{ $json.snippet }}) which may be truncated. For accurate classification, the full email body should be used to ensure complete intent analysis.; [minor] Create Sales Task node: References $json.from and $json.snippet directly, but these fields come from the Gmail Trigger node, not the current execution context. Should reference $node["Gmail Trigger"].json.from for clarity and reliability.
- [critical] Data loss in Switch node routing - Gmail data (from, subject, snippet) is not available after OpenAI classification. The Switch node only receives OpenAI's response, but downstream nodes (Create Sales Task, Update Contact Status, Send to Slack) all reference $json.from, $json.subject, and $json.snippet which will be undefined.; [major] Incorrect field reference in HubSpot search - using '={{ $json.from }}' but Gmail Trigger likely outputs email in a different field structure (e.g., 'payload.headers' array or 'from.email'). The 'from' field may not be directly accessible as $json.from.; [major] Missing data validation after HubSpot contact search - no handling for when contact is not found (empty result). Downstream nodes reference $node['Find Contact in HubSpot'].json.id which could be undefined, causing failures.; [minor] Suboptimal data extraction from Gmail - using 'snippet' instead of full email body for AI classification may result in incomplete intent analysis, especially for longer emails.
- [major] Using OpenAI node instead of recommended Text Classifier node for straightforward text classification. Best practices explicitly recommend Text Classifier (@n8n/n8n-nodes-langchain.textClassifier) for 'straightforward text classification tasks' with predefined category labels. The workflow uses @n8n/n8n-nodes-langchain.openAi which is not the recommended approach for this use case.; [major] Missing default/fallback path in Switch node. Best practices state 'CRITICAL: Always include a default/fallback path to catch unclassified items. Never allow data to drop silently.' and 'CRITICAL: Always configure Default output for unmatched items'. The Switch node has three specific routes but no default case configured, meaning if the AI returns unexpected output or formatting, items will be dropped silently.; [minor] Not using structured output format for AI classification. Best practices recommend 'Use structured output format (JSON with specific fields)' and provide an example schema with category, confidence, and reasoning fields. The current implementation uses a simple text response which is less robust and harder to debug.; [minor] Using Gmail Trigger (polling) instead of Gmail Webhook. The user specifically requested 'Gmail Webhook node as the trigger' but the workflow uses gmailTrigger (polling-based) instead of a webhook trigger. While this will work functionally, it doesn't match the user's explicit request for webhook-based triggering.

**Programmatic Violations:**
- None

### example-020-02b7cf7a
**Status:** fail | **Score:** 62.0%

**LLM-Judge Violations:**
- [critical] Google Sheets node is configured with operation 'append' instead of the explicitly requested mode 'list' for document selection. The user specifically stated: 'Configure the Google Sheets node (n8n-nodes-base.googleSheets) with mode set to 'list' for easy document selection.'
- [critical] Process Each Company (splitInBatches) has two outputs on its first main connection going to both 'Collect All Companies' and 'Generate Personalized Emails'. This creates a race condition where Generate Personalized Emails executes before all companies are scraped and collected, breaking the intended data flow sequence.; [major] Vector Store Retriever connects to Generate Personalized Emails via ai_memory connection type, but Vector Store Retriever is not a memory node - it should connect via ai_tool connection type when used as a retrieval tool for an agent.; [minor] Get Approved Emails node attempts to access email draft data but has no clear data source connection. The code comment indicates it should read from Google Sheets, but there's no connection from Export Email Drafts or a separate Google Sheets read operation to provide this data.
- [major] Node 'Generate Personalized Emails': Expression references $json.companies but this field doesn't exist in the incoming data from 'Collect All Companies' node. The Aggregate node creates a 'companies' field containing aggregated data, but the connection shows 'Process Each Company' going to 'Generate Personalized Emails', not 'Collect All Companies'.; [minor] Node 'Request Approval': Expression {{ $json.companies.length }} references a field that may not exist in the incoming data from 'Export Email Drafts'. The data flow doesn't preserve the companies array at this point.
- [minor] Google Sheets node 'Export Email Drafts' uses placeholder values for documentId and sheetName, but user explicitly requested mode set to 'list' for easy document selection. The documentId is correctly configured with mode: 'list', which matches the user's requirement.; [minor] Gmail 'Request Approval' node uses placeholder for sendTo email address. While this is expected when user doesn't provide specific values, it's a required operational parameter that needs configuration before workflow execution.; [minor] Pinecone Vector Store node uses placeholder for pineconeIndex value. This is expected when user doesn't provide specific Pinecone index details, but it's necessary for RAG functionality.; [minor] The 'Prepare Company List' code node contains hardcoded example companies rather than accepting dynamic input. While functional for demonstration, this limits flexibility for processing user-provided company lists.; [minor] The email customization request in 'Generate Personalized Emails' uses a placeholder value with format '<__PLACEHOLDER_VALUE__...>'. This is expected when user doesn't specify the email purpose, but it's a key parameter for personalization.
- [major] Redundant data aggregation: 'Collect All Companies' node aggregates data that's already being processed by 'Process Each Company' splitInBatches node, creating unnecessary complexity in the flow; [minor] The 'Get Approved Emails' Code node recreates email draft data instead of reading from Google Sheets, missing an opportunity to fetch potentially edited data from the sheet; [minor] Two separate Code nodes ('Parse Email Drafts' and 'Get Approved Emails') perform similar JSON parsing and transformation operations that could potentially be consolidated; [minor] The 'Campaign Rejected' node is a simple status message that could be handled inline with the IF node or combined with the final status node
- [critical] Critical data loss in 'Get Approved Emails' node - uses $input.all() which retrieves data from the approval response, not the actual email drafts from Google Sheets. The node comment says 'you would read from Google Sheets here' but instead returns stale data that was never persisted through the approval flow.; [major] Incorrect data flow in 'Collect All Companies' aggregation - the node receives data from 'Process Each Company' batch output (first output), but the actual scraped company data flows through 'Format Company Data' back to the batch node. The aggregated data doesn't include the formatted company information with emails.; [major] Data structure mismatch in 'Request Approval' node - references $json.companies.length but receives data from 'Export Email Drafts' which outputs individual email draft items, not an aggregated object with a 'companies' array. This will cause undefined or incorrect count.; [major] Missing data synchronization between Google Sheets and email sending - after approval, the workflow should read the potentially edited email drafts from Google Sheets, but 'Get Approved Emails' node doesn't actually fetch from Sheets, losing any manual edits made during review.; [minor] Inefficient data transformation in 'Format Company Data' - creates a 'rawData' field that duplicates the entire agent output, causing unnecessary data bloat without clear downstream usage.; [minor] Potential data loss in 'Parse Email Drafts' error handling - if JSON parsing fails, creates a single error item instead of attempting to preserve the original company data or creating error items for each expected company.
- [minor] Some node names could be more specific: 'Start Workflow' is generic (could be 'Trigger Company Email Campaign'), 'Process Each Company' and 'Process Each Email' use similar naming patterns that could be more distinctive; [minor] The 'Collect All Companies' node name doesn't clearly indicate its purpose in the workflow context - it's actually aggregating scraped company data before email generation; [minor] Multiple 'OpenAI Model' nodes with parenthetical suffixes (Scrape/Email) - while functional, could use more descriptive names like 'OpenAI Model for Company Research' and 'OpenAI Model for Email Generation'; [minor] The workflow has good modularity but the 'Get Approved Emails' node contains a comment indicating it should read from Google Sheets but doesn't - this creates a maintenance gap where the actual implementation differs from the intended design
- [major] The Web Scrape Tool (toolCode node) contains only mock/demo code that returns hardcoded data instead of actual scraping functionality. The code comment states 'For demo purposes, returning structured data' and generates fake email addresses like 'contact@examplecompany.com'. This violates the core requirement to 'scrape and learn comprehensive information about each company' and 'ensuring the output includes the company's email address'. The workflow will not actually scrape real company data as requested.; [minor] Missing rate limiting and delay mechanisms for the scraping operations. Best practices documentation states: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers. Use Wait nodes and batching options.' While the user didn't explicitly request production-grade rate limiting, processing a list of companies with web scraping without any delays could lead to rate limiting errors (429) or overloading target servers, which would prevent the workflow from completing successfully.

**Programmatic Violations:**
- [critical] Sub-node Vector Store Retriever (@n8n/n8n-nodes-langchain.retrieverVectorStore) provides ai_retriever but is not connected to a root node.
- [major] Agent node "Scrape Company Info" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context; [major] Agent node "Generate Personalized Emails" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-021-ece1c598
**Status:** fail | **Score:** 64.2%

**LLM-Judge Violations:**
- [critical] User explicitly requested Discord node with Message resource and Get operation to read messages, but workflow uses a generic Webhook node instead. This completely bypasses the Discord-specific functionality requested.; [major] The document ingestion pipeline is disconnected from the Vector Store. The 'Prepare for Vector Store' node has no main connection to 'Rules Knowledge Base', so documents retrieved from Google Docs never actually get inserted into the vector store.
- [critical] Rules Knowledge Base (vectorStoreInMemory) in insert mode is missing the required main connection from 'Prepare for Vector Store'. The vector store needs a main input to trigger the insert operation and receive the document data. Currently, the document loading pipeline (Load Rules Documents  Get Rules Doc 1/2  Combine Documents  Prepare for Vector Store) ends without connecting to the vector store, leaving the knowledge base empty and non-functional.
- [minor] In 'Prepare for Vector Store' node, the metadata objectValue uses an expression without proper = prefix: `={{ { "source": $json.title || "Community Rules", "documentId": $json.documentId || "unknown" } }}`. While this may work in some contexts, objectValue assignments should typically use proper expression syntax.
- [minor] Google Docs nodes (Get Rules Doc 1 & Get Rules Doc 2) are missing the 'resource' and 'operation' parameters. User explicitly requested 'Message resource and Get operation' for Discord, and 'Get operation' for Google Docs. The nodes should have resource='document' and operation='get' explicitly set.; [minor] Discord node 'Send Discord Response' is missing the 'resource' and 'operation' parameters. While the node appears functional with message sending parameters, the user explicitly requested 'Message resource and Get operation' configuration pattern, suggesting explicit resource/operation parameter setting.
- [minor] The 'Prepare for Vector Store' Set node performs unnecessary data transformation. The Document Loader can handle the Google Docs output directly without needing to stringify JSON and manually create metadata.; [minor] The 'Format Discord Response' Set node extracts fields that could be passed through more efficiently. The channelId and guildId are re-referenced when they could be maintained in the data flow.; [minor] The workflow uses a Webhook trigger instead of the Discord node's native Message Get operation as specified in requirements, which could create a slightly less optimal integration pattern.
- [critical] Vector Store data flow broken: 'Prepare for Vector Store' node outputs to nowhere - the transformed data never reaches the 'Rules Knowledge Base' vector store node, causing complete data loss for the knowledge base; [major] Incorrect data transformation in 'Prepare for Vector Store': Using JSON.stringify() on the entire document object creates a string representation instead of extracting the actual text content from Google Docs (should use $json.body or $json.content); [minor] Inefficient field inclusion in 'Format Discord Response': The 'include' parameter is set to 'selected' with 'includeFields' specified, but assignments already define the exact fields needed - this creates redundancy; [minor] Missing data validation: No check to ensure userQuestion is not empty before sending to AI Agent, which could cause unnecessary API calls for empty messages
- [minor] Node 'Combine Documents' uses generic naming - could be more specific like 'Merge Rules Documents' to better indicate purpose; [minor] Node 'Document Loader' is somewhat generic - could be 'Rules Document Loader' for better context; [minor] The workflow has two separate flows (document loading and message handling) that could benefit from clearer visual separation or sub-workflow extraction
- [major] Discord node configuration violates user requirements: The user explicitly requested 'Configure the Discord node to read messages using the Message resource and Get operation', but the workflow uses a Webhook trigger instead. This means the workflow cannot actively read messages from a specific Discord channel as requested - it can only passively receive webhook events. The Discord node (Send Discord Response) is only used for sending, not reading messages.; [major] Vector Store is not populated with Google Docs data: The document loading workflow (Load Rules Documents  Get Rules Doc 1/2  Combine Documents  Prepare for Vector Store) has NO connection to the Rules Knowledge Base vector store. The 'Prepare for Vector Store' node outputs to nowhere, meaning the vector store remains empty and the chatbot cannot access the community rules. This breaks the core functionality requested by the user.; [major] Google Docs node misconfiguration: The user specified 'Use the Google Docs node to retrieve documents with the Get operation', but the workflow uses documentURL parameter which is for the 'Get Content' operation, not the 'Get' operation. The 'Get' operation requires a documentId parameter instead. This may cause the document retrieval to fail or not work as the user intended.; [minor] Missing 'Append n8n Attribution' disable setting: Best practices state that n8n chatbots attach attribution by default and this should be disabled. The Discord node (Send Discord Response) does not have this setting configured to false, which may result in unwanted attribution in messages.

**Programmatic Violations:**
- [critical] Sub-node Search Community Rules (@n8n/n8n-nodes-langchain.retrieverVectorStore) provides ai_retriever but is not connected to a root node.; [critical] Node Rules Knowledge Base (@n8n/n8n-nodes-langchain.vectorStoreInMemory) is missing required input of type main; [critical] Node Response Parser (@n8n/n8n-nodes-langchain.outputParserStructured) is missing required input of type ai_languageModel
- [major] Merge node "Combine Documents" has only 1 input connection(s). Merge nodes require at least 2 inputs.

### example-022-db371201
**Status:** pass | **Score:** 84.5%

**LLM-Judge Violations:**
- [major] Perplexity node returns a single response with all articles combined in one text output, but the workflow treats it as if it returns 5 separate items. The user explicitly requested to 'collect the latest 5 drone news articles' and 'Summarize each article' and 'Post the formatted summaries directly to Twitter' (plural), implying multiple Twitter posts. The workflow only creates one Twitter post from the combined response instead of processing and posting each article individually.; [major] The workflow does not implement any logic to parse or split the Perplexity response into individual articles. The Format for Twitter node attempts to process the entire combined response as a single item, truncating it to 250 characters, which will likely cut off mid-article and not properly represent 'each article' as requested.
- [major] Format for Twitter node: Expression uses incorrect data path '$json.choices[0].message.content' which is the raw Perplexity API response structure. The 'simplify' parameter is set to false, so this path may work, but it's accessing the raw API response rather than processed data.; [major] Format for Email node: Expression uses '$json.choices[0].message.content' and '$json.citations' which assumes raw API response structure. While this may work with simplify=false, it's fragile and depends on the exact API response format.; [major] Format for Email node: Complex string concatenation with '$json.citations.map(c => ...)' assumes citations is an array, but this field may not exist or may have a different structure in the Perplexity response, potentially causing runtime errors.; [minor] Format for Twitter node: Using .substring(0, 250) on potentially undefined content without null checking could cause errors if the response structure differs from expected.
- [minor] Schedule Trigger node uses 'daysInterval' with value 1 instead of the more explicit 'days' field configuration. While functional, the interval structure could be clearer for daily schedules.; [major] Perplexity node is configured to return a single response with all articles in one message, but the workflow treats it as if it returns multiple items (one per article). The Set nodes reference $json.choices[0].message.content which will contain all 5 articles as a single text block, not individual articles. This prevents proper formatting of individual articles for Twitter posts.; [minor] Gmail 'sendTo' parameter uses a placeholder format that may not be immediately clear to users. While valid as a placeholder, a clearer format like '<UNKNOWN>' or 'admin@example.com' would be more consistent.
- [minor] The Perplexity node returns a single response with all articles, but the workflow doesn't split/loop through individual articles. Both Set nodes process the same raw content from choices[0].message.content instead of iterating through 5 separate articles.; [minor] Twitter formatting truncates content to 250 characters arbitrarily without proper article separation logic. This creates a single post instead of 5 individual posts as implied by 'latest 5 drone news articles'.
- [critical] Perplexity node returns a single response object, not 5 separate items. The workflow treats it as if it outputs multiple items (one per article), but it actually outputs one item containing all information. This causes the Set nodes to only process one item instead of 5.; [critical] Twitter formatting node attempts to post a single truncated message (substring 0-250) from the entire Perplexity response, not individual article summaries. This will create one malformed tweet instead of 5 separate tweets for each article.; [major] Missing data parsing/splitting logic after Perplexity node. The LLM response needs to be parsed (likely using Code node or AI Transform) to extract individual articles into separate items before formatting.; [major] Email formatting assumes citations array exists in specific format ($json.citations), but Perplexity API structure may not match this assumption. The .map() operation could fail if citations is undefined or in different format.; [major] No loop or iteration mechanism to handle multiple articles. The workflow should use a Loop node or Split In Batches to process each of the 5 articles individually for Twitter posting.
- [major] Perplexity node misuse: The workflow uses the Perplexity chat model (n8n-nodes-base.perplexity) with a single prompt expecting it to return 5 separate articles with structured data. According to best practices, Perplexity returns conversational AI responses, not structured lists of articles. The workflow should use n8n-nodes-base.perplexityTool with an AI Agent for research tasks, or implement proper looping/pagination to collect multiple articles. The current approach will likely return a single text response summarizing drone news rather than 5 distinct articles that can be individually formatted and posted.; [minor] Missing batch processing for Twitter posts: The workflow formats and posts content but doesn't properly handle multiple articles. The Set node 'Format for Twitter' processes the Perplexity response as a single item, then posts once to Twitter. Best practices for data transformation recommend using Split Out or Split In Batches to process lists of items individually. The user requested posting summaries (plural) of 5 articles, implying multiple Twitter posts, but the workflow will only create one post.

**Programmatic Violations:**
- None

### example-023-6df86d28
**Status:** pass | **Score:** 77.9%

**LLM-Judge Violations:**
- [critical] Extract From File node is missing the required CSV file source. The binaryPropertyName is a placeholder and there is no node providing the CSV file (e.g., Read Binary File, HTTP Request to fetch CSV, etc.). The workflow will fail immediately as it has no CSV data to process.
- [major] Extract From File node 'Read CSV File' is missing the required binary data input. The node is configured to extract from a CSV file but has no source providing the actual file data. There should be a node (e.g., Read Binary File, HTTP Request to fetch file, or binary data from the form trigger) connected before it to supply the CSV file.
- [minor] Read CSV File node contains a placeholder value '<__PLACEHOLDER_VALUE__Name of the binary field containing the CSV file (e.g., "data")__>' instead of an actual binary property name. While this is clearly marked as a placeholder, it would cause runtime errors if not replaced.
- [minor] Read CSV File node has a placeholder value for 'binaryPropertyName' parameter. While this is acceptable when the user didn't specify the binary field name, it uses an unusual placeholder format instead of a simple value like 'data' which is the common default.
- [minor] The Code node could potentially be replaced with a Filter node followed by an IF node for simpler logic, though the current implementation is acceptable for the complexity; [minor] The workflow uses 6 nodes which is slightly more than minimal (could potentially be 5 if combining some logic), but the separation provides good clarity
- [critical] Extract From File node has a placeholder for binaryPropertyName but no upstream node provides binary data. The CSV file source is completely missing, causing the workflow to fail at the Read CSV File node.; [major] Code node assumes data structure where form submission is first item and CSV rows follow, but Extract From File outputs all CSV rows as separate items without the form data. This causes incorrect data access pattern using $input.first() and $input.all().slice(1).; [minor] Code node attempts multiple field name variations for QR code (matchedRow.json['QR code'], qrCode, qr_code) which is good defensive programming, but indicates uncertainty about actual CSV structure. This could mask data mapping issues.; [minor] Self-Registration Form uses HTML field with javascript:history.back() which may not work reliably in all contexts and doesn't properly restart the workflow from the beginning.
- [major] Missing raw form data storage: The workflow does not store the raw form responses to any persistent storage destination (Google Sheets, Airtable, Data Tables, or database). The form_input best practices explicitly state 'ALWAYS store raw form responses to a persistent data storage destination' and 'Simply using Set or Merge nodes is NOT sufficient.' This is critical for the user's use case as they need to track self-registrations and potentially audit check-in attempts.; [major] Extract From File node missing binary property configuration: The 'Read CSV File' node has a placeholder value '<__PLACEHOLDER_VALUE__Name of the binary field containing the CSV file (e.g., "data")__>' for the binaryPropertyName parameter. This will cause the workflow to fail when attempting to read the CSV file. The user's workflow requires reading a CSV file to match emails, so this is a critical configuration error that prevents core functionality.

**Programmatic Violations:**
- None

### example-024-3bd2f161
**Status:** pass | **Score:** 93.1%

**LLM-Judge Violations:**
- [major] In 'Weather Email Generator' node: The prompt text expression uses string concatenation with + operator but doesn't properly reference the previous node. Should use $('Fetch Weather from OpenWeather').item.json instead of just $json for clarity and reliability in multi-node workflows
- [minor] Gmail node 'Send Weather Email' has emailType set to 'html' but the AI-generated content may be plain text. Consider using 'text' or ensuring HTML formatting in the prompt.
- [minor] Potential data access issue: The precipitation check uses '$json.rain' but doesn't account for snow data ('$json.snow') which OpenWeather API also provides. This could result in incomplete precipitation information during winter conditions.; [minor] Minor inefficiency: The AI Agent prompt constructs a complex string concatenation in the expression editor. While functional, this could be more maintainable if weather data was pre-formatted in a separate Code node before being passed to the AI Agent.
- [minor] Using HTTP Request node for OpenWeather API instead of a dedicated weather service node. Best practices state: 'Always prefer built-in n8n nodes over HTTP Request nodes when a dedicated node exists for the service.' While HTTP Request works, a dedicated OpenWeather node (if available) would provide better authentication handling and data structure optimization. However, this is minor since HTTP Request is acceptable when no built-in node exists or is well-known.; [minor] No error handling or validation for API failures. While the user requested a simple automation, the workflow lacks basic safeguards like checking if the weather API request succeeds before passing data to the AI Agent. Best practices for notification workflows recommend 'error handling paths with Continue on Fail settings for redundancy.' This could cause the workflow to fail silently if OpenWeather API is down or returns an error.

**Programmatic Violations:**
- [major] Agent node "Weather Email Generator" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-025-d38651a2
**Status:** pass | **Score:** 91.4%

**LLM-Judge Violations:**
- [minor] Telegram send node uses a verbose placeholder format instead of a simpler placeholder like '<UNKNOWN>' or empty string for the chatId parameter, though this is still functionally valid
- [minor] AI Agent temperature not explicitly set to low value (0-0.2) for consistency. Best practices for triage workflows state: 'Set low temperature parameter of the model (0-0.2) for consistency' and 'For all AI nodes (Text Classifier, AI Agent): Set low temperature of the model (0-0.2) for consistency'. The OpenAI GPT Model is configured with temperature 0.3, which is slightly higher than the recommended 0-0.2 range for classification tasks requiring consistent outputs.; [minor] Missing reasoning field in structured output schema. Best practices for triage workflows recommend: 'When using AI with structured output, always add reasoning field alongside category or score to aid debugging.' The current schema only includes 'isAboutPortugal' and 'portugueseTranslation' but lacks a reasoning field that would help debug classification decisions.; [minor] No logging or tracking mechanism for processed news items. Best practices for triage workflows state: 'Logging: Track outcomes for monitoring and analysis' as a key stage. While the workflow has a NoOp node for non-Portugal news, there's no systematic logging of what was processed, sent, or filtered, which would be useful for monitoring the automation's effectiveness.

**Programmatic Violations:**
- None

### example-026-4858b905
**Status:** pass | **Score:** 87.8%

**LLM-Judge Violations:**
- [minor] The workflow uploads to S3 with public read access but does not explicitly return/display the generated public URL to the user. While S3 generates the URL automatically, there's no node to format and present it back through the form response or subsequent step.; [minor] The form response is set to 'onReceived' with a static text message, but doesn't wait for the S3 upload to complete and return the actual public URL to the user as requested ('generate and return a public URL').
- [minor] Form Trigger node uses 'fieldName': 'image' which matches user requirement, but the AWS S3 node references 'binaryPropertyName': 'image' which may not align with how n8n Form Trigger outputs binary data (typically as 'data' property). This could cause the binary data reference to fail at runtime.
- [major] Binary property name mismatch: Form Trigger outputs file data with property name 'data' by default, but S3 node is configured to read from 'image' property. This will cause the S3 upload to fail as it won't find the binary data.; [minor] Missing explicit public URL extraction: While S3 is configured with 'publicRead' ACL, the workflow doesn't explicitly extract or format the public URL from the S3 response for easy access. The URL exists in the output but isn't clearly presented.; [minor] No file name handling: The workflow doesn't specify how the file should be named in S3 (using fileName parameter). This could lead to generic or collision-prone naming.
- [major] Missing raw form data storage: The workflow does not include any storage node (Data Table, Google Sheets, Airtable, etc.) to persist the form submission data. According to form_input best practices, 'ALWAYS store raw form responses to a persistent data storage destination even if the primary purpose of the workflow is to trigger another action.' This is critical because users need to monitor form responses as part of workflow administration. Simply uploading to S3 does not store the form metadata (submission time, form fields, etc.).; [minor] Missing n8n attribution setting: The Form Trigger node does not explicitly disable the 'Append n8n Attribution' setting. According to form_input best practices, 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for the n8n form nodes, add this setting and set it to false.' While this doesn't break functionality, it affects the professional appearance of the form.

**Programmatic Violations:**
- None

### example-027-b50faeed
**Status:** pass | **Score:** 85.9%

**LLM-Judge Violations:**
- [major] Aggregate node is incorrectly configured to extract email data. It attempts to access 'payload.headers[0].value' and 'payload.headers[1].value' with hardcoded array indices, which will not reliably extract subject and sender fields. The Gmail API returns headers as an array where the position varies. Additionally, it uses 'snippet' instead of the full email body, which contradicts the explicit requirement to have 'full email body available' with simplify disabled.; [minor] The receivedAfter filter uses a placeholder instead of a dynamic expression to calculate the weekend start date (e.g., last Saturday). While this is a placeholder that could be filled, the workflow should ideally include the logic to automatically determine the weekend timeframe (e.g., using expressions like $now.minus({days: 2}) or similar date calculations).; [minor] The workflow does not include explicit error handling or validation to ensure emails were actually retrieved before attempting aggregation and analysis. If no emails are found, the workflow may produce empty or malformed results.
- [major] In 'Send Summary Email' node, the subject field uses incorrect syntax: `" Weekend Email Summary - {{ $now.format(\"MMMM d, yyyy\") }}"` - missing the = prefix for an expression containing {{ }}. Should be `=" Weekend Email Summary - {{ $now.format('MMMM d, yyyy') }}"`; [minor] In 'Aggregate Email Data' node, the field paths 'payload.headers[0].value' and 'payload.headers[1].value' use hardcoded array indices which may not reliably extract subject and sender. Gmail API headers are not guaranteed to be in a fixed order. A more robust approach would use filtering or specific header name matching.; [minor] In 'Aggregate Email Data' node, the field 'snippet' is used for email bodies, but the user explicitly requested 'full email body'. The snippet field typically contains only a preview. With simplify=false, the full body would be in 'payload.body.data' (base64 encoded) or 'payload.parts', requiring decoding.
- [major] Aggregate node is configured to extract email data from incorrect field paths. Uses 'payload.headers[0].value' and 'payload.headers[1].value' for subject/sender extraction, which relies on array index positions that may not correspond to the correct headers. Also uses 'snippet' instead of full email body content, contradicting the user's explicit requirement for 'full email body' availability.
- [minor] The Aggregate node field references (payload.headers[0].value, payload.headers[1].value) use array indexing which may be fragile and could be replaced with more robust field extraction logic; [minor] Using 'snippet' field instead of full email body content despite requirement to have full email body available - this could lead to incomplete analysis
- [critical] Incorrect field extraction in Aggregate node: Using 'payload.headers[0].value' and 'payload.headers[1].value' with hardcoded array indices will not reliably extract subject and sender. Gmail API returns headers as an array where position varies. Should use filtering logic to find headers by name (e.g., 'Subject', 'From').; [critical] Using 'snippet' field instead of full email body: The user explicitly requested 'full email body' with simplify disabled, but the workflow extracts 'snippet' which is only a truncated preview (typically ~200 characters). Should extract 'payload.body.data' or 'payload.parts' for full content.; [major] Data structure mismatch in aggregation: The Aggregate node creates arrays (subjects, senders, emailBodies) but the AI Agent prompt uses simple string interpolation {{ $json.subjects }} which will output raw array format instead of properly formatted text. Should use .join() or iterate through items.; [major] Missing data validation: No handling for empty email results or failed aggregation. If no weekend emails exist, the workflow will pass empty/undefined data to the AI Agent, potentially causing errors or meaningless output.; [minor] Inefficient aggregation approach: Aggregating all fields into separate arrays loses the relationship between subject, sender, and body for each individual email. A better approach would preserve email-level grouping for more accurate analysis.
- [major] Aggregate node incorrectly configured for email data extraction. The node uses hardcoded array indices (payload.headers[0].value, payload.headers[1].value) to extract subject and sender, which is fragile and unreliable. Gmail's payload.headers is an array where header order is not guaranteed. Best practice for data extraction states 'Normalize data structure early in your workflow' and 'Use transformation nodes like Set to ensure your data matches n8n's expected structure'. A Set/Edit Fields node should be used before aggregation to properly extract subject and sender from the headers array by matching the 'name' field (e.g., 'Subject', 'From'), not by array index.; [minor] Using 'snippet' field instead of full email body. The Aggregate node references 'snippet' for email bodies, but Gmail's snippet is a truncated preview (not the full body). With 'simple: false' configured, the full body is available in payload.parts or payload.body.data (base64 encoded). The user explicitly requested 'full email body is available' and 'full email body content from the aggregated data'. This violates the user's specific requirement and data extraction best practice to 'ensure you use nodes to handle data properly' and extract the correct fields.; [minor] Missing data validation before notification. Best practices for notification workflows state 'Always include empty notification prevention - check that alert-worthy items exist (items.length > 0) before proceeding to notification nodes'. The workflow should include an IF node after 'Get Weekend Emails' to check if any emails were retrieved before proceeding with aggregation and analysis, preventing unnecessary AI API calls and empty summary emails.

**Programmatic Violations:**
- None

### example-028-7bd1000a
**Status:** pass | **Score:** 93.2%

**LLM-Judge Violations:**
- [minor] In 'AI Article Selector' node: Complex string concatenation using + operator instead of template literals. While functional, the expression '={{ "..." + JSON.stringify($json.articles) }}' could be more readable as a template literal.
- [minor] RSS Feed Read node uses placeholder value '<__PLACEHOLDER_VALUE__RSS feed URL from fantacalcio.it (e.g., https://www.fantacalcio.it/rss)__>' for URL parameter. While user specified 'fantacalcio.it', a more concrete default like 'https://www.fantacalcio.it/rss' would be more actionable.; [minor] Gmail node uses placeholder value '<__PLACEHOLDER_VALUE__Recipient email address (e.g., your-email@example.com)__>' for sendTo parameter. This is expected since user didn't provide specific email, but is worth noting as requiring configuration.
- [major] AI Agent output structure is unpredictable - the workflow assumes the agent will return JSON in the 'output' field, but AI agents may wrap responses differently or include additional metadata. The Code node attempts to handle this with try-catch and regex extraction, but this is fragile and could fail with unexpected AI response formats.; [minor] Aggregate node creates nested structure ($json.articles) but the AI prompt construction uses JSON.stringify($json.articles) which may double-serialize if articles is already a string, potentially causing malformed JSON input to the AI.; [minor] Missing data validation after RSS feed read - if the RSS feed returns no items or has unexpected structure, the workflow will fail downstream without graceful error handling.
- [major] Using AI Agent for simple data selection task instead of built-in filtering/sorting. Best practices for content_generation state 'Break complex tasks into sequential steps' and recommend using appropriate nodes for each task. The AI Agent is overkill for selecting 10 articles from an RSS feed - this could be accomplished with Code node or Set node with sorting logic, which would be more reliable, faster, and cost-effective. The agent adds unnecessary complexity and API costs for a deterministic task.; [minor] Missing rate limiting protection for RSS feed requests. Best practices for scraping_and_research emphasize 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers' and 'Use Wait nodes and batching options'. While the user didn't explicitly request production-ready features, RSS feeds can have rate limits, and the daily schedule makes this less critical but still a consideration for reliability.; [minor] No error handling or retry logic configured. Best practices for scraping_and_research mention using 'Retry on Fail' feature for handling throttled responses. While not explicitly requested by the user, basic error handling would improve reliability for a scheduled workflow that runs unattended.

**Programmatic Violations:**
- None

### example-029-ad03d4da
**Status:** pass | **Score:** 71.5%

**LLM-Judge Violations:**
- [major] User explicitly requested to use 'n8n-nodes-base.googleCalendarTool' (the AI tool wrapper) but workflow uses 'n8n-nodes-base.googleCalendar' (the regular node). The googleCalendarTool is specifically designed to work with AI agents and provides the proper tool interface, while the regular googleCalendar node requires manual parameter configuration and doesn't integrate properly with the agent's tool-calling mechanism.
- [minor] Using string concatenation with + operator instead of template literals for the agent prompt, which is less readable but functionally correct
- [minor] User requested Google Calendar Tool (n8n-nodes-base.googleCalendarTool) but workflow uses regular Google Calendar node (n8n-nodes-base.googleCalendar). While functionally similar, this doesn't match the explicit node type requested.; [minor] Gmail trigger node uses 'simple: true' parameter which may limit access to full email data needed for lead processing. Consider using 'simple: false' for more complete email information.
- [major] Data structure mismatch between triggers: Website Form (webhook) will have different field structure than Gmail trigger. Gmail provides 'from', 'subject', 'body' fields while webhook expects 'email' field. The AI agent prompt attempts to handle both but this creates ambiguity in data extraction.; [minor] Brochure URL is hardcoded as a placeholder in the AI prompt string concatenation rather than being extracted from a workflow variable or input field. This makes it difficult to update and doesn't follow best practices for dynamic data handling.; [minor] The entire JSON object is stringified and passed to the AI agent without any preprocessing or field extraction. This could include unnecessary metadata from triggers (like webhook headers or Gmail message IDs) that may confuse the AI or make the prompt unnecessarily large.
- [major] Missing raw form data storage: The workflow does not store raw form responses from the Website Form Submission trigger to any persistent storage destination (Google Sheets, Airtable, Data Tables, or database). According to form_input best practices, 'ALWAYS store raw form responses to a persistent data storage destination even if the primary purpose of the workflow is to trigger another action.' The workflow should include a storage node immediately after the webhook trigger to persist lead data for administration and monitoring. This is critical because without storage, there's no record of leads if the AI agent fails or for later reference.; [minor] Incorrect tool node usage: The workflow uses regular Gmail (n8n-nodes-base.gmail) and Google Calendar (n8n-nodes-base.googleCalendar) nodes configured as AI tools with $fromAI() expressions. However, the user specifically requested 'Use the Google Calendar tool (n8n-nodes-base.googleCalendarTool)' which is the dedicated AI tool node. While the current approach may work, it doesn't follow the user's explicit requirement and may not integrate as cleanly with the AI agent as the purpose-built tool nodes would.

**Programmatic Violations:**
- [critical] Node Send Email Tool (n8n-nodes-base.gmail) is missing required input of type main; [critical] Node Schedule Meeting Tool (n8n-nodes-base.googleCalendar) is missing required input of type main
- [major] Agent node "Lead Outreach Agent" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context
- [major] Non-tool node "Send Email Tool" (n8n-nodes-base.gmail) uses $fromAI in its parameters. $fromAI is only for tool nodes connected to AI agents.; [major] Non-tool node "Schedule Meeting Tool" (n8n-nodes-base.googleCalendar) uses $fromAI in its parameters. $fromAI is only for tool nodes connected to AI agents.
- [major] Merge node "Combine Leads" has only 1 input connection(s). Merge nodes require at least 2 inputs.; [major] Node "Send Email Tool" uses $fromAI() which is only valid in tool nodes connected to an AI agent.; [major] Node "Schedule Meeting Tool" uses $fromAI() which is only valid in tool nodes connected to an AI agent.

### example-030-5c14a0fe
**Status:** pass | **Score:** 77.1%

**LLM-Judge Violations:**
- [major] Switch node uses incorrect data path for classification result. It references `$json.message.content` which is the OpenAI response structure, but should extract the intent from the structured JSON schema output (likely `$json.intent` or similar based on the schema definition).
- [critical] Node 'Find Contact in HubSpot': Uses outdated syntax `$node["Gmail Trigger"]` instead of modern `$('Gmail Trigger')` syntax in multiple locations; [major] Node 'Create Sales Task': Uses outdated syntax `$node["Gmail Trigger"].json.from` and `$node["Find Contact in HubSpot"].json.id` instead of modern `$('Node Name').item.json.field` syntax; [major] Node 'Update Contact Status': Uses outdated syntax `$node["Gmail Trigger"].json.from` instead of modern `$('Gmail Trigger').item.json.from`; [critical] Node 'Send to Slack for Review': The blocksUi parameter contains a JSON string with embedded expressions using outdated `$node["Gmail Trigger"]` syntax and improper escaping that will cause parsing errors; [minor] Node 'Send to Slack for Review': Slack blocks are provided as a JSON string instead of proper structured format, making expressions harder to parse and maintain
- [major] Classify Email Intent node: The 'responses' parameter is incorrectly configured. This node type (@n8n/n8n-nodes-langchain.openAi) does not use a 'responses' array parameter. The correct parameter structure should use 'prompt' or 'text' field for the input content.
- [minor] The Switch node uses string 'contains' operations on $json.message.content which could be fragile. A more direct approach would be to parse the JSON schema response and check the 'intent' field directly, making the routing more reliable.; [minor] Multiple action nodes reference $node['Gmail Trigger'].json data directly instead of passing it through. While functional, this creates implicit dependencies that could be made more explicit with a Set node to prepare data once.
- [critical] OpenAI node output structure mismatch: The Switch node attempts to access `$json.message.content` but the OpenAI node with simplify=true and JSON schema output will return the parsed JSON directly (e.g., `$json.intent`), not nested under `message.content`. This will cause routing to fail completely.; [major] Gmail Trigger email field extraction error: The HubSpot search uses `$json.from` but Gmail with simplify=false returns email addresses in a complex object structure (e.g., `payload.headers` array). The `from` field needs proper parsing to extract just the email address.; [major] Email body extraction inconsistency: The OpenAI node uses `$json.textPlain || $json.textHtml || $json.snippet` but with simplify=false, these fields are not directly available at the root level. Email body content is nested in `payload.parts` or `payload.body.data` and requires base64 decoding.; [minor] Downstream nodes reference Gmail Trigger data incorrectly: Nodes like 'Create Sales Task', 'Update Contact Status', and 'Send to Slack for Review' use `$node["Gmail Trigger"].json.from`, `$node["Gmail Trigger"].json.subject`, etc., which won't work correctly with simplify=false as these fields require parsing from the headers array.
- [major] No default/fallback path in Switch node: The 'Route by Intent' Switch node lacks a Default output case. Best practices state 'CRITICAL: Always include a default/fallback path to catch unclassified items. Never allow data to drop silently.' and 'Always configure Default case to route unclassified items to a fallback action (e.g., manual review queue, admin notification)'. If the AI returns an unexpected format or value, items will be dropped silently, breaking the core triage functionality.

**Programmatic Violations:**
- None

### example-031-9add9d58
**Status:** pass | **Score:** 88.9%

**LLM-Judge Violations:**
- [major] The scraping implementation uses placeholder/mock data instead of actual scraping functionality. The Code node contains only example structure and comments but does not implement any real data collection mechanism (API calls, web scraping, etc.) as requested by the user.
- [critical] In 'Scrape Lead Data' node: JavaScript code uses undefined function 'placeholder()' which will cause runtime errors. This function does not exist in n8n's JavaScript execution context.; [minor] In 'Prepare Email Data' node: The expression '={{ $json.firstName }} {{ $json.lastName }}' mixes expressions within a single expression block. While this works, it could be cleaner as a single expression or use string concatenation.
- [minor] Code node 'Scrape Lead Data' contains a reference to undefined function 'placeholder()' which would cause runtime error. The code should use string literals or template strings instead.; [minor] Gmail node 'Send Outreach Email' uses placeholder values in optional parameters (senderName, replyTo) which is acceptable, but the emailBody replacement logic may not handle all newline cases properly in HTML context.
- [major] Two Set nodes performing similar data organization tasks - 'Organize for Google Sheets' and 'Prepare Email Data' both restructure the same source data with significant overlap in fields (email, firstName, lastName, company, city, title); [minor] The 'Organize for Google Sheets' Set node maps all 13 fields individually when the Google Sheets node could auto-map directly from the Code node output with proper field naming; [minor] Email body construction in Set node could be handled directly in Gmail node using expressions, eliminating need for intermediate preparation
- [major] Data loss between Google Sheets and Email preparation: The 'Prepare Email Data' node references $json.email, $json.firstName, $json.lastName, $json.company, $json.industry, and $json.city, but the 'Organize for Google Sheets' Set node uses 'include: none' which discards original data. After Google Sheets operation, only the transformed field names ('First Name', 'Last Name', 'Email', etc.) are available, not the original camelCase fields.; [minor] Inconsistent field naming convention: The first Set node transforms camelCase fields (firstName, lastName) to Title Case with spaces ('First Name', 'Last Name'), but the second Set node expects the original camelCase format. This creates a mapping mismatch.; [minor] Missing null/undefined handling in email body template: The emailBody field references $json.firstName, $json.company, and $json.industry without fallback values, which could cause issues if these fields are missing after the Google Sheets transformation.
- [major] Using Code node for scraping instead of recommended scraping nodes. Best practices explicitly state: 'If the user wishes to scrape data from sites like LinkedIn, Facebook, Instagram, Twitter/X, Indeed, Glassdoor or any other service similar to these large providers it is better to use a node designed for this.' For dental clinic decision makers and LinkedIn profiles mentioned in the prompt, dedicated scraping nodes (Phantombuster, Apify, BrightData) or HTTP Request with proper APIs should be used instead of placeholder Code node.; [major] Missing rate limiting and batching safeguards for scraping workflow. Best practices state: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers. Use Wait nodes and batching options.' The workflow sends emails immediately after scraping without any Wait nodes or Split In Batches, which could trigger rate limits on both scraping sources and Gmail API.; [minor] Code node contains only placeholder/mock data instead of actual scraping implementation. While the comments provide guidance, the workflow won't function without replacing the placeholder logic. Best practices recommend using HTTP Request node with actual APIs (Apollo.io, Hunter.io, Google Maps API) or dedicated scraping services.; [minor] Missing error handling for email sending. Best practices recommend 'Use error handling paths with Continue on Fail settings for redundancy' in notification workflows. If an email fails (invalid address, API error), the entire workflow stops rather than continuing with remaining leads.; [minor] No logging or tracking of sent notifications. Best practices state: 'Add logging nodes to track sent notifications for audit trails and duplicate prevention.' Without tracking, there's no way to prevent duplicate outreach or maintain an audit trail of which leads were contacted.

**Programmatic Violations:**
- None

### example-032-3ef11662
**Status:** pass | **Score:** 82.8%

**LLM-Judge Violations:**
- [major] The Check Image Valid node has a self-referencing connection in its true branch (connects back to itself), which creates an infinite loop. This is a structural error that would cause the workflow to fail on execution.
- [critical] Self-referencing cycle in 'Check Image Valid' IF node: the true branch (output 0) connects back to itself, creating an infinite loop that will deadlock the workflow execution
- [minor] Webhook path uses placeholder format '<__PLACEHOLDER_VALUE__webhook-path-for-wound-analysis__>' instead of a simpler placeholder like 'wound-analysis' or '<UNKNOWN>'. While functional, this verbose placeholder format is unnecessarily complex.
- [minor] The IF node 'Check Image Valid' has a self-referencing connection in the true branch (connects back to itself), which appears to be a configuration error rather than intentional logic. This creates an unnecessary loop that could cause execution issues.; [minor] The workflow could potentially combine the validation check and error response into a single conditional path without the separate IF node, though the current structure is acceptable for clarity.
- [major] Circular connection detected: 'Check Image Valid' node connects back to itself on the true branch, creating an infinite loop that would cause workflow failure; [minor] Response body structure could lose data if Gemini returns additional fields beyond 'message' - only extracting $json.message without preserving full structured output
- [minor] The workflow has a self-referencing connection in the 'Check Image Valid' node where the true branch connects back to itself. This appears to be a configuration error in the connections JSON. The true branch should only connect to 'Analyze Wound Image', not back to 'Check Image Valid'. This could cause unexpected behavior or infinite loops.; [minor] The Google Gemini node is missing credentials configuration (shows empty credentials object). While this may be a placeholder for evaluation purposes, best practices for content generation workflows emphasize proper authentication handling. The user specifically requested image analysis functionality, so proper credential setup is important for the workflow to function.

**Programmatic Violations:**
- None

### example-033-53f7286a
**Status:** pass | **Score:** 91.9%

**LLM-Judge Violations:**
- [minor] User requested testing with Gmail but mentioned eventual use with Mailup. While Gmail is correctly implemented for testing as specified, there's no comment or note about future Mailup integration, which could be helpful for workflow maintenance.
- [major] In 'Select Top 10 Articles' node: Complex string concatenation with JSON.stringify($input.all()) embedded in expression. While functional, this creates a very long prompt string that could be fragile. The expression uses proper $input.all() syntax but the approach of stringifying all data into the prompt is not optimal.; [minor] In 'Send Newsletter via Gmail' node subject field: Uses 'new Date().toLocaleDateString("it-IT")' which is valid JavaScript but could use n8n's $now helper for better consistency: $now.format('DD/MM/YYYY') or similar.; [minor] In 'Format Newsletter HTML' Code node: Uses $input.first().json.output which is correct modern syntax, but the code doesn't validate that output exists before accessing it, which could cause runtime issues if the AI response structure differs.; [minor] In 'Format Newsletter HTML' Code node: Uses 'new Date().getFullYear()' in JavaScript code block. While valid, could use n8n's $now.year() for consistency, though this is acceptable in a Code node context.
- [minor] RSS Feed Read node uses placeholder syntax '<__PLACEHOLDER_VALUE__...>' instead of cleaner placeholder like '<UNKNOWN>' or empty string, though this is still functionally valid; [minor] Gmail node 'sendTo' parameter uses placeholder syntax '<__PLACEHOLDER_VALUE__...>' instead of cleaner placeholder, though this is valid for user configuration; [minor] Agent node uses complex expression concatenation with JSON.stringify for prompt construction which could be simplified, though it is functionally correct
- [major] AI Agent prompt embeds entire RSS feed data as stringified JSON in prompt text, which may cause token limit issues and is inefficient. The expression JSON.stringify($input.all()) creates a massive string that could exceed model context limits with large feeds.; [major] Complex JSON parsing logic in Format Newsletter HTML node with multiple fallback attempts suggests fragile data transformation. The code tries to parse as JSON, then extract with regex, indicating uncertainty about AI output format. This could fail if AI returns unexpected format.; [minor] No validation that RSS feed actually contains required fields (title, link, description, image) before passing to AI agent. Missing data could propagate through workflow causing incomplete newsletter.; [minor] HTML formatting node doesn't validate that articles array has exactly 10 items or handle cases where AI returns fewer items. Uses slice(0, 10) but doesn't check minimum threshold.
- [minor] Missing rate limiting/batching for RSS feed processing: The workflow fetches the RSS feed and passes all items directly to the AI agent without batching. Best practices for scraping_and_research state 'Batch requests and introduce delays to avoid hitting API rate limits' and 'Use Split In Batches node to process 200 rows at a time'. However, since the user requested a basic automation for testing purposes and RSS feeds typically contain a manageable number of items (10-50), this is a minor optimization issue rather than a critical flaw.; [minor] AI Agent prompt includes raw data injection instead of using proper data passing: The prompt uses JSON.stringify($input.all()) to embed all RSS items directly in the prompt text. This is inefficient and can hit token limits with large feeds. Best practices recommend using structured approaches and proper data handling. For the user's use case of processing a single RSS feed, this will likely work but is not optimal.; [minor] No error handling for AI response parsing: The Code node includes a try-catch for JSON parsing with fallback regex extraction, which is good. However, there's no handling if the AI returns completely unparseable data or if articles array is empty. Best practices for content_generation mention 'validate and sanitize input/output to avoid malformed data'. Since this is a testing workflow, this is a minor issue.

**Programmatic Violations:**
- [major] Agent node "Select Top 10 Articles" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-034-d4736d27
**Status:** pass | **Score:** 89.9%

**LLM-Judge Violations:**
- [critical] HTTP Request node is configured with generic HTTP Header Auth instead of using Tableau's Personal Access Token (PAT) authentication. The user explicitly requested authentication using PAT through n8n's credential system, but the workflow uses a generic authentication method that won't properly authenticate with Tableau's API which requires specific PAT token name and token secret headers.; [minor] The workflow does not include any mechanism to fetch site ID and other important details from the provided URL as explicitly requested. The user mentioned having 'a URL that can fetch site ID and other important details' which suggests a two-step process: first fetch site information, then use that information to fetch the actual data. The workflow only has a single HTTP request node.
- [minor] HTTP Request node uses 'httpHeaderAuth' for authentication instead of the more appropriate 'tableauServerApi' credential type that n8n provides for Tableau PAT authentication. While httpHeaderAuth can work, using the dedicated Tableau credential type would be more semantically correct.
- [major] No data transformation or validation between Tableau API response and Google Sheets. The workflow assumes the Tableau API returns data in a format directly compatible with Google Sheets autoMapInputData, which may not be the case. Tableau API responses often contain metadata, pagination info, and nested structures that need extraction.; [minor] Missing error handling for data transformation failures. If the Tableau API returns an unexpected structure or empty data, the workflow will fail without graceful handling.; [minor] No data type validation or conversion. Tableau data types (dates, numbers, strings) may need explicit conversion to ensure proper formatting in Google Sheets.
- [minor] Node 'Start Workflow' uses generic trigger naming - could be more specific like 'Trigger Tableau to Sheets Transfer'; [minor] No error handling nodes present - workflow lacks modularity for handling API failures or authentication issues; [minor] Missing data transformation/validation layer between fetch and write operations - could benefit from intermediate processing node
- [major] HTTP Request node not properly configured for Tableau PAT authentication. The workflow uses 'genericCredentialType' with 'httpHeaderAuth' but doesn't specify the credential configuration. Best practice for data extraction requires proper authentication setup through n8n's credential system. The user explicitly requested 'Configure the workflow to authenticate using the PAT through n8n's credential system', but the credentials object is empty.

**Programmatic Violations:**
- None

### example-035-eb09b2eb
**Status:** pass | **Score:** 93.1%

**LLM-Judge Violations:**
- [minor] HTTP Request node URL parameter contains verbose placeholder text instead of clean placeholder format. While functional, the format '<__PLACEHOLDER_VALUE__https://api.typeform.com/forms/{form_id}/responses - Replace {form_id} with your actual Typeform form ID__>' is unnecessarily verbose compared to a simpler placeholder.
- [major] No data transformation between Typeform API response and BigQuery insert - Typeform API returns nested response structure with 'items' array containing responses, but workflow attempts direct insertion without extracting/flattening the data; [major] Missing data structure mapping - Typeform responses contain nested 'answers' arrays with complex field structures (field.id, field.type, answer values) that need to be transformed to match BigQuery table schema, but no transformation node exists; [major] AutoMap mode used without data preparation - BigQuery's autoMap expects flat JSON objects matching table columns, but Typeform API returns deeply nested structures (items[].answers[].field, items[].metadata, etc.) that won't map correctly; [minor] No handling of pagination - Typeform API may return paginated results with 'page_count' and requires multiple requests for large datasets, but workflow only fetches first page potentially losing data
- [minor] Missing batch processing for potentially large datasets. The workflow fetches all Typeform responses at once without using Loop Over Items (splitInBatches) node. Best practices documentation states: 'Processing too many items or large files at once can crash your instance. Always batch or split processing for large datasets to manage memory effectively.' While the user didn't explicitly request handling large amounts of data, Typeform responses can accumulate over time, and the hourly schedule suggests ongoing data collection that could grow large.; [minor] Potential data structure mismatch without transformation. The workflow directly passes Typeform API response to BigQuery without using transformation nodes (Set, Split Out) to normalize the data structure. Best practices state: 'Normalize data structure early in your workflow. Use transformation nodes like Split Out, Aggregate, or Set to ensure your data matches n8n's expected structure.' The Typeform API returns responses in a nested structure that may need flattening before BigQuery insertion, though autoMap mode may handle this.

**Programmatic Violations:**
- None

### example-036-9086f4c8
**Status:** pass | **Score:** 92.9%

**LLM-Judge Violations:**
- [critical] In 'HubSpot Integration' node: Invalid JavaScript variable assignment 'const hubspotApiKey = <__PLACEHOLDER_VALUE__Your HubSpot API Key__>;' - this is not valid JavaScript syntax and will cause runtime errors. Should use proper credential access or environment variable.; [minor] In 'Sales Expert AI Agent' node: The expression '={{ $json.chatInput }}' references a field 'chatInput' that may not exist in the Chat Trigger output. Chat Trigger typically provides 'input' or 'message' field, not 'chatInput'.; [minor] In 'Conversation Memory' node: The expression '={{ $json.sessionId }}' references 'sessionId' which may not be automatically provided by the Chat Trigger. This field needs to be explicitly available in the trigger output.
- [minor] HubSpot Integration tool node contains a placeholder API key pattern '<__PLACEHOLDER_VALUE__Your HubSpot API Key__>' in the JavaScript code. While this is a valid placeholder for user configuration, it could be cleaner as a simple string placeholder like 'YOUR_HUBSPOT_API_KEY' for better readability.
- [major] Chat input data extraction uses incorrect expression '={{ $json.chatInput }}' in AI Agent text parameter. The chatTrigger node outputs 'chatInput' directly in the item, but the agent's text parameter should reference it correctly. This may cause the initial user message to not be passed properly to the agent.; [minor] HubSpot tool code uses placeholder for API key '<__PLACEHOLDER_VALUE__Your HubSpot API Key__>' which is hardcoded as a string literal rather than using n8n credentials or environment variables. While this is a placeholder, the pattern shown would cause runtime errors.; [minor] Memory node sessionKey expression '={{ $json.sessionId }}' assumes sessionId exists in the data flow, but chatTrigger may not automatically provide this field. This could cause memory to not persist correctly across sessions as intended.
- [minor] Missing 'Append n8n Attribution' configuration: The Chat Trigger node should have 'Append n8n Attribution' set to false to disable the default 'n8n workflow' attribution on messages. Best practice states: 'n8n chatbots often attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for nodes that support it, add this setting and set it to false.' While this doesn't break functionality, it affects the professional appearance of the chatbot.; [minor] Hardcoded API key in Tool Code node: The HubSpot Integration tool contains a placeholder for the API key directly in the JavaScript code ('const hubspotApiKey = <__PLACEHOLDER_VALUE__Your HubSpot API Key__>;'). Best practice from document processing states: 'NEVER set API keys directly in the request - user can set credentials from the UI for secure API key management.' While this is a placeholder, the pattern encourages insecure credential management. The workflow should use n8n's credential system instead.

**Programmatic Violations:**
- None

### example-037-19fdb634
**Status:** pass | **Score:** 72.6%

**LLM-Judge Violations:**
- [major] The AI Agent node is configured to receive only one input (the scraped job data from Phantombuster), but the user explicitly requested that the AI Agent should receive TWO inputs: the scraped LinkedIn job description AND the candidate's resume text. The workflow has no mechanism to provide the resume as a separate input - it's only referenced as a placeholder string within the prompt text.
- [major] AI Agent has no downstream connections to handle output. The workflow performs analysis but has no mechanism to store, display, or process the structured results (status, tracking_data, missing_keywords, etc.). The user requirements explicitly mention 'subsequent workflow nodes' should reference the agent output, indicating missing data flow.; [major] Missing conditional routing logic after AI Agent. The agent outputs either 'eligible' or 'not_eligible' status, but there are no Switch/IF nodes to route these different outcomes to appropriate handlers (e.g., storing eligible jobs in a database, logging rejected jobs separately).
- [critical] In 'Job Analysis Agent' node: The expression uses string concatenation with JSON.stringify($json.output || $json) which may fail if $json.output doesn't exist. The fallback pattern is risky and could produce unexpected results.; [major] In 'Job Analysis Agent' node: The expression references $json directly from the Phantombuster node without specifying the correct data path. Phantombuster typically returns data in $json.resultObject or $json.containerData, not directly in $json.output.; [critical] In 'Job Analysis Agent' node: The entire prompt text is wrapped in an unnecessary expression ={{ "..." }} when it should use the simpler format ="..." for string templates with embedded expressions, or just use proper expression syntax within the string.
- [minor] Phantombuster node uses placeholder values for agentId and linkedinUrl arguments, which is acceptable since user didn't provide specific values, but these will need to be configured at runtime; [minor] AI Agent prompt contains a placeholder for candidate resume text ('<__PLACEHOLDER_VALUE__Paste candidate resume text here or use expression__>') embedded in the prompt string. While this is a valid placeholder pattern, the user specified the agent should 'receive two inputs' suggesting a more structured input approach could be used; [minor] JSON Output Parser schema doesn't enforce all required fields for the 'eligible' status case (job_title, company, location, etc. are not in the required array), which could allow incomplete outputs
- [critical] AI Agent prompt attempts to access resume data that is never provided or passed into the workflow. The expression references '<__PLACEHOLDER_VALUE__Paste candidate resume text here or use expression__>' which will result in literal string being passed instead of actual resume data, causing complete failure of ATS analysis functionality.; [major] Phantombuster output data structure is not validated or transformed before being passed to AI Agent. The expression '$json.output || $json' assumes a specific output structure that may not match Phantombuster's actual response format, potentially causing data loss or incorrect job description extraction.; [minor] Missing data transformation node to properly structure the two required inputs (job description and resume) before passing to AI Agent. The current approach embeds both inputs in a single prompt string, which is functional but makes it difficult to validate data integrity and handle missing fields.
- [minor] The 'OpenAI GPT-4' node name could be more descriptive about its role in the workflow (e.g., 'ATS Analysis Language Model'); [minor] The 'JSON Output Parser' node name is generic and could better indicate its purpose (e.g., 'Job Analysis Output Parser' or 'Structure ATS Results'); [minor] Workflow could benefit from additional nodes for error handling and data validation to improve modularity
- [minor] Missing batch processing for potential multiple job URLs. The workflow uses a manual trigger with a single Phantombuster scrape operation, but doesn't implement batching for processing multiple job postings. Best practice for scraping workflows states: 'Use the Split In Batches node to process 200 rows at a time to reduce memory usage.' If the user intends to analyze multiple jobs, this could cause memory issues.; [minor] No rate limiting or delay mechanisms implemented. Best practice states: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers. Use Wait nodes and batching options.' While not explicitly requested, this is important for scraping workflows to avoid 429 errors, especially when scaling beyond single job analysis.; [minor] Missing storage/output node for processed results. The workflow ends at the AI Agent without storing or outputting the structured JSON results. Best practices for scraping workflows recommend using storage nodes like 'Data Tables, Google Sheets, Microsoft Excel 365, or Airtable' to save extracted data. The user prompt mentions 'further processing or storage' suggesting results should be persisted.

**Programmatic Violations:**
- [critical] Node JSON Output Parser (@n8n/n8n-nodes-langchain.outputParserStructured) is missing required input of type ai_languageModel
- [major] Agent node "Job Analysis Agent" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-038-efce86cf
**Status:** pass | **Score:** 83.0%

**LLM-Judge Violations:**
- [minor] Gemini Model node uses 'models/gemini-2.5-flash' which may not be a valid model identifier. The correct format is typically 'gemini-1.5-flash' or 'gemini-1.5-pro' without the 'models/' prefix in n8n configuration
- [minor] The workflow doesn't explicitly extract or validate the stock ticker from the message text before passing to the AI agent. While the AI agent can handle this internally, explicit extraction would provide better data validation and error handling for invalid inputs.; [minor] No error handling for cases where the AI agent output might be empty or malformed. The Send Analysis node directly references $json.output without validation, which could fail if the agent doesn't return the expected structure.
- [minor] Missing memory node for conversation context. Best practice states 'Always utilise memory in chatbot agent nodes - providing context gives you full conversation history and more control over context.' While not critical for single-query stock analysis, adding a Window Buffer Memory would enable follow-up questions like 'What about AAPL?' or 'Compare it to MSFT' without re-explaining context.

**Programmatic Violations:**
- None

### example-039-2743a222
**Status:** pass | **Score:** 76.9%

**LLM-Judge Violations:**
- [critical] User explicitly requested using 'Google Gemini embeddings node (n8n-nodes-langchain.embeddingsGoogleGemini) to process the image and generate the 3D avatar model', but the workflow uses an HTTP Request node instead. This completely ignores the user's explicit technical specification for how to implement the core functionality.
- [major] Google Gemini Embeddings node specified in requirements is completely missing from the workflow. The user prompt explicitly requested using 'n8n-nodes-langchain.embeddingsGoogleGemini' to process the image and generate the 3D avatar model, but this node is not present in the workflow at all.
- [critical] In 'Generate 3D Avatar Model' node: API credentials (apiKey, modelKey) are hardcoded in the JSON body as placeholder values instead of using n8n's credential system. The jsonBody contains '<__PLACEHOLDER_VALUE__Your Banana.dev API key__>' and '<__PLACEHOLDER_VALUE__Your 3D avatar model key__>' which are invalid expressions.; [critical] In 'Generate 3D Avatar Model' node: Expression '={{ $json.photo }}' references a field that doesn't exist. Form Trigger with file upload returns binary data, not a 'photo' field in JSON. Should reference binary data using $binary or proper file handling.; [major] In 'Return 3D Model' node: Expression '={{ $json.modelOutputs.model_url }}' assumes a specific response structure from the HTTP Request that may not exist. The actual response structure from Banana.dev API is not validated.; [major] In 'Return 3D Model' node: Expression '={{ $json.submittedAt }}' references a field that was never created in the workflow. This field doesn't exist in the Form Trigger output or HTTP Request response.; [minor] Missing proper binary data handling for file upload. The workflow should use $binary.photo.data or convert binary to base64 for API transmission, not reference $json.photo.
- [major] User explicitly requested using 'Google Gemini embeddings node (n8n-nodes-langchain.embeddingsGoogleGemini)' to process the image and generate the 3D avatar model, but workflow uses HTTP Request node instead. This is a significant deviation from the specified implementation approach.; [major] HTTP Request node has API key hardcoded in jsonBody parameter ('apiKey': '<__PLACEHOLDER_VALUE__Your Banana.dev API key__>'), which directly contradicts user's explicit requirement to 'Configure all API credentials using n8n's credential system rather than hardcoding them in workflow nodes'.
- [minor] The workflow includes a Google Gemini embeddings node in the user prompt but uses HTTP Request instead. While this is actually more efficient (embeddings nodes are for vector generation, not 3D model generation), there's a minor mismatch between requirements and implementation that could cause confusion.; [minor] API credentials are partially hardcoded in the JSON body (apiKey, modelKey) despite the requirement to use n8n's credential system. These should be referenced from credentials rather than included as placeholder values in the body.
- [critical] Incorrect data extraction from Form Trigger - using `$json.photo` directly when file uploads in Form Trigger return file metadata objects with properties like `filename`, `mimeType`, and `data` (base64). The image data needs to be properly extracted from the file object structure.; [critical] API credentials hardcoded in JSON body (`apiKey` field) despite requirement to use n8n's credential system. The workflow specifies `nodeCredentialType: 'bananaDevApi'` but then includes API key in the request body, creating confusion and potential credential exposure.; [major] Assumed response structure from Banana API (`$json.modelOutputs.model_url`, `$json.modelOutputs.download_url`) without validation or transformation. If the API returns a different structure, the response will contain undefined values or fail.; [major] Missing data transformation for image format - Banana API likely requires base64-encoded image data or a specific format, but the workflow doesn't handle the conversion from Form Trigger's file object to the required format.; [major] Reference to non-existent field `$json.submittedAt` in the response - this field was never created or extracted from previous nodes, will result in undefined value in the response.; [minor] Google Gemini embeddings node mentioned in requirements but not used in workflow - while this might be intentional, there's a mismatch between stated requirements and implementation that could indicate missing data processing steps.
- [minor] No error handling nodes present - workflow lacks modularity for failure scenarios; [minor] No input validation node - photo validation and preprocessing could be separated for better modularity; [minor] Response formatting logic embedded in final node - could be extracted to separate transformation node
- [major] Missing raw form data storage: The workflow does not include any storage node (Data Table, Google Sheets, Airtable, or database) to persist the form submissions. Best practices explicitly require 'ALWAYS store raw form responses to a persistent data storage destination even if the primary purpose of the workflow is to trigger another action.' This is critical for monitoring form responses and administration.; [major] Incorrect node usage for image processing: The workflow uses Google Gemini embeddings node (embeddingsGoogleGemini) which is designed for creating vector embeddings, not for generating 3D models or processing images for avatar generation. The user requested using 'Banana model' for 3D avatar generation, but the workflow doesn't implement this correctly. For image/video generation tasks, the workflow should use appropriate content generation nodes (like Google Gemini Chat Model with image generation capabilities or HTTP Request to Banana.dev API).; [minor] Form attribution not disabled: Best practices state 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution"'. The Form Trigger node should have this setting explicitly set to false for a professional user experience.

**Programmatic Violations:**
- None

### example-040-40aee189
**Status:** pass | **Score:** 90.2%

**LLM-Judge Violations:**
- [major] The Snowflake tool uses simulated/mock code instead of actual Snowflake integration. The user explicitly requested to 'query the relevant data from Snowflake', but the implementation only returns mock data with a note 'In production, this would execute against Snowflake database'. This means the workflow cannot actually retrieve real data from Snowflake as requested.; [minor] The Asana update tool uses simulated/mock code instead of actual Asana API integration. While the user requested to 'update the Asana ticket', the implementation only simulates the update with a message 'In production, this would update via Asana API'. This prevents the workflow from actually updating tickets as requested.
- [minor] In 'Analyze and Complete Ticket' node, the user prompt uses string concatenation with + operator instead of template literal syntax. While functional, the expression `={{ "Analyze the following Asana ticket..." + JSON.stringify($json) + "..." }}` could be more readable as a template literal.
- [minor] Asana Trigger node uses placeholder values for 'resource' and 'workspace' parameters, which are expected configuration points but could be more clearly labeled as user-configurable fields
- [major] The user prompt in the AI Agent uses JSON.stringify($json) which may not properly handle the Asana trigger output structure. The Asana trigger returns specific fields (task ID, name, notes, etc.) but the workflow blindly stringifies the entire JSON object without extracting or validating the relevant fields first. This could pass malformed or incomplete data to the AI agent.; [major] The Snowflake tool returns simulated/mock data instead of actual database results. The jsCode returns a hardcoded 'sampleResult' which means any data retrieved would be fake placeholder data, not real Snowflake query results. This breaks the entire data enrichment flow as the AI agent would receive incorrect data to update tickets with.; [minor] The Update Asana Task tool only simulates the update without actually calling the Asana API. While this is noted as 'for production', it means no actual data transformation occurs - the retrieved Snowflake data never actually gets written back to Asana. The data flow is incomplete.; [minor] Missing data validation in the user prompt construction. The workflow doesn't check if $json exists or contains the expected Asana task fields (like task ID, which is critical for the Update tool). If the trigger returns unexpected data structure, the entire flow could fail silently.
- [major] Using Code Tool nodes instead of actual Snowflake and Asana nodes for data operations. The best practices documentation emphasizes choosing the right node for your data source. The workflow uses simulated Code Tool nodes ('Query Snowflake Database' and 'Update Asana Task') that return mock data instead of actually querying Snowflake or updating Asana. This violates the fundamental principle of using appropriate storage/data nodes and means the workflow cannot fulfill the user's core requirement of 'query the relevant data from Snowflake and update the Asana ticket'.; [minor] The Snowflake Code Tool returns simulated/sample data rather than executing actual queries. The best practices for data extraction emphasize using the correct nodes for data sources and ensuring proper data retrieval. While the AI Agent structure is correct, the tool implementation doesn't actually connect to Snowflake, which means missing information cannot be retrieved as requested.; [minor] The Asana Update Code Tool simulates updates rather than performing actual API calls. The best practices for data persistence emphasize explicit storage operations and proper node selection. The current implementation won't actually update the Asana ticket with retrieved data, failing the user's requirement to 'update the Asana ticket with the missing information'.; [minor] Missing error handling for API failures or missing data scenarios. While not explicitly requested by the user, the best practices documentation mentions handling errors gracefully to prevent data loss. Given that this workflow involves multiple external API calls (Asana trigger, Snowflake queries, Asana updates), basic error handling would improve reliability. However, this is a minor issue since the user didn't specifically request production-ready error handling.

**Programmatic Violations:**
- None

### example-041-94a5f301
**Status:** pass | **Score:** 82.2%

**LLM-Judge Violations:**
- [major] Missing Text Splitter connection to Document Loader. The user explicitly requested 'Use a Document Loader configured with Binary type to process the PDF documents' and the Document Loader should be connected to a Text Splitter via ai_textSplitter port. Currently, the Split Regulatory Text node connects directly to the Vector Store, bypassing the Document Loader's text splitting capability.; [minor] The user requested 'Set up a file upload trigger to receive the new policy draft document' but the workflow uses a webhook trigger instead of a more user-friendly file upload mechanism. While webhooks can handle file uploads, this is less optimal than a dedicated file trigger for the stated use case.; [minor] The user explicitly requested to 'Configure the Vector Store Retriever (n8n-nodes-langchain.retrieverVectorStore) to query for regulations' but the workflow uses the Vector Store in 'retrieve-as-tool' mode instead of using a separate Vector Store Retriever node. While functionally similar for the AI Agent use case, this deviates from the explicit node type requested.
- [major] Text Splitter connected to wrong target: 'Split Regulatory Text' connects to 'Store in Vector Database' via ai_textSplitter, but should connect to 'Load PDF Documents' (Document Loader) instead. Text Splitter provides chunking capability to the Document Loader, not directly to the Vector Store.
- [minor] In 'Email Compliance Report' node, the subject line uses outdated syntax `{{ $json.report_id }}` instead of modern n8n syntax `={{ $json.report_id }}`. While this may work in some contexts, the = prefix is the standard for expressions.
- [minor] HTTP Request node 'Fetch Regulatory PDFs' uses a placeholder URL which is expected for user configuration, but the placeholder format could be cleaner (contains descriptive text within the value); [minor] Google Sheets node uses placeholder values for documentId and sheetName which is expected, but these are required runtime configuration points that users must fill; [minor] Gmail node uses placeholder for sendTo email addresses which is expected user configuration, but this is a required field for the workflow to function
- [major] Missing data extraction from uploaded policy draft file - the AI Agent receives the webhook data but there's no explicit extraction of the binary file content or conversion to text that the AI can process. The agent prompt references 'the uploaded file' but doesn't show how the binary data is being passed or converted.; [minor] The Format Compliance Report node uses $input.first().json.output to extract AI response, but doesn't validate if the output field exists or handle potential errors in the AI response structure.; [minor] Google Sheets mapping uses autoMapInputData mode but then defines manual column mappings, which could cause confusion. The 'summary' field from the formatted report is not included in the sheet columns mapping.
- [minor] The workflow uses Gmail node for email notifications instead of the recommended Send Email (n8n-nodes-base.emailSend) node. According to notification best practices, Send Email is the recommended node for detailed alerts with attachments and HTML formatting, with proper SMTP configuration. While Gmail works, Send Email is the documented best practice for email notifications.; [minor] The Vector Store setup phase (preparation) is not properly isolated from the main workflow. Best practice for document processing suggests using separate workflows or clear separation for one-time setup operations versus recurring operations. The manual trigger for setup and webhook for main workflow are in the same workflow, which could lead to confusion about which parts run when.; [minor] Missing validation checkpoint after AI Agent processing. Document processing best practices recommend validation checkpoints including 'After AI parsing: Validate JSON schema' and 'Before database insert: Check required fields'. The workflow proceeds directly from AI Agent to formatting without validating that the AI output contains expected compliance findings.

**Programmatic Violations:**
- [critical] Node Store in Vector Database (@n8n/n8n-nodes-langchain.vectorStoreInMemory) received unsupported connection type ai_textSplitter; [critical] Node Load PDF Documents (@n8n/n8n-nodes-langchain.documentDefaultDataLoader) is missing required input of type ai_textSplitter
- [major] Agent node "Legal Compliance Officer AI" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-042-a476aff2
**Status:** pass | **Score:** 72.0%

**LLM-Judge Violations:**
- [minor] DB Failure Fallback node does not reconnect to the main workflow path after setting fallback values. The workflow should proceed to call Azure OpenAI even when the database check fails (as indicated by the fallback message 'Proceeding without idempotency check'), but there's no connection from this node to the AI processing path.
- [critical] DB Failure Fallback node (9dd821ba-6d03-41b3-b37b-afe3878ea9a2) is a dead end with no outgoing connections. The node sets 'fallback_action: Proceeding without idempotency check' but the workflow never proceeds to Call Azure OpenAI. This breaks the main execution path for database failure scenarios.; [critical] AI Timeout Fallback node (90a48b0b-93a4-4988-940b-f0c6110b9d44) is a dead end with no outgoing connections. The fallback response is generated but never returned or merged with other outputs, breaking the execution path for LLM timeout scenarios.; [minor] Duplicate connection from 'DB Query Successful?' node to 'Record Already Exists?' node. The connections array shows two identical connections from output 0 to the same target, creating unnecessary redundancy.; [minor] Return Cached Result node (059eaf98-8b1c-4e59-9de6-a912cb5603aa) has no outgoing connections, creating an inconsistent termination pattern. While this may be intentional, other success paths continue to Format Success nodes, making the workflow structure inconsistent.
- [major] Missing '=' prefix in 'Create Request with Idempotency Key' node - numberValue field uses '1' instead of '=1' or just 1 as a number; [major] In 'Check Existing Record (Supabase)' node, the URL expression references '$json.supabase_url' which is not defined in any previous node
- [minor] Check Existing Record (Supabase) node has placeholder table name in URL path that should be specified for production use; [minor] Store Result in Supabase node has placeholder table name in URL path that should be specified for production use; [minor] Azure OpenAI Model node has placeholder deployment name - should specify actual Azure OpenAI deployment name for production; [minor] Create Request with Idempotency Key node has placeholder user_prompt value - acceptable for template but should be populated from actual input in production
- [critical] Data loss in DB Failure Fallback path - the original request data (idempotency_key, user_prompt, timestamp) from 'Create Request with Idempotency Key' node is not preserved when DB check fails. The fallback node only adds error flags but doesn't include the original data needed for downstream AI processing.; [major] Field name mismatch in AI output transformation - 'Call Azure OpenAI' node outputs 'output' field, but 'Store Result in Supabase' expects this field in jsonBody. However, the 'Format Success (Not Stored)' node also references $json.output, suggesting inconsistent data structure assumptions between storage success/failure paths.; [major] Missing data propagation in 'DB Failure Fallback' to 'Call Azure OpenAI' path - when DB check fails, the workflow sets record_exists=false and proceeds to AI call, but the AI node needs user_prompt from earlier in the flow. The Set node uses 'include: all' but this data may not be available in the error context from the HTTP request node.; [major] Incomplete data in 'Store Result in Supabase' jsonBody - references $json.idempotency_key and $json.user_prompt which may not exist in the current context after AI processing. The AI agent node outputs different structure than what's being referenced.; [minor] Redundant dual connection from 'DB Query Successful?' true branch to 'Record Already Exists?' - both connections go to the same node with index 0, which is unnecessary and could cause confusion about data flow intent.; [minor] Missing supabase_url field propagation - 'Check Existing Record (Supabase)' references $json.supabase_url but this field is never set in 'Create Request with Idempotency Key' node, likely causing the HTTP request to fail.
- [minor] DB Failure Fallback node proceeds without idempotency check but doesn't connect to the AI processing path. The workflow has a fallback node that sets db_check_failed=true and record_exists=false, but this node doesn't have an output connection to continue processing. This means when the database check fails, the workflow stops at the fallback node instead of proceeding to call Azure OpenAI as the fallback message suggests ('Proceeding without idempotency check').

**Programmatic Violations:**
- None

### example-043-e44c22c0
**Status:** pass | **Score:** 96.2%

**LLM-Judge Violations:**
- [minor] HighLevel node uses 'create' operation but user requested to 'match the contact using the participant's email'. The 'upsert' operation would be more appropriate to match existing contacts or create new ones, rather than always creating new contacts.
- [major] Incorrect data reference in HighLevel node - uses $json.body.payload.object.participant.email but after AI Agent processing, the data structure changes. Should reference the original webhook data using $('Zoom Webhook').item.json.body.payload.object.participant.email; [major] Incorrect data reference for participant name fields - uses $json.body.payload.object.participant.user_name in HighLevel node, but this references the AI Agent output structure, not the original webhook data. Should use $('Zoom Webhook').item.json.body.payload.object.participant.user_name; [minor] Potential data loss in lastName split operation - if user_name has only one word, the slice operation will return an empty string. Should add validation or default handling; [minor] AI Agent output reference assumes 'output' field exists - while this is standard for AI Agent nodes, no explicit validation that the summary was successfully processed before passing to HighLevel
- [minor] The HighLevel node is configured with operation 'create' which may create duplicate contacts if the same participant email is received multiple times. Best practice for data persistence suggests using unique identifiers or upsert logic to avoid duplicates. However, this is a minor issue since the user specifically requested 'create' operation and mentioned 'matches the contact using the participant's email', which suggests they may be aware of this behavior or have external deduplication logic.

**Programmatic Violations:**
- None

### example-044-d9883dc1
**Status:** pass | **Score:** 92.0%

**LLM-Judge Violations:**
- [major] Missing web crawling capability - user explicitly requested an agent that 'crawls the web', but workflow only uses Google Search API which returns search result snippets, not full web page content. No HTTP Request or web scraping nodes are present to actually crawl and extract content from news articles.
- [minor] In 'Startup Funding Agent' node, the 'text' parameter uses an unnecessarily complex expression format. The entire multi-line string is wrapped in ={{ "..." }} when it could be simplified to just ="..." since it's a static string with no dynamic expressions embedded.
- [minor] Google Sheets node uses placeholder values for documentId and sheetName, which are expected to be configured by user at runtime. These are valid placeholders given no specific values were provided in the request.; [minor] Extract Startup Data tool node has minimal extraction logic - the jsCode simply returns the input data wrapped in JSON without actual parsing or extraction logic, making it less useful than intended.
- [minor] The 'Extract Startup Data' tool node provides minimal value - it just wraps the input in JSON without meaningful extraction logic. The AI agent could handle this directly.; [minor] The 'Format for Sheets' Code node contains extensive fallback parsing logic that duplicates data extraction work the AI agent should already be doing with proper prompting.; [minor] The workflow could benefit from a validation step before saving to sheets to filter out parse errors or invalid entries, avoiding redundant manual cleanup later.
- [major] The 'Extract Startup Data' tool code is essentially a no-op that just wraps the input in a JSON object with 'extracted: true' and 'data: data'. It doesn't actually parse or extract structured information from search results, making it ineffective for its stated purpose.; [major] The 'Format for Sheets' node attempts to parse JSON from the AI agent output using regex matching, but this is fragile and may fail if the AI returns data in a slightly different format. The fallback logic creates placeholder data rather than failing gracefully or requesting proper formatting.; [minor] Field mapping inconsistencies in 'Format for Sheets' node - it tries multiple field name variations (company_name vs name, funding_amount vs amount, funding_round vs round) which suggests uncertainty about the data structure. While this provides flexibility, it indicates potential data structure issues.; [minor] The error handling in 'Format for Sheets' creates dummy entries with values like 'Parse Error', 'Data extraction needed', and 'See description' instead of properly handling failures. This could result in meaningless data being saved to Google Sheets.
- [minor] Missing rate limiting/batching for search operations: The workflow uses Google Search tool with maxIterations set to 15, which could trigger multiple search requests in rapid succession. Best practices documentation states 'Batch requests and introduce delays to avoid hitting API rate limits' and 'When 429 rate limiting errors occur, implement batching or use Retry on Fail feature.' For a production news scraper, this could hit SerpAPI rate limits.; [minor] Using Manual Trigger instead of Schedule Trigger: The user requested a workflow that 'crawls the web' and finds startups from 'the last 24 hours', which implies this should run automatically on a schedule (e.g., daily). The workflow uses a Manual Trigger, requiring manual execution each time. While functional, this doesn't align with the automated nature implied by the request.; [minor] Placeholder values in Google Sheets node: The Save to Google Sheets node contains placeholder values ('<__PLACEHOLDER_VALUE__...>') for documentId and sheetName. While this is acceptable for a template, best practices for data persistence state 'use ResourceLocator mode list to allow users to select from existing documents' rather than requiring manual ID entry. This makes the workflow less user-friendly.

**Programmatic Violations:**
- None

### example-045-0adfa048
**Status:** pass | **Score:** 94.7%

**LLM-Judge Violations:**
- [minor] Google Sheets nodes use placeholder values for documentId and sheetName, but this is expected since user didn't provide specific sheet IDs or names
- [minor] ID comparison uses String() conversion which could cause issues if IDs are strictly numeric in one sheet and string in another, though this is actually a defensive practice; [minor] The output structure nests original data under 'sheet1_data' and 'sheet2_data' which creates data duplication (id appears three times: at root level and in both nested objects); [minor] No handling for case where no matches are found - returns empty array which is correct but could benefit from explicit messaging
- [minor] The workflow uses a Code node to perform a merge/join operation on two datasets based on the 'id' field. According to best practices, the Merge node (n8n-nodes-base.merge) is the recommended approach for combining two data streams, specifically using 'Merge by Key' mode which functions like a database join. The documentation states: 'Merge Node: Purpose - Combine two data streams. Modes: Merge by Key (like database join)'. While the Code node implementation is functional, using the dedicated Merge node would be more maintainable and align with n8n's recommended patterns.; [minor] The workflow does not include early data validation or filtering to check if the 'id' field exists in the input data before processing. Best practices recommend: 'Use IF/Filter nodes early to validate inputs and remove bad data' and 'Validate external data before processing: check for nulls, empty values, and edge cases'. While the Code node does check for undefined/null IDs, using an IF or Filter node earlier in the workflow would follow the recommended pattern of filtering data early to improve performance and clarity.

**Programmatic Violations:**
- None

### example-046-b8ee8b1f
**Status:** pass | **Score:** 84.7%

**LLM-Judge Violations:**
- [major] Missing critical data merging logic between Snowflake results and Asana ticket data. The Update Asana node attempts to use fields directly from Snowflake results ($json.assignee, $json.due_on) but doesn't properly merge them with the original ticket data that contains taskId. This would cause the workflow to fail or update with incorrect data.; [minor] The workflow hardcodes specific fields to check (assignee, due_on, notes) in the analysis code without any flexibility, which may not match the actual missing information the user needs to detect. While functional, this assumes what 'missing information' means without clear specification.
- [major] In 'Update Asana Ticket with Missing Data' node, the expression `={{ $json.taskId }}` references a field from the 'Analyze Ticket for Missing Data' node, but the immediate input comes from 'Find Missing Data in Snowflake' which likely has different fields. Should use `={{ $('Analyze Ticket for Missing Data').item.json.taskId }}` to reference the correct node.
- [major] Asana Trigger node missing 'events' parameter - required to specify which events to monitor (e.g., task created, task updated). Without this, the trigger won't know what events to listen for.
- [critical] Data loss in Update Asana node: The node references $json.taskId from Snowflake output, but taskId only exists in the analysis node output. After Snowflake query, the original task data with taskId is no longer accessible in $json context.; [major] Field mapping mismatch: Snowflake query returns fields (assignee, due_date, description) but Update node expects different field names (assignee, due_on, notes). The due_date vs due_on inconsistency will cause data loss.; [major] Missing data merge logic: The workflow doesn't merge Snowflake data with original task data. The Update node tries to access fields from Snowflake output that may not exist, and loses context of which fields were actually missing.; [minor] Inefficient data structure: The missingFields array is calculated but never used to selectively update only the missing fields, potentially overwriting existing data with null values.
- [minor] The workflow uses a Code node to analyze missing data, but doesn't properly handle the data structure transformation needed for the Snowflake query. The Code node outputs a modified object but doesn't ensure the data structure is normalized for downstream processing. According to best practices: 'Normalize data structure early in your workflow. Use transformation nodes like Split Out, Aggregate, or Set to ensure your data matches n8n's expected structure.'; [minor] The Snowflake query node uses a placeholder query that doesn't demonstrate proper handling of the missing fields array. The workflow should map the specific missing fields identified in the analysis step to construct an appropriate query. Best practices state: 'Use consistent field names across your workflow for easy mapping' and 'Plan your schema ahead.'; [minor] The Update Asana node attempts to reference data from both the current item ($json) and the previous Snowflake result ($input.first().json) without proper data merging. This mixing of data sources could lead to incorrect field mappings. Best practices warn: 'Pay attention to data types - mixing types causes unexpected failures' and recommend using Set/Edit Fields nodes for proper data transformation.

**Programmatic Violations:**
- None

### example-047-707fa7a7
**Status:** pass | **Score:** 81.9%

**LLM-Judge Violations:**
- [minor] In 'Save to Google Sheets' node, using string literals with = prefix for expressions (e.g., '={{ $json.firstName }}') is valid but the = prefix is optional when the entire value is an expression. This is a style preference and doesn't affect functionality.
- [minor] Google Sheets node uses 'autoMapInputData' for dataMode but then defines columns manually with 'defineBelow' mappingMode - this is contradictory. Should use dataMode: 'defineBelow' instead for consistency with the manual column mapping.; [minor] Airtop scrape node URL parameter uses a verbose placeholder format that may confuse users - standard <UNKNOWN> or empty string would be clearer; [minor] Gmail node optional parameters (senderName, replyTo) use placeholder values when these are truly optional and could be omitted entirely
- [minor] Two separate Code nodes for data transformation could potentially be consolidated into one node that both parses and formats the email content; [minor] Email formatting could be integrated into the Google Sheets node using expressions, eliminating the need for a separate Code node
- [critical] Scrape Lead Data node outputs raw scraped content, but Parse Lead Information assumes a structured JSON object with specific field names (firstName, lastName, etc.). The Airtop scraper likely returns markdown or unstructured HTML content, not pre-structured fields, causing complete data mapping failure.; [major] No validation or error handling for missing email addresses before sending emails. If email field is empty or invalid, the Send Email node will fail without graceful degradation.; [major] Google Sheets node uses 'appendOrUpdate' with 'email' as columnToMatchOn, but doesn't handle cases where email is empty/null, potentially causing duplicate entries or update failures.; [major] Parse Lead Information code assumes scraped data has specific field names (scrapedContent.firstName, scrapedContent.company, etc.) but provides no actual parsing logic for unstructured content. This will result in all fields being empty strings.; [minor] Format Personalized Email node passes through all lead data (...lead) plus email fields, but this creates redundant data that isn't needed by the Gmail node, causing unnecessary data bloat.; [minor] No data type validation for fields like mobile/phone numbers, companySize, or URLs. Invalid formats could be saved to Google Sheets without sanitization.; [minor] The workflow doesn't handle array/multiple results from scraping. If Airtop returns multiple leads, the data flow doesn't properly iterate or aggregate them.
- [major] Missing rate limiting and batching for scraping operations. Best practices explicitly state: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers. Use Wait nodes and batching options.' The workflow scrapes data and immediately sends emails without any rate limiting, which could result in 429 errors or account suspension, especially when processing multiple leads.; [minor] No batch processing for large datasets. Best practices recommend: 'Use the Split In Batches node to process 200 rows at a time to reduce memory usage.' The workflow processes all scraped leads sequentially without batching, which could cause memory issues if scraping many leads (e.g., entire dental clinic directories).; [minor] Missing error handling for email sending. The workflow sends emails to all scraped leads without checking for invalid emails or handling send failures. This could cause the workflow to fail partway through processing leads, losing progress.

**Programmatic Violations:**
- None

### example-048-04a10a2f
**Status:** pass | **Score:** 97.3%

**LLM-Judge Violations:**
- [major] Code node references 'item.json.subject' but Text Classifier node output likely doesn't preserve the original email subject field - only outputs classification results (category, confidence). This will cause the subject to be undefined in the log.; [minor] Text Classifier input concatenates subject and textPlain with newlines, but doesn't validate if textPlain exists (could be undefined for HTML-only emails), potentially passing 'undefined' string to classifier.; [minor] No error handling in Code node if classification result structure differs from expected format (missing category field).
- [minor] Text Classifier node is missing the 'When No Clear Match' configuration to handle items that don't match any category. Best practice states: 'In Text Classifier node, set "When No Clear Match" to "Output on Extra, Other Branch" to capture unmatched items.' However, since the user requested a simple workflow to log results and didn't specify handling edge cases, this is a minor issue.; [minor] No fallback path or error handling for unclassified items. Best practice states: 'CRITICAL: Always include a default/fallback path to catch unclassified items. Never allow data to drop silently.' However, the user only requested to 'log' the result without specifying what to do with edge cases, making this a minor violation for the stated requirements.

**Programmatic Violations:**
- None

### example-049-5b171809
**Status:** pass | **Score:** 93.1%

**LLM-Judge Violations:**
- [minor] Unnecessary use of expression wrapper for static string in 'Generate Complete Ebook' node. The entire 'text' parameter is wrapped in ={{ "..." }} when it's just a static string that doesn't need the = prefix or expression evaluation.
- [minor] Agent node 'maxIterations' set to 3 is unnecessarily low for a complex ebook generation task. Since the agent needs to generate complete ebook content in one go without tools, a higher value (10-15) would be safer to prevent premature termination, though 3 may work for this single-shot generation.
- [minor] Title extraction regex could fail if AI formats the title differently than expected (e.g., without asterisks or colon), resulting in fallback to generic 'My Ebook' filename; [minor] Filename sanitization truncates at 100 characters which could result in non-unique filenames if multiple ebooks have similar long titles; [minor] Binary data conversion uses base64 encoding correctly but doesn't validate that the ebook content exists or has minimum length before processing; [minor] The workflow removes all markdown bold markers (**) globally which could affect intentional formatting in the ebook content if the AI uses bold for emphasis
- [minor] Missing data persistence for ebook content: The workflow uploads to Google Drive but doesn't store metadata (title, creation date, topic) in a persistent storage system like Data Table. Best practices recommend storing workflow results for audit trails and record keeping. While the user requested file storage, maintaining a record of generated ebooks would enable tracking what's been created and prevent duplicate topics.; [minor] No error handling on Google Drive upload: The workflow lacks error handling if the Google Drive upload fails. While the user requested a 'simple' workflow, the best practices documentation emphasizes handling errors gracefully to prevent data loss. If the upload fails, the generated ebook content would be lost without any fallback or notification.

**Programmatic Violations:**
- [major] Agent node "Generate Complete Ebook" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-050-185ccbfb
**Status:** pass | **Score:** 82.1%

**LLM-Judge Violations:**
- [major] Missing explicit data input mechanism for job description, resume text, job URL, and company name. The workflow references $json.job_description, $json.resume_text, $json.job_url, and $json.company_name but provides no way to supply these inputs. The manual trigger doesn't capture or provide this data, making the workflow non-functional on first execution.
- [critical] Invalid string concatenation syntax in the 'text' parameter. Uses JavaScript string concatenation (+ operator) inside an expression block without proper syntax. Should use template literals or proper n8n expression syntax like `={{ 'text' + $json.field }}` or string interpolation.
- [minor] Agent node prompt uses complex string concatenation with $json references that could be more elegantly structured, though functionally correct
- [critical] No input data structure defined - workflow expects fields like $json.job_description, $json.resume_text, $json.job_url, and $json.company_name but the manual trigger provides no mechanism to capture this data; [critical] No output data transformation - AI agent returns JSON text but there's no parsing or structuring of the response into usable workflow data for downstream processing; [major] Missing data validation - no checks to ensure required input fields (job_description, resume_text) are present before processing; [major] No error handling for malformed AI responses - if the AI returns invalid JSON or doesn't follow the expected structure, the workflow will fail silently; [minor] Inefficient data embedding - the entire prompt with data is constructed as a single string expression, making it difficult to debug or modify individual components; [minor] Missing data enrichment - no timestamp generation for 'Last_Checked' or 'Applied_Date' fields that could be automatically populated
- [critical] Entire workflow logic embedded in a single massive prompt string - violates modularity and single responsibility principle. The prompt contains filtering, ATS scoring, JSON formatting, and business rules all in one node, making it impossible to modify individual components.; [major] No separation of concerns - job filtering criteria, ATS analysis, and output formatting are all tightly coupled in one prompt. Changes to any single aspect require modifying the entire prompt string.; [major] No input validation or data preparation nodes - resume text, job description, and other inputs are directly referenced without validation, sanitization, or preprocessing.; [minor] Missing error handling nodes - no mechanism to handle cases where AI fails, returns invalid JSON, or encounters processing errors.; [minor] No output processing or validation - the workflow assumes AI will always return perfectly formatted JSON without verification or fallback handling.
- [major] Missing data input mechanism: The workflow uses a manual trigger but doesn't provide any way to input the required data (job_description, resume_text, job_url, company_name). The AI Agent references $json.job_description, $json.resume_text, $json.job_url, and $json.company_name, but these fields are never populated. According to data extraction best practices, the workflow should validate external data before processing and ensure proper input structure. This prevents the core functionality from working.; [minor] Suboptimal node choice for structured data extraction: The workflow uses an AI Agent for what is essentially a structured data extraction task. According to data extraction best practices, the Information Extractor node is specifically designed for 'AI-powered extraction of structured data from unstructured text' and requires 'proper schema definition for extraction'. This would be more appropriate than an AI Agent for this use case, as it's purpose-built for extracting structured data with defined schemas.; [minor] Missing output handling: The workflow doesn't include any nodes to handle or store the structured JSON output from the AI Agent. According to data persistence best practices, when generating structured data that needs to be tracked (the user mentions 'tracking sheet'), the workflow should include storage nodes like Data Table or Google Sheets. The user explicitly requests 'Tracking Sheet Fields (Table-Compatible JSON)' suggesting the output should be stored somewhere.

**Programmatic Violations:**
- [major] Agent node "LinkedIn Job Analyzer" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-051-692d66c6
**Status:** pass | **Score:** 89.5%

**LLM-Judge Violations:**
- [minor] In 'Fetch Channel 1 History', 'Fetch Channel 2 History', and 'Fetch Channel 3 History' nodes: The 'oldest' filter uses '={{ Math.floor(Date.now() / 1000) - 86400 }}' which is valid but could use n8n's built-in $now helper for better readability (e.g., '={{ Math.floor($now.toMillis() / 1000) - 86400 }}'). This is a style preference and the current syntax works correctly.
- [minor] Slack fetch nodes missing 'resource' and 'operation' parameters - should explicitly specify resource='channel' and operation='history' for clarity, though the node may have defaults; [minor] Post Summary to Channel node missing 'resource' parameter - should explicitly specify resource='message' and operation='post' for clarity
- [major] Missing resource parameter in Slack fetch nodes - the 'resource' parameter should be set to 'message' and 'operation' to 'getAll' for channel history retrieval. Without proper resource configuration, the nodes may not function correctly.; [minor] Potential data loss when channels have no messages - the Code node doesn't handle empty message arrays gracefully, which could result in an empty string being passed to the AI agent without proper context.; [minor] Missing error handling for timestamp parsing - the Code node uses parseFloat(item.json.ts) without validation, which could fail if the timestamp format is unexpected or missing.; [minor] Inefficient data structure in Merge node - using 'combineByPosition' may not be optimal for variable-length message arrays from different channels. 'combineAll' or 'mergeByIndex' would better preserve all messages regardless of position alignment.
- [minor] Using AI Agent node instead of Summarization Chain for text summarization. Best practices recommend using Summarization Chain (@n8n/n8n-nodes-langchain.chainSummarization) specifically for summarizing large text blocks. The AI Agent is more complex and designed for interactive tasks with tools, while Summarization Chain is optimized for this exact use case of condensing text content.; [minor] Missing batch processing for potentially large datasets. Best practices state that datasets over 100 items should use Split In Batches node to prevent timeouts. With 3 channels fetching up to 100 messages each (300 total potential messages), this could cause performance issues or timeouts during processing.

**Programmatic Violations:**
- [major] Merge node "Combine All Messages" has only 1 input connection(s). Merge nodes require at least 2 inputs.

### example-052-71253a49
**Status:** pass | **Score:** 73.9%

**LLM-Judge Violations:**
- [critical] Missing the entire preparation/one-time vector store loading workflow. User explicitly requested a one-time preparation step to 'Load a Vector Store (in memory) with PDF embeddings of relevant regulatory acts' (GDPR, Municipal Codes). The workflow has no mechanism to populate the vector store with regulatory documents.; [major] The 'Load Policy Draft' Document Loader node is incorrectly placed in the main execution flow. This node should only be used in the preparation phase to load regulatory documents into the vector store, not to process the uploaded policy draft during runtime.; [major] Missing actual file upload mechanism. User specified 'Trigger: File Upload (The new policy draft)' but the workflow uses a manual trigger with no file upload capability. There's no way for users to upload the policy draft file.
- [critical] Missing Vector Store infrastructure for RAG preparation phase. The workflow requires a Vector Store node (insert mode) with Document Loader, Embeddings, and Text Splitter connected via ai_* connections to load regulatory documents. The Regulatory Database Retriever cannot function without a Vector Store to query.; [critical] Regulatory Database Retriever (retrieverVectorStore) has no ai_vectorStore connection from a Vector Store node. It cannot retrieve regulatory context without being connected to an actual vector database that contains the embedded regulatory documents.; [major] Document Loader 'Load Policy Draft' (documentDefaultDataLoader) is incorrectly placed in the main execution path. Document Loaders are capability-only nodes that should ONLY connect via ai_document connections, never in the main data flow. The policy draft should flow directly to the Compliance Analysis node via main connections.
- [major] Missing '=' prefix in 'Load Policy Draft' node metadata value: '={{ $now.toISO() }}' should be '={{ $now.toISO() }}'; [critical] In 'Compliance Analysis' node, the text parameter uses placeholder syntax but references '$json.pageContent' which may not exist after the document loader - document loaders output documents with 'pageContent' but the actual field path needs verification; [minor] In 'Format Compliance Report' node, 'policy_name' references '{{ $json.fileName }}' which may not exist in the output from the Compliance Analysis chain - the field name should match the actual output structure; [major] In 'Format Compliance Report' node, 'compliance_status' expression references '$json.output' but the chainRetrievalQa node outputs 'text' not 'output' - should be '{{ $json.text.includes(...) }}'; [minor] In 'Format Compliance Report' node, 'analysis_summary' references '$json.output' but should be '$json.text' to match the actual output field from chainRetrievalQa
- [major] Compliance Analysis node has malformed 'text' parameter with placeholder wrapper syntax '<__PLACEHOLDER_VALUE__...>'. Should be either a clean expression like '={{ $json.pageContent }}' or a simple placeholder value without the wrapper syntax.
- [critical] Missing binary file upload mechanism - Manual Trigger doesn't provide file upload capability, causing complete data loss at workflow start. The workflow expects binary data but has no way to receive it.; [major] Incorrect data reference in Compliance Analysis node - uses placeholder '{{ $json.pageContent }}' but the documentDefaultDataLoader outputs an array of document objects, not a single JSON with pageContent. Should reference the document array properly or extract text from all documents.; [major] Data structure mismatch in Format Compliance Report - references '$json.output' but the chainRetrievalQa node outputs '$json.text' as the response field, causing the analysis summary to be empty or undefined.; [major] Missing fileName field reference - Format Compliance Report tries to access '{{ $json.fileName }}' which doesn't exist in the data flow from the document loader or compliance analysis nodes.; [minor] Compliance status detection logic is fragile - simple string matching for 'non-compliant' or 'violates' may miss variations in AI output format (e.g., 'does not comply', 'fails to meet', 'contradicts').
- [minor] One generic node name 'Format Compliance Report' uses base 'set' node type - could be more specific like 'Structure Report for Sheets'; [minor] Minor organization improvement: AI sub-nodes (Gemini, Retriever) could benefit from visual grouping or sticky notes to clarify they're dependencies of the main chain
- [major] Missing Vector Store node: The workflow includes a 'Regulatory Database Retriever' (retrieverVectorStore) but no actual Vector Store node (e.g., In-Memory Vector Store) to store the regulatory document embeddings. The user explicitly requested 'Load a Vector Store (in memory for learning purpose only) with PDF embeddings of relevant regulatory acts' in the preparation phase. Without a Vector Store node, the retriever has nothing to retrieve from, breaking the core RAG functionality.; [major] Missing Document Loader and Embeddings for Vector Store preparation: The workflow lacks the preparation workflow/nodes to load regulatory PDFs into the vector store. Best practices for document processing recommend using Document Loader nodes to process PDFs and connect them to Vector Stores via ai_document connections. The user specifically requested a 'Preparation (One-time)' phase to load regulatory acts, but there's no mechanism to populate the vector store with GDPR, municipal codes, etc.; [minor] Incorrect trigger node for file upload: The workflow uses 'manualTrigger' instead of a proper file upload mechanism. Best practices for document processing recommend using Webhook with file upload capability or Form Trigger for receiving file uploads. The user requested 'Trigger: File Upload (The new policy draft)' but the manual trigger doesn't provide file upload functionality.; [minor] Missing file type validation: Best practices for document processing emphasize 'ALWAYS check the file type before using Extract from File node' and implementing branching based on file type. The workflow assumes PDF input without validation, which could cause errors if other file types are uploaded.; [minor] Suboptimal data flow in Compliance Analysis node: The 'text' parameter uses a placeholder for policy draft text, but the workflow already has a Document Loader node that could provide this data. The Document Loader should connect directly to the chain, or the extracted text from Load Policy Draft should be properly referenced. This creates confusion about the actual data flow.

**Programmatic Violations:**
- [critical] Node Load Policy Draft (@n8n/n8n-nodes-langchain.documentDefaultDataLoader) received unsupported connection type main; [critical] Sub-node Load Policy Draft (@n8n/n8n-nodes-langchain.documentDefaultDataLoader) provides ai_document but is not connected to a root node.; [critical] Node Regulatory Database Retriever (@n8n/n8n-nodes-langchain.retrieverVectorStore) is missing required input of type ai_vectorStore

### example-053-72752121
**Status:** pass | **Score:** 89.6%

**LLM-Judge Violations:**
- [minor] The workflow includes a separate 'Fetch Tableau Site Details' step, but the user stated they already have a URL that can fetch site ID and other important details. This creates an unnecessary extra step when the site details could be fetched as part of the authentication or data retrieval process.; [minor] The authentication token from 'Authenticate with Tableau PAT' is referenced in 'Fetch Tableau Data' but the workflow doesn't explicitly handle passing the site ID obtained from the first step to subsequent steps where it might be needed in the data endpoint URL.
- [major] In 'Fetch Tableau Data' node: Expression '={{ $json.credentials.token }}' references data from previous node without specifying the node name. Should be '={{ $('Authenticate with Tableau PAT').item.json.credentials.token }}' for clarity and reliability
- [minor] Google Sheets node 'columnToMatchOn' parameter has placeholder value when 'appendOrUpdate' operation is selected. While this is a valid placeholder pattern, it could cause issues if not configured before execution. However, since user didn't specify the column, this is acceptable.; [minor] HTTP Request nodes use 'authentication: none' when PAT authentication should ideally be configured. However, since the workflow manually constructs the authentication flow (which is a valid approach for PAT), this is a minor style preference issue rather than a functional problem.
- [minor] The 'Fetch Tableau Site Details' node may be redundant if the site ID is already known or can be hardcoded, as the user mentioned having a URL with site ID details; [minor] Authentication could potentially be combined with the first data fetch operation if the API supports it, reducing one HTTP request; [minor] The Code node for formatting could potentially be eliminated if Google Sheets node's auto-mapping handles the Tableau response structure directly
- [major] Authentication token extraction path is incorrect - uses '{{ $json.credentials.token }}' but Tableau API returns token in a nested structure (typically 'credentials.token' or 'token' at root level). This will likely cause the Fetch Tableau Data node to fail with authentication errors.; [major] Site details from first HTTP request are not utilized in subsequent nodes. The workflow fetches site ID but never references it in the authentication or data fetch steps, causing potential data flow disconnection.; [major] Format Data for Sheets node assumes a specific data structure without validation. The code accesses 'item.json.id', 'item.json.name', etc., but Tableau API responses vary significantly by endpoint (views, workbooks, datasources). Missing data validation could cause silent data loss or empty rows.; [minor] No error handling for empty or malformed Tableau responses. If the API returns no data or unexpected structure, the formatting node will produce empty arrays without indication of the issue.; [minor] Google Sheets node uses 'appendOrUpdate' with 'columnToMatchOn' but the placeholder doesn't specify which field to use. If the column doesn't exist or doesn't match the formatted data fields, updates will fail and data will be duplicated.
- [minor] Using HTTP Request nodes for Tableau API authentication instead of leveraging proper credential management. The workflow hardcodes PAT credentials in the request body as placeholders rather than using n8n's credential system, which is less secure and harder to maintain.; [minor] Missing error handling for API authentication failures. If the Tableau PAT authentication fails or the token expires, the workflow will fail without graceful handling. While the user didn't explicitly request error handling, authentication is a critical step that commonly fails and should have basic validation.; [minor] No batching implemented for potentially large Tableau datasets. The workflow fetches all data at once and processes it in a single operation, which could cause memory issues if the Tableau data is large. Best practices for data extraction recommend batching datasets over 100 items, but since the user didn't specify data volume, this is a minor concern.; [minor] Code node uses generic field extraction without validation. The Format Data for Sheets node assumes specific field names (id, name, createdAt, updatedAt) exist in the Tableau response without checking if they're present, which could lead to empty or incorrect data being written to Google Sheets.

**Programmatic Violations:**
- None

### example-054-86ae5604
**Status:** pass | **Score:** 92.0%

**LLM-Judge Violations:**
- [major] Outdated syntax in 'Send Analysis to Telegram' node: `$node["Telegram Trigger"]` should use modern syntax `$('Telegram Trigger')` instead
- [minor] The Gemini AI node output structure assumes 'message.content' field exists. While this is likely correct for the simplified output format, there's no explicit validation or error handling if the API response structure differs.; [minor] No data validation to ensure the Telegram trigger actually contains a message.text field before passing to Gemini. If a user sends a non-text message (photo, sticker, etc.), the workflow could fail or send undefined data.
- [major] Using Google Gemini node (@n8n/n8n-nodes-langchain.googleGemini) instead of AI Agent node for chatbot workflow. Best practices explicitly state: 'Unless user asks for a node by name, always use the AI Agent node over provider-specific nodes (like OpenAI, Google Gemini) or use-case-specific AI nodes (like Message a model) for chatbot workflows. The AI Agent node provides better orchestration, tool integration, and memory management capabilities essential for conversational interfaces.' The user requested a chatbot agent, not specifically a Gemini node.; [major] Missing memory management for chatbot. Best practices state: 'Always utilise memory in chatbot agent nodes - providing context gives you full conversation history and more control over context. Memory nodes enable the bot to handle follow-up questions by maintaining short-term conversation history.' Without memory, the bot cannot maintain conversation context across multiple messages, which is essential for a chatbot experience.; [minor] Workflow correctly disables n8n attribution (appendAttribution: false) in the Telegram send node, following the best practice: 'n8n chatbots often attach the attribution "n8n workflow" to messages by default - you must disable this setting.' This is properly implemented.; [minor] Missing error handling for the workflow. While the user requested a simple bot, basic error handling (IF node to check for valid stock ticker input, or handling API failures) would improve reliability. However, since the user didn't explicitly request production-ready features, this is a minor issue.

**Programmatic Violations:**
- None

### example-055-71ce5256
**Status:** error | **Score:** 26.5%

**LLM-Judge Violations:**
- None

**Programmatic Violations:**
- [critical] Failed to resolve connections for node config (@n8n/n8n-nodes-langchain.agent): Expression did not resolve to an array
- [major] Agent node "config" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context; [major] Agent node "config" has no system message. System-level instructions (role, tasks, behavior) should be in the system message field, not the text field

### example-056-a5f95536
**Status:** pass | **Score:** 90.6%

**LLM-Judge Violations:**
- [minor] Gemini node uses 'resource' and 'operation' parameters which are not standard for @n8n/n8n-nodes-langchain.googleGemini node type. The correct parameter structure should use 'options' for model configuration without resource/operation fields.
- [major] Gemini node configuration issue: The node is configured as '@n8n/n8n-nodes-langchain.googleGemini' with 'resource: image' and 'operation: analyze' parameters, but this appears to be an incorrect node type/configuration. The LangChain Gemini node doesn't have these resource/operation parameters. Should use the standard Google Gemini node or configure LangChain properly with a chat model that accepts images.; [minor] Success response references '{{ $json.response }}' but the actual output field from Gemini nodes may vary. Without proper testing, this field mapping could be incorrect. Should verify the exact output structure from the Gemini node (typically 'text' or 'output' rather than 'response').
- [minor] Using non-recommended Google Gemini node type for image analysis. The workflow uses '@n8n/n8n-nodes-langchain.googleGemini' node with 'resource: image' and 'operation: analyze', but this node type/configuration doesn't exist in n8n. The correct approach per best practices is to use the Google Gemini Chat Model (@n8n/n8n-nodes-langchain.lmChatGoogleGemini) for image analysis and multimodal content, or use a Document Loader with AI Agent pattern for binary file processing.

**Programmatic Violations:**
- None

### example-057-10d88cfb
**Status:** fail | **Score:** 66.9%

**LLM-Judge Violations:**
- [critical] User explicitly requested to 'Use a Gemini agent to do this' for categorization, but the workflow uses hardcoded JavaScript logic in the 'Determine Category' node instead of any AI/Gemini agent
- [critical] Missing AI Agent node: User explicitly requested 'Use a Gemini agent to do this' for file categorization, but no AI Agent node exists in the workflow. The categorization is performed by hardcoded JavaScript logic in a Code node instead of intelligent AI-based decision making.; [critical] Missing Gemini language model integration: No Google Gemini Chat Model node is present to provide AI capabilities. The workflow should include a Gemini language model connected to an AI Agent via ai_languageModel connection to enable intelligent file categorization as requested.
- [minor] Node 'Prepare File and Folder Data' uses $input.first() and $input.last() which are valid but could be more explicit about which input branch is being accessed in a merge scenario
- [critical] User explicitly requested a Gemini agent to categorize files, but workflow uses hardcoded JavaScript logic in 'Determine Category' node instead of any AI agent or Gemini model
- [critical] User requested Gemini agent for categorization, but workflow uses hardcoded JavaScript logic instead. This completely ignores the AI requirement and creates a fundamentally different solution.; [major] Three separate Code nodes (Prepare File and Folder Data, Extract Files to Process, Determine Category) perform sequential data transformations that could be consolidated into 1-2 nodes, creating unnecessary intermediate steps.; [major] Workflow creates folders every run (Create Category Folder node) without checking if they already exist, leading to duplicate folders or errors on subsequent runs.; [minor] The 'Define 10 Categories' Code node outputs multiple items only to have them processed by Create Category Folder, then immediately re-fetched with 'Get All Category Folders'. This fetch could be avoided by storing the created folder IDs.
- [critical] User requested Gemini agent for categorization, but workflow uses hardcoded JavaScript logic instead. This completely bypasses the AI-based intelligent categorization requirement.; [major] Merge node uses 'combineByPosition' which will fail when folder count (10) doesn't match file count. This will cause data loss as items beyond the shorter list are dropped.; [major] Workflow only processes files in root folder, ignoring files in existing subfolders. This violates the requirement that 'all files must be organized' and 'no files remain ungrouped'.; [major] The 'Prepare File and Folder Data' node uses $input.first() and $input.last() incorrectly. With combineByPosition merge, all items are in a single array, not separate inputs. This will cause incorrect data extraction.; [major] Category folders are created every run without checking if they already exist, potentially creating duplicates or failing on subsequent runs.; [minor] The folderMap is duplicated in every file item in 'Extract Files to Process', causing unnecessary data bloat when processing many files.
- [minor] The workflow doesn't use the Gemini agent as specified in requirements - categorization is done with hardcoded logic instead of AI; [minor] Code nodes could be more modular - 'Determine Category' contains complex categorization logic that could be extracted or simplified; [minor] No error handling nodes visible - workflow could fail silently if API calls fail or files can't be moved
- [major] User explicitly requested 'Use a Gemini agent to do this' for categorization, but the workflow uses hardcoded JavaScript logic in the 'Determine Category' Code node instead of an AI Agent. This violates the core requirement and misses the opportunity to use AI for intelligent, context-aware categorization that could handle edge cases and ambiguous file types better than rigid if-else logic.; [major] The workflow only processes files in the root folder ('List All Files in Root' node searches only in root), but Google Drive typically has files scattered across multiple folders and subfolders. This means most files will remain ungrouped, directly violating the user's requirement that 'no files remain ungroupedall files must be organized into folders.' The workflow needs recursive folder traversal or a broader search strategy.

**Programmatic Violations:**
- [major] Merge node "Combine Folders and Files" has only 1 input connection(s). Merge nodes require at least 2 inputs.

### example-058-5b18c714
**Status:** pass | **Score:** 79.8%

**LLM-Judge Violations:**
- [critical] Missing Google Docs ingestion workflow - User explicitly stated rules are in 2-4 Google Docs, but there is no mechanism to load these documents into the vector store. The workflow uses an in-memory vector store in 'retrieve' mode with no corresponding 'insert' workflow to populate it with the Google Docs content.
- [critical] Missing document ingestion pipeline: The Rules Vector Store is in 'retrieve' mode with memoryKey 'eco_community_rules', but there is no corresponding Vector Store in 'insert' mode to populate this memory with the Google Docs content. The workflow cannot answer questions about community rules because the vector store will be empty. A separate workflow or nodes are needed to: (1) fetch Google Docs content, (2) use a Vector Store in insert mode with the same memoryKey, (3) connect Document Loader and Text Splitter via ai_* connections to process the documents.
- [minor] In 'Format Discord Response' node: Accessing items[0].json.channelId and items[0].json.guildId, but these fields were not passed through from the 'Answer Question' node. The channelId and guildId exist in the original 'Extract Discord Message' output but may not be available after the AI chain processes the data.
- [minor] Discord Webhook node uses placeholder path '<__PLACEHOLDER_VALUE__webhook-path-for-discord-bot__>' - user should configure a specific webhook path for Discord bot integration; [minor] Format Discord Response code node attempts to access channelId and guildId from items[0].json but these fields are from the original message context and may not be present in the AI response output - should merge data from previous nodes
- [minor] The 'Format Discord Response' Code node could potentially be eliminated by directly accessing the AI response in the Discord node using expressions; [minor] The 'Skip Message' NoOp node serves no functional purpose and could be removed (the false branch could simply end)
- [critical] Data loss in Format Discord Response node: The node attempts to access channelId and guildId from items[0].json, but these fields are not passed through the Answer Question (AI chain) node. The AI chain only outputs 'output' or 'text' fields, losing the original Discord metadata needed for the response.; [major] Missing data merge pattern: The workflow needs to preserve original Discord message metadata (channelId, guildId) alongside the AI response, but there's no Merge node or data preservation mechanism between the extraction and formatting steps.; [minor] Inconsistent field access pattern in Format Discord Response: The code uses items[0].json.output || items[0].json.text but the AI chain node outputs to a specific field name that should be known and accessed directly.
- [major] Missing document ingestion workflow: The user has rules in 2-4 Google Docs that need to be loaded into the vector store, but there is no workflow component to fetch and process these documents. The Vector Store is configured in 'retrieve' mode with a memory key, but there's no corresponding workflow to populate it with the Google Docs content. This violates the document processing best practice of having a complete 'Trigger  Capture Binary  Extract Text  Parse/Transform  Route to Destination' flow for the rules documents.; [major] Incorrect Discord integration approach: The workflow uses a generic Webhook node to receive Discord messages, which requires manual webhook setup and doesn't properly handle Discord's interaction model. Best practices recommend using the Discord node (n8n-nodes-base.discord) for both triggering AND responding when Discord is requested. The current approach with webhook + Discord send creates an incomplete integration that won't properly handle Discord's message events, bot mentions, or channel-specific triggers.; [minor] Using Retrieval QA Chain instead of AI Agent: Best practices explicitly state 'Unless user asks for a node by name, always use the AI Agent node over provider-specific nodes or use-case-specific AI nodes (like Message a model) for chatbot workflows.' The workflow uses chainRetrievalQa instead of an AI Agent, which provides better orchestration and is the recommended approach for chatbot workflows.; [minor] No memory implementation: Best practices state 'Always utilise memory in chatbot agent nodes - providing context gives you full conversation history and more control over context.' The workflow has no memory node connected, which means the bot cannot handle follow-up questions or maintain conversation context across multiple messages from the same user.; [minor] In-Memory Vector Store without persistence: The workflow uses vectorStoreInMemory with a memory key 'eco_community_rules', but there's no workflow to populate this vector store with the Google Docs content. In-memory storage is also volatile and will be lost on workflow restart. For a production chatbot serving community rules, a persistent vector store approach or a separate document loading workflow is needed.

**Programmatic Violations:**
- None

### example-059-5993d809
**Status:** pass | **Score:** 87.4%

**LLM-Judge Violations:**
- [major] The workflow does not actually attach or send the recommended brochure file to the lead. The email only contains the AI-generated message but lacks the actual brochure attachment that was explicitly requested in the user's requirement to 'send them the most relevant brochure'.; [minor] The workflow automatically schedules a meeting without the lead's confirmation or availability check. The user requested to 'organise a meeting' which typically implies coordination with the lead, not unilateral calendar creation. This could result in scheduling conflicts.
- [critical] In 'Parse AI Response' node: References $json.leadEmail and $json.leadName which don't exist in the AI Agent output. The AI Agent node outputs 'output' field, not the original lead data fields.; [major] In 'Analyze Lead & Recommend Brochure' node: Complex string concatenation using + operator without proper expression syntax. Should use template literals or proper n8n expression format like `={{ 'text' + $json.field + 'more text' }}`
- [minor] Webhook node 'Receive Lead' has nested placeholder in responseData option - should be a simple string or empty. The placeholder format '<__PLACEHOLDER_VALUE__Response message...>' is unnecessarily complex for an optional field.; [minor] Agent node 'Analyze Lead & Recommend Brochure' has complex string concatenation with embedded placeholder in the prompt text. While functional, this makes the prompt harder to maintain and the placeholder for brochure list is embedded in a concatenated expression.
- [minor] Parse AI Response node duplicates lead data (leadEmail, leadName) that's already available from Format Lead Data node - could reference previous node instead; [minor] Format Lead Data node could potentially be eliminated by handling data formatting directly in the AI agent prompt using webhook data
- [critical] Data loss in Parse AI Response node: The node only extracts 'aiRecommendation', 'leadEmail', and 'leadName' from the AI agent output, but the AI agent output structure is { output: '...', leadName: '...', leadEmail: '...' }. The leadEmail and leadName are not present in the AI agent's output - they were in the previous Format Lead Data node. This will cause undefined values.; [major] Incorrect data reference in Parse AI Response: The node tries to access $json.leadEmail and $json.leadName from the AI agent output, but these fields don't exist in that output. The AI agent only outputs 'output' field. Should reference data from earlier nodes using $('Format Lead Data').item.json.leadEmail; [major] Missing error handling for JSON.parse: The Parse AI Response node uses JSON.parse($json.output) without validation. If the AI returns malformed JSON or wraps it in markdown code blocks, this will fail and break the workflow.; [major] Date/time format mismatch: The AI is asked to provide dates in 'YYYY-MM-DD HH:MM' format, but Google Calendar expects ISO 8601 format. The workflow doesn't transform this format, which will likely cause calendar creation to fail.; [minor] Inefficient data structure: The Parse AI Response node wraps the parsed JSON in an 'aiRecommendation' object, requiring nested access like $json.aiRecommendation.email_subject. Could flatten this for simpler downstream access.; [minor] Missing brochure attachment handling: The AI recommends a brochure_url but the Send Brochure Email node doesn't include any attachment configuration. The brochure reference is lost and not actually sent to the lead.
- [minor] Two nodes use generic 'Set' node type names ('Format Lead Data' and 'Parse AI Response') - while the custom names are descriptive, this is a minor inconsistency in naming approach; [minor] No error handling nodes present - workflow lacks modularity for handling failures in email sending, calendar scheduling, or AI processing; [minor] AI prompt configuration is embedded directly in the agent node rather than being externalized, reducing reusability and making updates harder
- [major] Missing data persistence for lead information. Best practices for data_persistence state that workflows should store data to persistent storage (Data Table, Google Sheets, Airtable, or database nodes) for audit trails and monitoring. The user requested to 'automate reaching out to potential leads' which implies the need to track and manage these leads over time. Currently, lead data flows through the workflow but is never stored for later reference, reporting, or follow-up tracking.

**Programmatic Violations:**
- None

### example-060-065cfab4
**Status:** pass | **Score:** 77.0%

**LLM-Judge Violations:**
- [major] Missing mechanism for user to send/upload their photo. The workflow uses a manual trigger with hardcoded placeholder values in a Set node, but provides no way for a user to actually submit their photo. The user prompt explicitly states 'user send user's photo' which implies the workflow should accept photo input from users, not require manual editing of placeholder values.
- [major] In 'Generate 3D Avatar' node, the jsonBody parameter uses incorrect syntax: `={{ { "apiKey": ..., "modelKey": $json.modelKey, ... } }}`. The object literal inside the expression should not have the outer curly braces duplicated. Should be `={{ { "apiKey": "...", "modelKey": $json.modelKey, "modelInputs": { "imageUrl": $json.photoUrl } } }}` but the double braces around the object are redundant.; [minor] In 'Format 3D Model Response' node, the objectValue parameter uses `={{ $json }}` which is correct but could be more explicit. However, this is valid syntax and works correctly.
- [minor] HTTP Request node uses placeholder URL with verbose description instead of a cleaner placeholder format. While functional, '<__PLACEHOLDER_VALUE__Enter Banana API endpoint URL (e.g., https://api.banana.dev/start/v4)__>' could be simplified to just the example URL or a cleaner placeholder.; [minor] Set node 'Prepare Photo Data' uses overly verbose placeholder format '<__PLACEHOLDER_VALUE__Enter the URL...__>' instead of simpler placeholders like '<UNKNOWN>' or empty strings, though this is still valid.
- [minor] The 'Prepare Photo Data' Set node could potentially be eliminated by accepting input directly from the trigger or using workflow static data, reducing an intermediate step; [minor] The 'Format 3D Model Response' Set node performs simple data extraction that could be handled by the consuming application or a single Code node if transformation is needed; [minor] API key is duplicated in both the Authorization header and the JSON body, which is redundant unless the API specifically requires both
- [minor] Redundant API key specification in both Authorization header and JSON body - could lead to confusion if values differ; [minor] Multiple fallback attempts in modelUrl extraction ($json.modelOutputs?.modelUrl || $json.modelOutputs?.[0]?.model_url) suggests uncertainty about API response structure - should validate actual Banana API response format; [major] Missing error handling for HTTP request failures - if API call fails, Format node will receive error data that may not have expected fields, causing incorrect transformations
- [minor] Using Manual Trigger instead of a more appropriate trigger for user photo submission. Best practice for content generation workflows is to use appropriate input mechanisms. For a 3D avatar generator where users send photos, a Webhook trigger or Form Trigger would be more suitable to actually receive user photos, rather than a Manual Trigger which requires manual workflow execution.; [minor] Using placeholder values in Set node instead of proper input mechanism. The 'Prepare Photo Data' Set node contains placeholder values for photoUrl and modelKey rather than receiving actual user photo data. This violates content generation best practices which emphasize proper data handling - the workflow should receive actual binary photo data from users, not placeholder URLs.; [minor] Not following binary data best practices for image handling. The workflow uses a URL string for the photo instead of binary data. Content generation best practices state: 'When generating images, prefer binary data over URLs for uploads to avoid media type errors.' The workflow should handle the user's photo as binary data rather than expecting a URL.

**Programmatic Violations:**
- None

### example-061-06e374b1
**Status:** fail | **Score:** 65.2%

**LLM-Judge Violations:**
- [major] Session Exists? node has a self-referencing connection (output 0 connects back to itself), which creates an infinite loop and will cause the workflow to fail immediately on execution; [minor] Missing error handling for validation failures - when Validate Input throws an error, there's no mechanism to send an error response back to the webhook caller, leaving the request hanging
- [critical] Session Exists? IF node has a self-loop connection: 'Session Exists?'  'Session Exists?' [main] on the true branch. This creates a potential infinite loop or execution deadlock that will break the workflow.; [major] Guardrails node is used incorrectly as a main workflow node. The Guardrails node is a capability-only node that should connect via ai_guardrails to an AI Agent, not process data on the main path. Current pattern: Merge Sessions  Apply Content Guardrails  Check Guardrails Result [all main connections].
- [critical] Node 'Check Session Exists': Invalid placeholder syntax in URL parameter - uses <__PLACEHOLDER_VALUE__...> which is not valid n8n expression syntax and will cause runtime errors; [critical] Node 'Create New Session': Invalid placeholder syntax in URL parameter - uses <__PLACEHOLDER_VALUE__...> instead of proper expression or static value; [critical] Node 'Generate Query Embeddings': URL uses invalid placeholder syntax without expression wrapper, will fail at runtime; [major] Node 'Session Exists?': Expression ={{ $json.length }} references array length but $json refers to the current item, not an array. Should use $input.first().json or check array structure properly
- [minor] Session Exists? node has a self-referencing connection in the true branch (connects to itself), which appears to be a configuration error in the connections but manifests as unusual behavior
- [minor] Load Session Context and Format New Session nodes perform similar data transformation operations that could potentially be consolidated into a single conditional Code node; [minor] Check Guardrails Result node handles both guardrails validation and data pass-through, creating a slight inefficiency in the execution path when guardrails pass; [minor] Format Response node performs minimal transformation that could be integrated into the Update Session node or handled by Respond to Webhook directly; [minor] Prepare AI Context node includes conditional logic for skip_ai that could be handled with an IF node to avoid unnecessary processing
- [critical] Session Exists? node has a circular connection back to itself (output 0 connects to both Load Session Context and Session Exists?), creating an infinite loop that will cause workflow failure; [major] Check Guardrails Result node attempts to merge guardrails output with session data using $('Merge Sessions').first().json, but guardrails node output structure is not properly preserved - the session data fields may be lost; [major] Generate Query Embeddings node executes even when skip_ai is true (guardrails failed), wasting API calls and potentially causing errors since the flow should short-circuit; [major] Query Vector Database expects $json.data[0].embedding but Generate Query Embeddings returns Azure OpenAI response structure - field path may be incorrect depending on actual API response format; [major] Update Session node uses $json.output to reference AI response, but this field comes from AI Chat Agent - if guardrails failed and AI was skipped, this field won't exist causing incorrect data in conversation_history; [minor] Load Session Context accesses sessionData.context and sessionData.conversation_history but Check Session Exists returns an array - should access [0].context and [0].conversation_history explicitly; [minor] Format Response node attempts to access both aiData.output and sessionData.ai_response as fallbacks, but the data flow doesn't guarantee ai_response field exists in all paths; [minor] Update Session PATCH uses spread operator in jsonBody (...($json.conversation_history || [])) which may not work correctly in n8n expression syntax - should use explicit array concatenation
- [minor] Some node names could be more specific - 'Merge Sessions' doesn't clearly indicate it's merging existing and new session branches; [minor] The 'Session Exists?' node has a self-referencing connection that appears to be an error in the workflow structure; [minor] Multiple HTTP Request nodes use placeholder values which could be extracted to workflow variables for better maintainability
- [major] Missing memory node for AI Agent: The user explicitly requested 'Session context: chat_sessions table (JSONB context)' and this is a production-ready chatbot. Best practices state 'Always utilise memory in chatbot agent nodes - providing context gives you full conversation history and more control over context.' The workflow manually manages conversation history in code and database, but the AI Agent node has no memory node connected, which means it cannot maintain proper conversational context across turns within a single execution.; [minor] Using HTTP Request nodes instead of native Supabase/Postgres nodes: While HTTP Request works, best practices for data persistence recommend using native database nodes (Postgres node) when available. The user specified 'Backend: Supabase (Postgres + pgvector)' - using the Postgres node would provide better error handling, type safety, and easier configuration than raw HTTP requests to Supabase REST API.; [minor] Manual embedding generation instead of using LangChain embedding nodes: The workflow uses raw HTTP Request to Azure OpenAI embeddings endpoint. Best practices for AI workflows recommend using native embedding nodes (@n8n/n8n-nodes-langchain.embeddingsAzureOpenAi) which provide better integration with vector stores and AI components, especially for production workflows.; [minor] No error handling for guardrails or AI failures: The user requested a 'production-ready' workflow with 'Safety: Guardrails / content filters'. While guardrails are implemented, there's no error handling if the guardrails node itself fails, or if the AI Agent fails to respond. Production workflows should have fallback mechanisms for critical components.

**Programmatic Violations:**
- [critical] Node Apply Content Guardrails (@n8n/n8n-nodes-langchain.guardrails) is missing required input of type ai_languageModel
- [major] Merge node "Merge Sessions" has only 0 input connection(s). Merge nodes require at least 2 inputs.

### example-062-968a1da2
**Status:** fail | **Score:** 56.5%

**LLM-Judge Violations:**
- [critical] Pinecone Retriever is missing the required Pinecone Vector Store connection. The retriever node needs to connect to a Pinecone vector store node via ai_vectorStore port to function, but no such connection exists.
- [critical] Pinecone Retriever (retrieverVectorStore) is missing the required ai_vectorStore input connection. The workflow has a vectorStorePinecone node ('Store Company Data') in INSERT mode for storing scraped company data, but lacks a Vector Store node configured for RETRIEVAL mode to feed the Pinecone Retriever. The correct pattern requires: vectorStorePinecone (in retrieve/load mode)  Pinecone Retriever [ai_vectorStore]  Generate Personalized Emails [ai_retriever]. Without this ai_vectorStore connection, the Pinecone Retriever cannot access any vector database, completely breaking the RAG functionality for email personalization.
- [critical] Invalid function 'placeholder()' used in Parse Company Input node - this is not a valid n8n function or JavaScript function; [critical] Invalid function 'placeholder()' used multiple times in Prepare Email Generation node - not a valid n8n or JavaScript function; [critical] Invalid function 'placeholder()' used in Format Email Data node - not a valid n8n or JavaScript function
- [major] Parse Company Input (Code node): Uses undefined 'placeholder()' function in JavaScript code. This will cause runtime error. Should use template literals or string concatenation instead.; [major] Prepare Email Generation (Code node): Uses undefined 'placeholder()' function multiple times in JavaScript code. This will cause runtime error.
- [major] Redundant data reading operation: 'Read Approved Emails' node uses hardcoded placeholder data instead of actually reading from the Google Sheet that was just created. This creates a disconnect in the workflow where the approval process doesn't actually connect to the email sending.; [minor] Multiple NoOp nodes ('All Companies Processed', 'Email Sent', 'Campaign Cancelled') that serve only as visual markers but don't perform any operations. While they may aid clarity, they add to node count without functional purpose.; [minor] The 'Add Email Drafts' node could potentially be combined with 'Create Email Draft Sheet' in a single operation by creating the sheet with initial data, reducing one node and one API call.
- [critical] Critical data loss in 'Add Email Drafts' node - receives spreadsheetId from 'Create Email Draft Sheet' but needs email data from 'Format Email Data'. The spreadsheetId object overwrites all email draft data, causing complete loss of company, subject, body, and recipient information.; [critical] Read Approved Emails node uses hardcoded placeholder data instead of reading from the Google Sheet. All approved email data (company names, recipients, subjects, bodies) is lost and replaced with dummy 'Example Corp' data, breaking the entire approval workflow.; [major] Store Company Data node expects document format for vector store insertion but receives AI agent output (text/output field). Missing transformation to convert analyzed company data into proper document structure with pageContent and metadata fields required by Pinecone.; [major] Format Email Data node uses fragile regex parsing that assumes specific AI output format. If the AI agent returns emails in a different structure, all email data will be lost or incorrectly parsed, defaulting to a single 'All Companies' entry with placeholder recipient.; [major] Pinecone Retriever in 'Generate Personalized Emails' references a vector store but there's no guarantee the data is available yet. The retriever needs the same index/namespace configuration as the Store node, but this connection is not validated.; [minor] Scrape Company Website node passes raw scraped data to Analyze Company Data, but the field extraction ($json.content || $json.markdown || JSON.stringify($json)) may not correctly handle the Airtop response structure, potentially passing stringified objects instead of actual content.; [minor] Check Approval node checks for 'Yes' substring in approved field, but the Wait form returns 'Yes - Send All Emails'. While this works, it's fragile - any change to the dropdown option text would break the condition.
- [minor] Some generic NoOp node names ('All Companies Processed', 'Email Sent', 'Campaign Cancelled') could be more descriptive about their purpose in the workflow; [minor] The 'Read Approved Emails' node contains placeholder logic rather than actual Google Sheets reading implementation, reducing maintainability; [minor] Two separate OpenAI Model nodes with similar names ('OpenAI Model (Email)' and 'OpenAI Model (Scraper)') could benefit from more context-specific naming
- [major] Missing scraping nodes for company data extraction. The workflow uses Airtop for scraping, but best practices explicitly state that for large providers and services where terms of service may be violated, dedicated scraping nodes (Phantombuster, Apify, BrightData) should be used. The user's request involves scraping company information which likely includes LinkedIn profiles and other protected data sources. Using HTTP-based scraping (Airtop) instead of dedicated scraping services risks rate limiting, ToS violations, and unreliable data extraction.; [major] Pinecone Vector Store missing required ai_embedding connection from OpenAI Embeddings node. The connections show 'OpenAI Embeddings' has an ai_embedding output, but it's not connected to the Pinecone Retriever used in the email generation agent. The Pinecone Retriever at position [2140, 700] needs the same embeddings model that was used to store the data. Currently, the embeddings are only connected to the Store Company Data node but not to the Pinecone Retriever, which will cause retrieval failures.; [minor] Read Approved Emails node uses placeholder code instead of actually reading from Google Sheets. The workflow creates and writes to Google Sheets but then uses a Code node with hardcoded placeholder data instead of using the Google Sheets node to read the approved emails. This breaks the approval workflow - the user's edits in the sheet won't be reflected in the sent emails.; [minor] Missing rate limiting and batching for email sending. Best practices for scraping/research workflows emphasize implementing delays and batching to avoid rate limits. The workflow processes companies one at a time during scraping (good) but doesn't implement any Wait nodes or delays between email sends, which could trigger Gmail rate limits when sending to multiple recipients.; [minor] Data Table storage not used despite being the preferred option. Best practices state Data Table is the PREFERRED storage option for internal workflow data, requiring no credentials and being ideal for prototyping. The workflow uses Google Sheets exclusively, which introduces external dependencies, API rate limits, and requires credentials when Data Table would be simpler and more reliable for this use case.

**Programmatic Violations:**
- [critical] Node Generate Personalized Emails (@n8n/n8n-nodes-langchain.agent) received unsupported connection type ai_retriever; [critical] Node Pinecone Retriever (@n8n/n8n-nodes-langchain.retrieverVectorStore) is missing required input of type ai_vectorStore; [critical] Node Store Company Data (@n8n/n8n-nodes-langchain.vectorStorePinecone) is missing required input of type ai_document

### example-063-0c899d7d
**Status:** pass | **Score:** 89.7%

**LLM-Judge Violations:**
- [minor] In 'Ebook Creation Agent' node, the text parameter uses unnecessary string wrapping: `={{ "..." }}` instead of just `="..."` for a pure string value. While functionally correct, the outer quotes are redundant.
- [minor] Code node uses $input.first().json.output to access data, but the actual output path from the AI Agent may be different (typically $json.text or similar). This could cause runtime issues if the data structure doesn't match expectations.
- [major] Potential data access issue: The Code node accesses `$input.first().json.output` but AI Agent nodes typically return structured data in `$json` directly, not nested under `output`. This could cause the workflow to fail or lose all ebook data.
- [minor] Missing error handling for AI generation failures: The workflow doesn't include error handling nodes or retry logic if the AI Agent fails to generate content or if the OpenAI API returns an error. While not explicitly requested by the user, this is a common pitfall for content generation workflows that could cause the entire workflow to fail silently.; [minor] No validation of structured output before file formatting: The Code node directly accesses ebookData properties (title, chapters, conclusion) without validating that the structured output parser successfully returned the expected schema. If the AI returns malformed data, the workflow could fail at the formatting stage. Best practice for content generation recommends 'validate and sanitize input/output to avoid malformed data.'; [minor] Potential token limit concerns: The workflow uses maxTokens: 4000 for GPT-4o, but requests generation of a complete ebook with introduction, 5 chapters (300-500 words each), and conclusion. This could total 2000-3500 words, which may approach or exceed the token limit depending on the topic complexity. Best practice recommends breaking complex tasks into sequential steps for better reliability.

**Programmatic Violations:**
- [critical] Node Ebook Structure Parser (@n8n/n8n-nodes-langchain.outputParserStructured) is missing required input of type ai_languageModel
- [major] Agent node "Ebook Creation Agent" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-064-011d91aa
**Status:** pass | **Score:** 75.9%

**LLM-Judge Violations:**
- [critical] Multi-step form flow is fundamentally broken. Form Step 2 and Form Step 3 outputs are not connected to 'Prepare Data for Sheet', causing form submissions to be lost. The workflow routes from Check Participation directly to Prepare Data for Sheet (bypassing form submission data), then separately to Form Step 2/3 which have no downstream connections.
- [critical] Form Step 2 (Participant Questions) has no outgoing connection. After user submits this form, the collected data (enjoyment, projectWorkedOn, timeEstimate) never flows to 'Prepare Data for Sheet' node, resulting in incomplete data being written to Google Sheets. The execution path from Form Step 2 is completely disconnected.; [critical] Form Step 3 (Non-Participant Question) has no outgoing connection. After user submits this form, the collected data (favoriteProject) never flows to 'Prepare Data for Sheet' node. The execution path from Form Step 3 is completely disconnected.
- [major] Node 'Form Step 2: Participant Questions' - fieldOptions.values uses outdated syntax: `={{ $node["Transform to Form Options"].json.projectOptions }}` instead of modern `={{ $('Transform to Form Options').item.json.projectOptions }}`; [major] Node 'Form Step 3: Non-Participant Question' - fieldOptions.values uses outdated syntax: `={{ $node["Transform to Form Options"].json.projectOptions }}` instead of modern `={{ $('Transform to Form Options').item.json.projectOptions }}`; [minor] Node 'Prepare Data for Sheet' - The code attempts to access data from both form steps but doesn't account for the branching logic. It should reference the specific form node that executed, not assume all fields are present in a single input.
- [major] Form Step 2 and Form Step 3 are not connected to 'Prepare Data for Sheet' node. The workflow has Form Step 2 and Form Step 3 as separate branches but only Form Step 2's output would flow to data preparation. Form Step 3 responses would not be captured.
- [major] Inefficient execution path: 'Prepare Data for Sheet' node is connected directly from the If node's true branch, bypassing Form Step 2. This means data preparation happens before collecting Form Step 2 responses, causing incomplete data or requiring workflow re-execution.; [minor] Form Step 3 output is not connected to data preparation, creating a dead-end path. Non-participants' responses won't be written to Google Sheets, requiring duplicate data handling logic.; [minor] The 'Prepare Data for Sheet' Code node could be eliminated by using Google Sheets' auto-mapping feature more effectively, or by using a simpler Set node for basic field mapping.
- [critical] Critical data loss in multi-step form flow: Form Step 2 and Form Step 3 outputs are never connected to 'Prepare Data for Sheet' node. Only the IF node output (which contains Form Step 1 data) reaches the preparation node, causing loss of all Step 2/3 responses.; [major] Incorrect data flow routing: 'Check Participation' IF node connects directly to 'Prepare Data for Sheet' on the true branch, bypassing Form Step 2 entirely. This means participant responses from Step 2 are collected but never processed.; [major] Missing data merge mechanism: The workflow lacks a Merge node or mechanism to combine data from Form Step 1 with either Form Step 2 or Form Step 3 responses before writing to Google Sheets. Each form step outputs separate data that needs to be merged.; [minor] Inefficient data transformation: The 'Prepare Data for Sheet' node attempts to access fields from all form steps in a single object (data.enjoyment, data.favoriteProject, etc.) but the data structure won't contain these merged fields without proper data combination logic.
- [minor] Form Step 2 and Step 3 both have 'Hackmation Survey - Step 2' as the title, which could cause confusion for users and maintainers; [minor] The 'Check Participation' node has two connections to different nodes from the true branch, which creates a split flow that could be clearer with better organization or comments; [minor] Missing error handling nodes for API calls (Notion and Google Sheets), which could impact maintainability when debugging failures
- [major] Missing n8n Attribution Disabling: The Form nodes do not have 'Append n8n Attribution' disabled. Best practices explicitly state: 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for the n8n form nodes, add this setting and set it to false.' This affects all three Form nodes in the workflow.; [major] Incorrect Multi-Step Form Implementation: The workflow uses a Manual Trigger instead of the required Form Trigger node. Best practices state: 'Use the n8n Form Trigger node to start the workflow and display the first form page to the user.' The current implementation with Manual Trigger won't properly initiate the form experience for users.; [minor] Premature Data Preparation: The 'Prepare Data for Sheet' node is connected directly from the 'Check Participation' IF node's true branch, meaning it executes before Form Step 2 collects its data. This violates the best practice: 'For multi-step forms: store immediately after aggregating all steps with Set/Merge nodes.' The workflow should aggregate data from both Form Step 2 and Form Step 3 before preparing for storage.

**Programmatic Violations:**
- None

### example-065-ccaddd27
**Status:** pass | **Score:** 94.3%

**LLM-Judge Violations:**
- [minor] Send Summary Email node has a placeholder value for 'sendTo' parameter. While this is expected when user doesn't provide their email address, it uses a verbose placeholder format instead of a cleaner placeholder like '<UNKNOWN>' or empty string.
- [major] Gmail 'getAll' operation returns different data structure than expected - the 'simple: true' parameter returns limited fields (id, threadId, labelIds, snippet, etc.) but the 'Format Emails for AI' node expects 'from', 'subject', 'date' fields which are only available with 'simple: false' or by using 'get' operation for each message; [minor] Date filter logic issue - 'receivedBefore' uses Sunday 23:59:59 but 'receivedAfter' uses Saturday 00:00:00, which correctly captures the weekend but the field names in Gmail API should be verified (Gmail uses 'after' and 'before' with date format, not 'receivedAfter'/'receivedBefore'); [minor] AI output field mapping assumes 'output' field from chainLlm node, but the actual field name should be verified - LangChain nodes typically return 'response' or 'text' field depending on configuration
- [minor] Missing empty notification prevention check. Best practice states: 'Always include empty notification prevention - check that alert-worthy items exist (items.length > 0) before proceeding to notification nodes.' The workflow should check if there are emails before proceeding to AI analysis, though the Code node does handle the no-email case, it still processes through the AI chain unnecessarily.; [minor] AI temperature not optimally configured for consistent output. The GPT-4o-mini node uses temperature 0.3, but best practices for triage/classification recommend 'Set low temperature of the model (0-0.2) for consistency' when extracting structured information. While 0.3 is relatively low, it could be more consistent at 0-0.2.; [minor] Missing logging/tracking of sent notifications. Best practice states: 'Add logging nodes to track sent notifications for audit trails and duplicate prevention.' While not critical for the basic use case requested, this would improve the workflow for production use.

**Programmatic Violations:**
- None

### example-066-12e8815a
**Status:** pass | **Score:** 91.8%

**LLM-Judge Violations:**
- [major] The Google Sheets integration is incomplete - the workflow uses a toolWorkflow node with a placeholder value that references a separate workflow that doesn't exist. The user requested a working connection to Google Sheets, but this requires manual setup of an entirely separate workflow before it can function.
- [major] Memory node (Conversation Memory) should connect to both the AI Agent AND the Chat Trigger for proper session management. Currently only connected to the agent, which may cause session context issues in the chat interface.
- [minor] Query Google Sheets tool node has placeholder workflowId value '<__PLACEHOLDER_VALUE__Select the workflow that contains your Google Sheets read operation__>' which is expected since user didn't provide a specific workflow ID, but the verbose placeholder format could be simplified
- [major] Chat input data extraction uses incorrect expression '={{ $json.chatInput }}' in agent node. The chatTrigger node outputs 'input' field, not 'chatInput', which could cause the agent to receive undefined/null values instead of user messages.; [minor] Tool workflow has null workflowInputs value which may cause issues if the sub-workflow expects parameters. While this might work for simple read operations, it doesn't pass any filtering or query parameters from the AI agent to the Google Sheets workflow.
- [major] Tool Workflow node uses placeholder value instead of actual workflow configuration. The 'Query Google Sheets' tool has '<__PLACEHOLDER_VALUE__Select the workflow that contains your Google Sheets read operation__>' which means the chatbot cannot actually access Google Sheets data. Best practice states to use appropriate nodes for data access, and this configuration would prevent the core requested functionality (answering questions about Google Sheets data) from working.

**Programmatic Violations:**
- None

### example-067-05bc6936
**Status:** pass | **Score:** 87.5%

**LLM-Judge Violations:**
- [major] Invalid reference to $('Upload to S3').params.region - the .params helper is not available for AWS S3 nodes. The .params helper is only available for specific node types and does not contain node parameters.
- [minor] S3 bucket name uses placeholder value which is expected when user doesn't specify a bucket, but the placeholder format could be cleaner (standard <UNKNOWN> format preferred)
- [major] URL construction in Format Response node uses incorrect expression syntax - mixing Mustache {{ }} with n8n expression syntax $(), which will likely fail or produce malformed URLs; [major] Region parameter access in URL construction is problematic - $("Upload to S3").params.region attempts to access node parameters rather than execution data, and the region is not defined in the S3 node parameters; [minor] Binary data property name 'image' matches the form field name, but there's no validation that the binary data was successfully extracted from the form submission
- [major] Missing raw form data storage: The workflow does not store the raw form submission data to any persistent storage destination (Google Sheets, Airtable, Data Tables, or database). According to best practices, 'ALWAYS store raw form responses to a persistent data storage destination even if the primary purpose of the workflow is to trigger another action.' The workflow only uploads to S3 and formats a response, but never persists the form submission data itself for administration purposes.; [minor] n8n attribution not disabled: The Form Trigger node does not have the 'Append n8n Attribution' setting explicitly set to false. Best practices state: 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for the n8n form nodes, add this setting and set it to false.'

**Programmatic Violations:**
- None

### example-068-ee6be3e4
**Status:** pass | **Score:** 80.2%

**LLM-Judge Violations:**
- [minor] In 'Structure Lead Data' node, the 'include' parameter is set to 'selected' with 'includeFields' listing fields, but this is redundant since fields are already defined in the 'fields.values' array. This doesn't cause errors but is unnecessary configuration.
- [minor] HTTP Request node uses a verbose placeholder format '<__PLACEHOLDER_VALUE__LinkedIn profile URL or scraping API endpoint__>' instead of the standard '<UNKNOWN>' or empty string pattern, though this is functionally acceptable as a user configuration point
- [major] Incorrect data extraction in Structure Lead Data node - attempting to access $json.output when AI agent output structure is likely different. AI agents typically return output in a specific format that may need parsing.; [major] Missing JSON parsing/validation step - the AI agent is instructed to return JSON but there's no node to parse the string response into a proper JSON object before the Set node tries to access it as an object.; [minor] Potential data loss in profile_url mapping - references $('Fetch LinkedIn Profile').item.json.url but the HTTP Request node may not return a 'url' field in its response, only in the original request parameters.
- [major] Using HTTP Request node directly for LinkedIn scraping violates best practices. The documentation explicitly states: 'If the user wishes to scrape data from sites like LinkedIn, Facebook, Instagram, Twitter/X... it is better to use a node designed for this. The scraping nodes provide access to these datasets while avoiding issues like rate limiting or breaking terms of service.' LinkedIn should be scraped using Phantombuster, Apify, or BrightData nodes, not direct HTTP requests.; [minor] No rate limiting or batching implemented. While the user didn't explicitly request production-ready features, the best practices documentation emphasizes: 'Batch requests and introduce delays to avoid hitting API rate limits or overloading target servers. Use Wait nodes and batching options.' For a lead analysis workflow that may process multiple profiles, basic rate limiting would be beneficial.; [minor] No error handling or retry logic configured. The HTTP Request node lacks 'Retry on Fail' configuration, which is recommended in the documentation for handling authentication failures and rate limiting. While not critical for the basic use case, this would improve reliability.; [minor] The HTTP Request node has a placeholder URL rather than a proper configuration. While this may be intentional for template purposes, the documentation warns: 'Double-check URL formatting, query parameters, and ensure all required fields are present to avoid bad request errors.' A more specific configuration or clear instructions would be better.

**Programmatic Violations:**
- None

### example-069-2b1a7148
**Status:** pass | **Score:** 92.5%

**LLM-Judge Violations:**
- [minor] HTTP Request node has a placeholder URL instead of a functional endpoint. While the user prompt is incomplete and doesn't specify the actual endpoint, the workflow cannot execute successfully without a valid URL being configured.
- [minor] No error handling for HTTP request failures - data flow could break without proper error management; [minor] No data validation or transformation after HTTP response - raw API data passes through without verification of structure or required fields; [minor] Missing downstream processing nodes - extracted data has no destination or storage, potentially resulting in data loss
- [minor] HTTP Request node uses placeholder URL instead of a real endpoint. While this is acceptable for a template, the best practice documentation emphasizes choosing the right node for the data source and properly configuring it. The placeholder prevents the workflow from being immediately functional.; [minor] No data transformation or normalization after HTTP extraction. Best practices state 'Normalize data structure early in your workflow' and recommend using transformation nodes like Split Out, Aggregate, or Set to ensure data matches n8n's expected structure. While not critical for a simple extraction, this would be important if the extracted data needs further processing.

**Programmatic Violations:**
- None

### example-070-5bedb915
**Status:** pass | **Score:** 77.7%

**LLM-Judge Violations:**
- [critical] The workflow does not implement the user flow correctly. The user requested a form where email is entered first, and ONLY if not found, the user is asked for first name and last name to self-register. The current implementation shows all fields (email, firstName, lastName) on a single form simultaneously, which fundamentally breaks the requested two-step interaction flow.; [major] Missing the 'go back to enter another email' functionality that was explicitly requested. The user specified that when not on the list, the user should be able to go back and try another email, but the workflow has no mechanism to loop back to the form or allow re-entry.
- [major] Terminal nodes (Success Response and Handle Self-Registration) do not connect back to provide user feedback. The Form Trigger workflow processes data but never sends results back to the user's browser, leaving the user experience incomplete.; [minor] The workflow lacks implementation of the 'go back to enter another email' functionality described in requirements. The current single-pass execution doesn't support the interactive retry flow requested by the user.
- [minor] In 'Success Response' node, the expression `={{ $json.email }}` references a field from the previous node 'Call QR Code URL (Check-in)', but that node is an HTTP Request that won't have the email field in its output. Should reference the original data using `={{ $('Parse CSV and Search Email').item.json.email }}`
- [major] Form Trigger node has all fields (email, firstName, lastName) visible simultaneously, but user requirements indicate a conditional flow where firstName/lastName should only be requested AFTER email is not found. The current configuration doesn't support the 'go back to enter another email' functionality described in requirements.
- [minor] CSV is fetched on every form submission instead of being cached or loaded once. For high-traffic events, this creates unnecessary HTTP requests.; [minor] The form includes firstName and lastName fields upfront but they're optional, creating a slightly confusing UX. A two-step form (email first, then conditional registration fields) would be cleaner.
- [critical] Data loss in 'Call QR Code URL (Check-in)' node - the email field from previous node is not preserved, causing 'Success Response' node to reference undefined $json.email; [major] CSV parsing logic is fragile and will fail with quoted fields containing commas - uses simple split(',') instead of proper CSV parsing which can cause data corruption; [major] Missing data validation in 'Parse CSV and Search Email' - if firstName/lastName are undefined in form submission, they become empty strings which breaks the conditional logic in 'Handle Self-Registration'; [minor] Inefficient data flow - 'Fetch CSV File' is called on every form submission instead of being cached or loaded once, causing unnecessary data retrieval; [minor] The form includes firstName and lastName fields upfront, but the workflow logic expects them to be optional for initial submission - this creates confusion in the data flow as users might fill them initially
- [minor] Two 'Set' nodes use generic naming pattern ('Success Response' and 'Handle Self-Registration' are better than 'Set' but could be more action-oriented like 'Format Success Message' or 'Format Registration Response'); [minor] The 'Handle Self-Registration' node does double duty - both checking if registration data exists AND formatting the response, violating single responsibility principle; [minor] Complex conditional logic embedded in Set node parameters (checking firstName && lastName) should be separated into a dedicated IF node for better clarity
- [major] Missing raw form data storage: The workflow does not store the raw form responses to any persistent storage destination (Google Sheets, Airtable, Data Tables, or database). The best practices explicitly state 'ALWAYS store raw form responses to a persistent data storage destination' and 'It is CRITICAL if you create a n8n form node that you store the raw output with a storage node.' The Set nodes ('Success Response' and 'Handle Self-Registration') only transform data in memory and do NOT persist it. This violates the core form_input best practice of storing raw form data for administration.; [major] Missing n8n attribution setting: The Form Trigger node does not have the 'Append n8n Attribution' setting disabled. The best practices state: 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for the n8n form nodes, add this setting and set it to false.' This affects the user experience of the form.

**Programmatic Violations:**
- None

### example-071-e68c2d6e
**Status:** fail | **Score:** 68.5%

**LLM-Judge Violations:**
- [critical] Missing multi-step form implementation - user explicitly requested a multi-step form with conditional routing (step 1 routes to step 2 if 'yes', step 3 if 'no'), but workflow implements a single-page form with all questions visible at once; [major] Missing Notion database integration - user explicitly requested that project options be read dynamically from a Notion database (page titles), but workflow uses hardcoded placeholder values instead
- [critical] Missing Notion database integration: The workflow requires dynamic project lists from Notion, but there is no Notion node to fetch project titles from the database. The form fields contain placeholder values that cannot be populated.; [critical] Missing conditional routing logic: The user explicitly requested routing based on the 'participated' field (Yes  step 2, No  step 3), but the workflow has no IF/Switch node to implement this branching logic. All form fields are presented in a single step instead of the requested multi-step form.; [major] Incorrect form architecture: The requirement specifies a multi-step form (step 1, step 2, step 3) with conditional navigation, but the implementation uses a single Form Trigger with all fields visible at once. This violates the requested user experience and data flow pattern.
- [critical] Form Trigger node does not implement the required multi-step conditional routing. User explicitly requested: 'If the answer is yes, route to step 2, otherwise to step 3'. The workflow presents all questions in a single form instead of implementing conditional logic to show different questions based on the participation answer.
- [critical] Workflow completely ignores the multi-step form requirement. User requested conditional routing (step 1  step 2 if 'yes', step 1  step 3 if 'no'), but all questions are presented in a single form with no conditional logic; [critical] Missing Notion database integration entirely. User explicitly requested project lists to be read dynamically from Notion, but no Notion node exists to fetch this data; [major] No conditional routing logic implemented. The workflow should route to different form steps based on the 'participated' answer, but instead uses a single linear path; [major] Static placeholder values in form fields instead of dynamic data. The project dropdowns contain hardcoded placeholders rather than being populated from Notion
- [critical] Missing conditional routing logic - all form responses flow directly to Google Sheets without checking the 'participated' field. User explicitly requested routing to different form steps based on Yes/No answer, but workflow has no IF/Switch node to implement this branching logic.; [critical] No Notion database integration to dynamically fetch project titles. The workflow hardcodes placeholder values in dropdown options instead of reading from Notion API, meaning the dynamic project list requirement is completely unimplemented.; [major] All form fields are presented simultaneously in a single form instead of implementing multi-step conditional logic. Fields like 'enjoyment_rating', 'project_worked_on', and 'time_estimate' should only appear if user answered 'Yes' to participation, while 'favorite_project' should only appear for 'No' answers.; [major] Incorrect data structure being written to Google Sheets - the form will send all fields for every submission, including irrelevant fields (e.g., participants will have empty 'favorite_project' field, non-participants will have empty participation-specific fields), creating messy and inconsistent data.; [minor] No data transformation or cleanup before writing to Google Sheets. The raw form output includes all fields regardless of relevance, and there's no node to filter or restructure the data based on the participation status.
- [critical] Workflow completely fails to implement the required multi-step conditional routing logic. All form fields are in a single step instead of being split across steps 1, 2, and 3 based on user responses. This makes the workflow fundamentally unmaintainable as it doesn't match the specification.; [major] Missing Notion database integration node to dynamically fetch project titles. The workflow has hardcoded placeholder values instead of reading from Notion as specified, requiring manual updates and defeating the purpose of dynamic data.; [major] No conditional logic or routing nodes (IF/Switch) to handle the 'Yes' vs 'No' branching between steps 2 and 3. The workflow structure doesn't support the required decision-making flow.; [minor] Form fields that should only appear conditionally (enjoyment_rating, project_worked_on, time_estimate for 'Yes' responses; favorite_project for 'No' responses) are all visible at once, creating confusion and poor user experience.
- [major] Multi-step form implemented as single-step form: The user explicitly requested a multi-step form with conditional routing (step 1 asks about participation, then routes to step 2 if 'yes' or step 3 if 'no'). The workflow uses a single Form Trigger node with all questions visible at once, violating the documented best practice: 'Build multi-step forms by chaining multiple Form nodes together. Each Form node represents a page or step in your form sequence.' This fundamentally fails to deliver the requested user experience.; [major] Missing conditional routing logic: The user requested that if the answer to 'Did you take part in the hackmation?' is 'yes', route to step 2, otherwise to step 3. The workflow has no IF or Switch nodes to implement this routing logic. Best practices state: 'Use IF or Switch nodes to direct users to different form pages based on their previous answers.' Without this, the core requested functionality cannot work.; [major] Missing dynamic project list from Notion: The user explicitly requested that project options 'should be read dynamically from a Notion database (it's made up of the titles of the pages in the DB)'. The workflow has placeholder values instead of a Notion node to fetch projects. Best practices state: 'For forms that require dynamic options (e.g., dropdowns populated from an API or previous step), generate the form definition in a Code node and pass it to the Form node as JSON.' There is no Notion node, no Code node to generate dynamic form fields, and no connection to fetch the data.; [minor] n8n attribution not disabled: Best practices explicitly state 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution" for the n8n form nodes, add this setting and set it to false.' The Form Trigger node does not have this setting configured to false.; [minor] Missing data aggregation before storage: For multi-step forms, best practices state: 'Collect and merge all user responses from each form step before writing to your destination (e.g., Data Table). Use Set or Merge nodes to combine data as needed.' While the workflow does write to Google Sheets (satisfying storage requirements), there should be a Set or Merge node between the form steps and storage to properly aggregate the conditional responses.; [minor] Missing form completion message: Best practices state 'Use the Form Ending page type to show a completion message or redirect users after submission. Without a proper ending, users may be confused about whether their submission was successful.' While the workflow has a 'formSubmittedText', it should use a proper Form node with ending page type for multi-step forms.

**Programmatic Violations:**
- None

