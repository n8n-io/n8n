# One-Shot Evaluation Violations (Jan 26, 2026)

**Evaluation Type:** LLM Judge + Programmatic
**Examples:** 10
**Pass Rate:** 100%
**Average Score:** 87.68%

---

### example-001-f1b1d9b7
**Status:** pass | **Score:** 92.7%

**LLM-Judge Violations:**
- [major] Missing '=' prefix in 'Send Summary Report' node subject field: '"ðŸ“§ Weekend Email Summary - {{ $now.toFormat("MMM dd, yyyy") }}"' should be '="ðŸ“§ Weekend Email Summary - {{ $now.toFormat("MMM dd, yyyy") }}"'
- [minor] Send Summary Report node uses placeholder email address '<__PLACEHOLDER_VALUE__your-email@example.com__>' in sendTo parameter - this is expected since user didn't specify recipient email
- [critical] The Summarize Weekend Emails node expects text input but receives Gmail message objects. The chainSummarization node with 'operationMode: nodeInputJson' requires properly formatted text data, but Gmail returns structured objects with fields like 'id', 'threadId', 'snippet', 'payload', etc. The workflow doesn't extract the email body/content before passing to summarization.; [major] The Format Report node attempts to access summary data with fallback logic ($input.first().json.response || $input.first().json.text), but the actual output field from chainSummarization node may differ. This creates uncertainty in data extraction and could result in 'No emails found' being displayed even when emails exist.; [minor] The weekend date calculation uses $now.minus({ days: 2 }) which assumes the workflow runs exactly on Monday. If it runs late or is manually triggered, the date range won't accurately represent the weekend (Saturday-Sunday). Should calculate based on the previous Saturday and Sunday specifically.; [minor] The emailCount in Format Report uses $('Fetch Weekend Emails').all().length which counts all items, but if Gmail returns no emails, this could cause issues. No null/empty check before accessing length property.
- [minor] The Gmail 'Fetch Weekend Emails' node uses 'returnAll: true' without batching. Best practices for data extraction recommend using Loop Over Items (splitInBatches) when processing large amounts of data to avoid memory issues and timeouts. While the user requested a simple workflow, weekend emails could potentially be a large dataset (100+ items), and the documentation specifically warns that 'Processing too many items or large files at once can crash your instance.'; [minor] The workflow uses dynamic date calculations ({{ $now.minus({ days: 2 }) }}) which assumes the workflow runs exactly on Monday morning. If the workflow is delayed or runs at a different time, it may not capture the full weekend period (Saturday-Sunday). A more robust approach would calculate the previous Saturday and Sunday dates explicitly to ensure consistent weekend coverage regardless of execution timing.

**Programmatic Violations:**
- None

### example-002-00a012f0
**Status:** pass | **Score:** 93.3%

**LLM-Judge Violations:**
- [major] Invalid credentials reference syntax in 'Fetch News from NewsAPI' node: `={{ $credentials.newsApi.apiKey }}`. The $credentials object is not accessible in expressions; credentials should be configured through the node's credentials panel, not referenced in parameter expressions.; [minor] Inefficient string concatenation in 'Summarize Article with OpenAI' node. While `={{ "Title: " + $json.title + "
- 
- Description: " + $json.description + "
- 
- Please provide a concise summary." }}` works, template literals would be cleaner: `={{ `Title: ${$json.title}
- Please provide a concise summary.` }}`; [minor] Inefficient string concatenation in 'Generate Image with DALL-E' node. While `={{ "Create a professional news illustration for this headline: " + $json.title }}` works, template literals would be more readable: `={{ `Create a professional news illustration for this headline: ${$json.title}` }}`
- [minor] HTTP Request node uses expression for apiKey in query parameters instead of proper authentication header. While functional, best practice is to use authentication options or header-based API key.
- [critical] Data loss in OpenAI summarization node - the summary response is not properly merged with the original article data (title, url, source, etc.). The OpenAI node outputs only the summary text, losing all other article fields needed for downstream processing and final output.; [major] Image generation node receives incomplete data context - only has the summary from previous node, but the original article title used in the prompt may not be accessible if not properly passed through from the OpenAI node output.
- [minor] Missing rate limiting protection for OpenAI API calls. Best practices documentation states: 'Batch requests and introduce delays to avoid hitting API rate limits' and 'When 429 rate limiting errors occur, implement batching or use Retry on Fail feature.' The workflow processes 5 stories sequentially with OpenAI calls (summarization + image generation) without any Wait nodes or retry configuration, which could lead to rate limiting issues during nightly execution.; [minor] Missing error handling for API failures. While not explicitly requested by the user, the workflow lacks any conditional logic or retry mechanisms to handle potential failures from NewsAPI, OpenAI summarization, or DALL-E image generation. Given this is a nightly automated workflow, a single API failure would stop the entire process. Best practices recommend using IF nodes for error handling and 'Retry on Fail' features for API calls.; [minor] No data persistence or output mechanism. The workflow generates summaries and images but doesn't store or send them anywhere (no Google Sheets, Data Tables, email, or other output node). While the user didn't explicitly request storage, a nightly workflow that generates content typically needs to save or deliver results. Best practices for content generation mention using appropriate output nodes for generated content.

**Programmatic Violations:**
- None

### example-003-6d770bf0
**Status:** pass | **Score:** 93.2%

**LLM-Judge Violations:**
- [major] Missing error handling for weather API response structure - the workflow assumes weather[0] exists without validation, which could cause failures if the API returns an empty weather array or unexpected structure; [minor] No data validation or type checking for numeric values (temp, humidity, wind speed) before string interpolation - could result in 'undefined' appearing in the email if API response structure changes
- [minor] Using HTTP Request node for OpenWeather API instead of a dedicated weather service node. While HTTP Request works correctly, best practices state 'Always prefer built-in n8n nodes over HTTP Request nodes when a dedicated node exists for the service.' However, since n8n doesn't have a dedicated OpenWeather node, this is a very minor point and the implementation is appropriate.

**Programmatic Violations:**
- None

### example-004-9c543c85
**Status:** pass | **Score:** 75.9%

**LLM-Judge Violations:**
- [critical] Critical workflow logic error: The splitInBatches node creates a loop back to itself from the validation step, but processed invoices (both valid and invalid) never loop back to continue processing remaining batches. The loop connection goes to 'Process Each Invoice' but there's no connection from 'Format Valid Invoice' or 'Log Invalid Invoice' back to the splitInBatches node to continue the loop. This breaks the batch processing mechanism.; [minor] The workflow uses Google Gemini for document analysis, but the user specifically requested extraction from 'PDF/image invoices'. While Gemini can handle documents, there's no explicit file download or binary data handling step between 'Fetch Invoice Files' (which returns JSON) and 'Extract Invoice Data' (which expects binary data in 'data' property). The workflow assumes the HTTP request returns binary invoice files directly, but it's configured to expect JSON response format.
- [critical] Validate Invoice Data (true branch) connects directly to Process Each Invoice, bypassing Format Valid Invoice node. This breaks the intended data flow where valid invoices should be formatted before looping back.; [critical] Log Invalid Invoice node has no output connection back to Process Each Invoice. In a Split In Batches loop, all branches must reconnect to complete the cycle. Invalid invoices will cause the loop to hang indefinitely.; [major] Split In Batches loop architecture is incomplete. Both Format Valid Invoice and Log Invalid Invoice should connect back to Process Each Invoice to properly complete the loop, but only the direct bypass connection exists from the IF node.
- [minor] In 'Send/Save Report' node: Using `={{ $json }}` to send entire JSON object is valid but could be more explicit with `={{ $input.item.json }}` for clarity
- [major] Extract Invoice Data node (Google Gemini) has incorrect configuration: 'resource' and 'operation' parameters are not valid for @n8n/n8n-nodes-langchain.googleGemini node type. This node type is for LangChain integration and doesn't have 'document' resource or 'analyze' operation. The correct approach would be using a vision-capable model with proper LangChain document processing.
- [major] The 'Log Invalid Invoice' node only adds status fields but doesn't feed back to the loop properly - invalid invoices are not being aggregated with valid ones for the final report; [minor] The 'Format Valid Invoice' Set node adds minimal value (just status and timestamp) when this could be done in the Parse Extracted Data node or validation node; [minor] The Parse Extracted Data Code node handles error cases but these aren't properly routed back to the aggregation - creating a path inefficiency
- [critical] Critical data flow architecture flaw: The workflow has a circular loop where validated invoices flow back to 'Process Each Invoice' node, but processed invoices never reach the 'Aggregate All Invoices' node. The aggregate node only receives the initial trigger from splitInBatches output 1, not the processed invoice data from the validation branch.; [critical] Complete data loss in aggregation: The 'Aggregate All Invoices' node receives empty/initial data from the splitInBatches first output before any invoices are processed. All processed invoice data (valid and invalid) flows through the loop but never reaches the aggregation node, resulting in an empty or incomplete report.; [major] Missing data flow for invalid invoices: Invalid invoices from 'Log Invalid Invoice' node have no connection back to the aggregation process. These invoices are lost and won't appear in the weekly report's invalidInvoiceDetails section.; [major] Binary data handling issue: The 'Extract Invoice Data' node expects binary data in the 'data' property, but there's no transformation in 'Fetch Invoice Files' or 'Process Each Invoice' to ensure invoice files are properly loaded as binary data with the correct property name.; [major] Data structure mismatch in report generation: The 'Generate Weekly Report' code expects an array of processed invoices with 'status', 'validationError', and other fields, but due to the broken flow, it will receive incomplete or incorrectly structured data.
- [minor] Two Set nodes use generic naming pattern ('Format Valid Invoice' and 'Log Invalid Invoice') when they could be more descriptive about their specific purpose in the workflow context; [minor] The 'Generate Weekly Report' Code node contains extensive business logic (statistics calculation, grouping, formatting) that could be broken into smaller, more modular components for better reusability
- [major] Missing file type detection before Extract from File node. Best practices explicitly state: 'ALWAYS check the file type before using Extract from File node' and 'Use IF node to check file extension or MIME type first'. The workflow processes invoices from both PDF and image formats (as stated in user prompt), but goes directly to Google Gemini for extraction without checking file type first. This violates the critical requirement to branch based on document characteristics and could result in processing failures.; [major] Using Google Gemini node incorrectly for document extraction. The node is configured with resource='document' and operation='analyze', but Google Gemini Chat Model is documented for 'Image analysis and generation, video generation from text prompts using nano banana, multimodal content creation' - not specifically for invoice data extraction. Best practices recommend using 'Document Loader node (set Data Source to Binary)' connected to 'AI Agent or LLM Chain for processing' for binary file extraction, or using specialized nodes like 'Mindee for invoice and receipt parsing' or 'AWS Textract for advanced OCR with table and form detection'.; [minor] Missing OCR fallback strategy. Best practices state: 'Always implement fallback for extraction failures: Check if text extraction returns empty, If empty, automatically route to OCR'. The workflow processes both PDF and image invoices but has no fallback mechanism if the AI extraction fails or returns empty results, which is particularly important for scanned documents or images.; [minor] Suboptimal loop architecture with Split In Batches. The workflow connects 'Validate Invoice Data' back to 'Process Each Invoice' creating a loop, but best practices recommend using Split In Batches with proper output handling: 'Output 0 done: Executes after all batches are processed - use for final aggregation or notifications' and 'Output 1 loop: Connect processing nodes here'. The current implementation has the aggregation happening on output 0 (correct) but the loop connection back is non-standard and could cause issues with batch completion detection.

**Programmatic Violations:**
- None

### example-005-05fd23ad
**Status:** pass | **Score:** 81.1%

**LLM-Judge Violations:**
- [major] The AI qualification logic uses string parsing ('contains "qualified": true') to extract structured data from the AI agent's output, which is unreliable. The agent returns free-form text that may not consistently format JSON properly, making the qualification check fragile and prone to failure.
- [critical] Schedule Calendar Appointment node uses outdated $node syntax: $node["Form Submission Webhook"].json.name and $node["Qualify Lead with AI"].json.output instead of modern $('Node Name').item.json syntax; [critical] Format Rejection Response node uses outdated $node syntax: $node["Qualify Lead with AI"].json.output instead of $('Qualify Lead with AI').item.json.output; [minor] Qualify Lead with AI node uses string concatenation with JSON.stringify instead of cleaner template literal approach, though this is functionally correct
- [minor] Webhook path uses placeholder format instead of a concrete value like 'lead-form' or 'webhook-lead-qualification', though this is acceptable if user didn't specify; [minor] Google Calendar 'calendar' parameter uses placeholder format instead of actual calendar ID, though this is acceptable if user didn't specify the calendar
- [minor] The AI agent output parsing could be more robust - currently using string contains check for '"qualified": true' which is fragile. A Code node to parse JSON would be more reliable.; [minor] Two separate Set nodes for success/rejection responses could potentially be consolidated into a single Switch/Set combination, though the current approach is acceptable for clarity.
- [critical] AI Agent output parsing issue: The IF node checks if the raw output contains '"qualified": true' as a string, but AI agent output is typically in $json.output as text, not parsed JSON. This will cause incorrect routing since the JSON structure won't be accessible for downstream nodes.; [major] Missing JSON parsing step: The AI agent returns text output that needs to be parsed into JSON before the qualification decision can be properly evaluated. The workflow attempts to use JSON properties (like score, suggestedMeetingDuration) that aren't extracted from the text response.; [major] Calendar appointment duration hardcoded: The workflow requests 'suggestedMeetingDuration' from AI but then hardcodes 30 minutes in the calendar event (.plus(30, 'minutes')), losing the AI's recommendation.; [major] Data reference error in rejection response: The 'Format Rejection Response' node references $node['Qualify Lead with AI'].json.output which will contain the full unparsed text response, not just the reason field, making the response unclear.; [minor] Inefficient data access pattern: Multiple nodes reference $node['Form Submission Webhook'].json instead of using $json from the current execution context, which could cause issues if the workflow structure changes.
- [major] Missing raw form data storage: The workflow does not store the raw form submission data to any persistent storage (Google Sheets, Airtable, Data Tables, or database). According to form_input best practices, 'ALWAYS store raw form responses to a persistent data storage destination even if the primary purpose of the workflow is to trigger another action.' This is critical for administration and monitoring of form responses. The workflow only processes the data through AI and calendar scheduling without persisting the original submission.; [minor] Webhook response handling could be improved: The workflow uses 'responseMode: lastNode' but has two different ending paths (Format Success Response and Format Rejection Response) that don't merge back together. This means only one path will properly return a response to the form submitter. Best practice would be to merge both response paths or ensure both branches properly handle the webhook response.; [minor] AI Agent output parsing is fragile: The workflow uses string matching ('contains "qualified": true') to parse AI output in the IF node. This is brittle and could fail if the AI formats the JSON differently (e.g., with extra whitespace or different quote styles). Best practice for AI-powered triage is to use structured output format and parse it properly, or use the Text Classifier node for classification tasks.; [minor] Missing n8n attribution setting: According to form_input best practices, 'n8n forms attach the attribution "n8n workflow" to messages by default - you must disable this setting which will often be called "Append n8n Attribution"'. The webhook node does not show this setting disabled, though it may be using default values.

**Programmatic Violations:**
- [major] Agent node "Qualify Lead with AI" has no expression in its prompt field. This likely means it failed to use chatInput or dynamic context

### example-006-7f622f05
**Status:** pass | **Score:** 86.0%

**LLM-Judge Violations:**
- [minor] In 'Format Summary' node: The stringValue contains placeholder text instead of actual field references. While the syntax structure '=ðŸ“Š New Database Entry...' with = prefix is correct, the placeholder '<__PLACEHOLDER_VALUE__...>' should be replaced with actual expressions like {{ $json.field_name }}
- [minor] Query New Entries node: SQL query parameter contains a placeholder value. While the placeholder provides helpful guidance, a more concrete example query would be better for immediate usability.; [minor] Format Summary node: The summary stringValue contains a placeholder for field formatting. While this is acceptable given no specific fields were mentioned by the user, a more complete example would improve usability.
- [major] IF node condition uses '={{ $json }}' with 'notEmpty' operator which checks if the entire JSON object is not empty. This will pass even if the database query returns zero rows but still has metadata. Should check for array length or specific field existence instead.; [major] Format Summary node uses 'include: all' which will pass through all original database fields along with the new 'summary' field. This creates data redundancy and the Slack node only uses the summary field, making the extra data unnecessary.; [minor] No handling for multiple database entries - the Format Summary node will process each item individually, potentially sending multiple Slack messages. Depending on intent, this might need aggregation to send a single summary of all new entries.; [minor] Placeholder values in Format Summary node mean the actual field mappings cannot be validated. The summary string template is incomplete and may not correctly reference database fields when implemented.
- [minor] Missing batch processing for potentially large datasets. Best practices recommend using Split In Batches node when handling 100+ items to prevent timeouts and memory issues. The workflow queries a database every 5 minutes without batching, which could cause performance problems if many new entries are returned.; [minor] Inefficient condition checking pattern. The IF node checks '{{ $json }}' for notEmpty, but this evaluates per item. Best practices recommend checking items.length > 0 to prevent empty notifications. The current pattern will process each item through the IF node individually rather than checking if any items exist before proceeding.; [minor] Missing post-notification logging/tracking. Best practices recommend adding logging nodes to track sent notifications for audit trails and duplicate prevention. This is particularly important for database monitoring workflows to avoid sending duplicate alerts for the same entries.

**Programmatic Violations:**
- None

### example-007-ca13700c
**Status:** pass | **Score:** 88.9%

**LLM-Judge Violations:**
- [major] The GitHub 'Update Issue with Labels & Assignee' node references incorrect data paths. It uses '={{ $json.issue.number }}' but the data from the Information Extractor node outputs extracted fields (labels, assignee, priority) without preserving the original issue data. The issue number from the trigger is lost in the data flow.
- [major] In 'Update Issue with Labels & Assignee' node: Expression `={{ $json.issue.number }}` references field from previous node ('On New GitHub Issue') but the immediate input is from 'Classify Issue with AI' which outputs extracted data (labels, assignee, priority), not the original issue object. Should use `={{ $('On New GitHub Issue').item.json.issue.number }}` to reference the trigger node.
- [major] GitHub node 'Update Issue with Labels & Assignee' is missing required 'owner' and 'repository' parameters for the edit operation. These are necessary to identify which repository the issue belongs to.
- [critical] Data type mismatch in labels field: AI extractor returns comma-separated string but GitHub API expects an array of label strings. The expression '={{ $json.labels }}' will pass a string like 'bug, feature' instead of ['bug', 'feature'], causing API failure.; [major] Missing repository owner and name parameters in Update Issue node. The GitHub edit operation requires 'owner' and 'repository' parameters which are not being passed from the trigger data (available as $json.repository.owner.login and $json.repository.name).; [minor] Assignee field handling is fragile: if AI returns empty/null assignee, the expression '={{ $json.assignee }}' will pass empty string to GitHub API which may cause validation issues. Should include conditional logic or data validation.
- [minor] Missing default/fallback path for unclassified issues. The workflow doesn't handle cases where AI classification might fail or return unexpected data. Best practice states: 'CRITICAL: Always include a default/fallback path to catch unclassified items. Never allow data to drop silently.' However, since the user requested a basic automation without mentioning error handling, this is a minor rather than major violation.; [minor] No validation or error handling between AI classification and GitHub update. If the AI returns malformed data (e.g., invalid assignee username or label names), the GitHub update could fail silently. Best practice recommends implementing error handling for unexpected AI outputs, though this wasn't explicitly requested by the user.; [minor] Temperature setting of 0.3 is slightly higher than recommended for classification consistency. Best practice states: 'Set low temperature of the model (0-0.2) for consistency' in classification tasks. While 0.3 is relatively low, it's above the recommended range for triage workflows.

**Programmatic Violations:**
- None

### example-008-65ba9714
**Status:** pass | **Score:** 93.0%

**LLM-Judge Violations:**
- [minor] Split In Batches node 'batchSize' parameter contains a placeholder string instead of a numeric value. While this is acceptable when user didn't specify a value, the parameter expects a number type.
- [major] CSV parsing logic is fragile and will fail on CSV files with commas inside quoted fields (e.g., 'John, Doe' or 'Company, Inc.'). The simple split(',') approach doesn't handle standard CSV escaping rules.; [major] Data transformation applies toUpperCase() to all values indiscriminately, which will corrupt numeric data, dates, and other non-text fields. Numbers like '123.45' become '123.45' (string), dates become uppercase strings, breaking downstream processing.; [minor] No data validation or error handling in the Code node. Empty or malformed CSV rows could cause silent data loss or incorrect field mapping when values.length doesn't match headers.length.
- [major] Using Wait node for rate limiting instead of Split In Batches delay mechanism. The Wait node (n8n-nodes-base.wait) with 'timeInterval' resume mode is designed for pausing workflow execution until a specific time, not for implementing delays between batch operations. This creates an inefficient loop pattern and doesn't properly implement rate limiting as documented in best practices. The Split In Batches node should handle the rate limiting through its built-in batch processing capabilities, potentially with a simple delay between batches using the Wait node's webhook mode or by adjusting batch size and timing.; [minor] Manual CSV parsing in Code node instead of using Extract From File node. The workflow uses custom JavaScript to parse CSV data (splitting by newlines and commas), which is error-prone and doesn't handle edge cases like quoted fields with commas, escaped characters, or different line endings. Best practices for data_extraction explicitly recommend using Extract From File (n8n-nodes-base.extractFromFile) with the 'Extract from CSV' operation to convert binary CSV data to JSON properly. This would be more reliable and require less custom code.

**Programmatic Violations:**
- None

### example-009-7b389a23
**Status:** pass | **Score:** 90.7%

**LLM-Judge Violations:**
- [minor] In 'Format Email Content' node, string concatenation uses outdated syntax with + operator instead of template literals or modern n8n string interpolation (e.g., '="Order Confirmation - Order #" + $json.orderId' could be '="Order Confirmation - Order #{{ $json.orderId }}"'); [minor] In 'Format Email Content' node, complex HTML string concatenation is inefficient and hard to read. While functional, using template literals with embedded expressions would be cleaner (e.g., the emailBody field concatenates multiple strings with + operator); [minor] In 'Update Inventory' node, the jsonBody uses double curly braces within the expression '={{ { "orderId": ... } }}' which is redundant but functional. The outer ={{ }} is for n8n expression, inner { } is for JSON object - this works but could be cleaner
- [minor] Webhook node uses placeholder value for 'path' parameter - while valid as a placeholder, a more concrete default like 'ecommerce-order' would be more helpful; [minor] HTTP Request node 'Update Inventory' uses placeholder for 'url' parameter - expected since user didn't provide specific endpoint, but required for actual execution; [minor] Email Send node uses placeholder for 'fromEmail' parameter - expected since user didn't provide sender email, but required for actual execution
- [major] Two separate Set nodes ('Extract Order Data' and 'Format Email Content') could potentially be consolidated or one could be eliminated by using expressions directly in downstream nodes; [minor] Email formatting could be done directly in the Send Email node parameters instead of requiring a separate Set node, reducing node count by 1; [minor] No error handling for inventory update failure - workflow will fail silently if inventory API is down, requiring additional nodes for production reliability
- [critical] Data loss after Update Inventory node - the HTTP Request node returns inventory API response data, which overwrites the original order data (orderId, customerEmail, customerName, totalAmount). The Format Email Content node tries to access $json.customerName, $json.customerEmail, and $json.totalAmount which no longer exist in the data stream.; [major] Missing data validation - no validation that required fields (orderId, customerEmail, items) exist before processing. If webhook receives incomplete data, downstream nodes will fail or send incomplete emails.; [minor] Type conversion issue - totalAmount is extracted as numberValue but concatenated as string in email body without proper formatting (e.g., no decimal places, currency formatting).
- [major] Missing error handling for inventory update failure: The workflow sends confirmation emails even if the inventory update fails. Best practice for notification workflows states 'Consider using error handling paths with Continue on Fail settings for redundancy.' For an e-commerce order processing workflow, sending confirmation when inventory wasn't updated could lead to overselling and customer service issues. The workflow should validate the inventory update succeeded before proceeding to email confirmation.

**Programmatic Violations:**
- None

### example-010-92237768
**Status:** pass | **Score:** 82.0%

**LLM-Judge Violations:**
- [major] Workflow uses manualTrigger instead of an appropriate trigger for user-submitted content. The user requested a workflow that 'reviews user-submitted content', which implies content should be received automatically (e.g., via webhook, form submission, or API endpoint), not manually triggered for testing purposes.
- [major] Content Moderation AI node: String concatenation using + operator instead of proper template literal syntax. Should use `={{ "...Content to review: " + $json.content }}` or better yet `="...Content to review: {{ $json.content }}"`
- [minor] HTTP Request node 'Notify Moderators' uses a placeholder URL with descriptive text format. While this is acceptable when user didn't provide a specific webhook URL, a cleaner placeholder format like '<UNKNOWN>' or 'https://example.com/webhook' would be more standard.
- [critical] The IF node checks for '"flagged": true' as a string within the AI output, but the AI agent returns unstructured text output, not guaranteed JSON. This string matching approach is fragile and will likely fail if the AI formats the response differently (e.g., 'flagged: true', 'flagged":true', or with different spacing).; [major] No JSON parsing of the AI output before the IF node. The workflow treats the AI response as raw text and attempts string matching instead of properly parsing the JSON structure that was requested in the prompt. This means downstream nodes cannot access structured fields like 'severity' or 'categories'.; [minor] The 'Format Moderator Alert' and 'Log Approved Content' nodes reference '$json.content' which assumes the original content field is still available after the AI agent node, but the AI agent output structure may have transformed the data. This could result in missing content data.; [minor] The moderation_result field in both Set nodes stores the raw AI output text, which is inefficient. The workflow should parse and extract specific fields (flagged, reason, severity, categories) for better data structure and usability.
- [major] Missing data persistence for moderation results. Best practices for triage workflows state 'Always include logging nodes to track outcomes for monitoring and analysis.' The workflow lacks any storage node (Data Table, Google Sheets, Airtable, etc.) to persist moderation decisions, flagged content, or approved content. This prevents administrators from reviewing moderation history, analyzing patterns, or auditing decisions - critical for a content moderation system.; [minor] Using Manual Trigger instead of appropriate event-based trigger. Best practices for notification workflows recommend 'Choose between event-based triggers (webhooks, form submissions, CRM events) for immediate notifications.' For a content moderation workflow that reviews 'user-submitted content,' a Webhook trigger or Form Trigger would be more appropriate than a Manual Trigger, which requires manual execution rather than automatic processing of submissions.

**Programmatic Violations:**
- None

