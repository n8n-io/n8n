{
	"$schema": "http://json-schema.org/draft-07/schema#",
	"$id": "openrouter_chat_completion_request_schema",
	"title": "OpenRouter Chat Completion Request",
	"type": "object",
	"properties": {
		"response_format": {
			"displayname": "Response Format",
			"type": "object",
			"properties": {
				"type": {
					"type": "string",
					"enum": ["json_object", "text"]
				}
			},
			"required": ["type"],
			"description": "Allows to force the model to produce specific output format."
		},
		"stop": {
			"displayname": "Stop Sequences",
			"anyOf": [{ "type": "string" }, { "type": "array", "items": { "type": "string" } }],
			"description": "Up to 4 sequences where the API will stop generating further tokens."
		},
		"stream": {
			"displayname": "Stream Response",
			"type": "boolean",
			"description": "If true, partial message deltas will be sent, like in ChatGPT."
		},
		"max_tokens": {
			"displayname": "Maximum Tokens",
			"type": "number",
			"minimum": 1,
			"description": "The maximum number of tokens to generate."
		},
		"temperature": {
			"displayname": "Temperature",
			"type": "number",
			"minimum": 0,
			"maximum": 2,
			"description": "What sampling temperature to use, between 0 and 2."
		},
		"seed": {
			"displayname": "Seed",
			"type": "integer",
			"description": "If specified, the model will make a deterministic generation."
		},
		"top_p": {
			"displayname": "Top P",
			"type": "number",
			"exclusiveMinimum": 0,
			"maximum": 1,
			"description": "An alternative to sampling with temperature, called nucleus sampling."
		},
		"top_k": {
			"displayname": "Top K",
			"type": "number",
			"minimum": 1,
			"description": "The number of highest probability vocabulary tokens to keep for top-k filtering. Not available for OpenAI models."
		},
		"frequency_penalty": {
			"displayname": "Frequency Penalty",
			"type": "number",
			"minimum": -2,
			"maximum": 2,
			"description": "Penalize new tokens based on their existing frequency in the text."
		},
		"presence_penalty": {
			"displayname": "Presence Penalty",
			"type": "number",
			"minimum": -2,
			"maximum": 2,
			"description": "Penalize new tokens based on whether they appear in the text so far."
		},
		"repetition_penalty": {
			"displayname": "Repetition Penalty",
			"type": "number",
			"exclusiveMinimum": 0,
			"maximum": 2,
			"description": "Similar to frequency/presence penalty, but applied to all tokens."
		},
		"logit_bias": {
			"displayname": "Logit Bias",
			"type": "object",
			"patternProperties": {
				"^[0-9]+$": {
					"type": "number"
				}
			},
			"description": "Modify the likelihood of specified tokens appearing in the completion."
		},
		"top_logprobs": {
			"displayname": "Top Log Probs",
			"type": "integer",
			"description": "An integer that specifies how many logprobs to include in the output."
		},
		"min_p": {
			"displayname": "Min P",
			"type": "number",
			"minimum": 0,
			"maximum": 1,
			"description": "Minimum probability to keep for filtering."
		},
		"top_a": {
			"displayname": "Top A",
			"type": "number",
			"minimum": 0,
			"maximum": 1,
			"description": "Top-A sampling parameter."
		},
		"transforms": {
			"displayname": "Transforms",
			"type": "array",
			"items": {
				"type": "string"
			},
			"description": "OpenRouter-only parameter for Prompt Transforms."
		},
		"models": {
			"displayname": "Models",
			"type": "array",
			"items": {
				"type": "string"
			},
			"description": "List of models to route to."
		},
		"route": {
			"displayname": "Route Strategy",
			"type": "string",
			"enum": ["fallback"],
			"description": "Routing strategy."
		},
		"user": {
			"displayname": "User Identifier",
			"type": "string",
			"description": "A stable identifier for your end-users. Used to help detect and prevent abuse."
		}
	}
}
