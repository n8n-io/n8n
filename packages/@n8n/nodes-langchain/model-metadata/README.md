# Model Metadata Files

This directory contains metadata for AI models used in n8n's LangChain nodes. The metadata is displayed in the "Browse Models" mode, providing users with detailed information about each model's capabilities, pricing, and recommended use cases.

## Directory Structure

```
model-metadata/
â”œâ”€â”€ README.md
â”œâ”€â”€ openai/          # OpenAI and OpenAI-compatible models
â”‚   â”œâ”€â”€ gpt-4o.json
â”‚   â”œâ”€â”€ gpt-4o-mini.json
â”‚   â”œâ”€â”€ o1.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ anthropic/       # Anthropic Claude models
â”‚   â”œâ”€â”€ claude-3-5-sonnet-20241022.json
â”‚   â”œâ”€â”€ claude-3-opus-20240229.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ google/          # Google Gemini models
â”‚   â”œâ”€â”€ gemini-2.5-flash.json
â”‚   â”œâ”€â”€ gemini-2.5-pro.json
â”‚   â”œâ”€â”€ _aliases.json
â”‚   â””â”€â”€ ...
â””â”€â”€ mistral/         # Mistral AI models
    â”œâ”€â”€ mistral-large-latest.json
    â”œâ”€â”€ mistral-small-latest.json
    â”œâ”€â”€ _aliases.json
    â””â”€â”€ ...
```

## JSON Schema

Each model metadata file must follow this structure:

```json
{
  "id": "model-identifier",
  "name": "Display Name",
  "shortName": "Short",
  "provider": "Provider Name",
  "pricing": {
    "promptPerMilTokenUsd": 2.5,
    "completionPerMilTokenUsd": 10.0
  },
  "contextLength": 128000,
  "maxOutputTokens": 16384,
  "capabilities": {
    "functionCalling": true,
    "structuredOutput": true,
    "extendedThinking": false
  },
  "inputModalities": ["text", "image"],
  "outputModalities": ["text"],
  "intelligenceLevel": "high"
}
```

## Field Definitions

| Field | Type | Required | Description | Example Values |
|-------|------|----------|-------------|----------------|
| `id` | string | âœ… | Model identifier (must match API) | `"gpt-4o"`, `"claude-3-5-sonnet-20241022"` |
| `name` | string | âœ… | Human-readable display name | `"GPT-4o"`, `"Claude 3.5 Sonnet"` |
| `shortName` | string | âŒ | Abbreviated name for compact display | `"GPT-4o"`, `"Claude 3.5S"` |
| `provider` | string | âœ… | Provider/company name | `"OpenAI"`, `"Anthropic"`, `"Google"` |
| `pricing.promptPerMilTokenUsd` | number | âœ… | Input price per 1M tokens (USD) | `2.5`, `0.15` |
| `pricing.completionPerMilTokenUsd` | number | âœ… | Output price per 1M tokens (USD) | `10.0`, `0.6` |
| `contextLength` | number | âœ… | Maximum context window in tokens | `128000`, `200000` |
| `maxOutputTokens` | number | âŒ | Maximum output tokens | `16384`, `4096` |
| `capabilities.functionCalling` | boolean | âŒ | Supports tool/function calling | `true`, `false` |
| `capabilities.structuredOutput` | boolean | âŒ | Supports JSON schema output | `true`, `false` |
| `capabilities.extendedThinking` | boolean | âŒ | Extended reasoning (o1-style) | `true`, `false` |
| `inputModalities` | enum[] | âŒ | Accepted input types | `["text", "image", "audio", "video"]` |
| `outputModalities` | enum[] | âŒ | Generated output types | `["text"]`, `["text", "image"]` |
| `intelligenceLevel` | enum | âœ… | Capability level | `"low"`, `"medium"`, `"high"` |
| `recommendedFor` | string[] | âŒ | Use case recommendations | `["coding", "analysis", "multimodal"]` |

## Modalities

Modalities define what types of input the model accepts and what it can output. Visual capabilities (vision, image generation, audio, video) are automatically derived from these fields in the UI.

### Valid Modality Values

- **`"text"`** - Text input/output (all models support this)
- **`"image"`** - Image input (vision) or output (generation)
- **`"audio"`** - Audio input (transcription/understanding) or output (TTS)
- **`"video"`** - Video input (understanding)
- **`"file"`** - File/document input (PDF, etc.)

### Examples

```json
// Vision model (image input)
"inputModalities": ["text", "image"],
"outputModalities": ["text"]

// Image generation model (image output)
"inputModalities": ["text"],
"outputModalities": ["text", "image"]

// Multimodal model (audio + vision)
"inputModalities": ["text", "image", "audio", "video"],
"outputModalities": ["text", "audio"]
```

**UI Display**: The frontend automatically shows capability icons based on modalities:
- ðŸ‘ï¸ Vision = `"image"` in `inputModalities`
- ðŸŽ¨ Image Generation = `"image"` in `outputModalities`
- ðŸŽ¤ Audio = `"audio"` in input or output modalities
- ðŸ“¹ Video = `"video"` in `inputModalities`
- ðŸ“„ File Support = `"file"` in `inputModalities`

## Intelligence Levels

Use these standardized values for `intelligenceLevel`:

- **`"low"`** - Basic models, older generations, fast inference
  - Examples: GPT-3.5 Turbo, older Claude 2 models
  - Use when: Speed and cost are more important than capability

- **`"medium"`** - Capable models, good for most tasks
  - Examples: GPT-4o Mini, Claude 3 Haiku, Claude 3.5 Haiku
  - Use when: Balanced performance and cost for everyday tasks

- **`"high"`** - Advanced models, best reasoning and complex tasks
  - Examples: GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, o1
  - Use when: Maximum accuracy and capability required

## Adding New Models

### 1. Create the JSON file

```bash
# For OpenAI models
touch model-metadata/openai/your-model-id.json

# For Anthropic models
touch model-metadata/anthropic/your-model-id.json
```

### 2. Fill in the metadata

Copy the schema from above and fill in all required fields. You can copy an existing file as a template:

```bash
cp model-metadata/openai/gpt-4o.json model-metadata/openai/new-model.json
```

### 3. Validate the metadata

```bash
cd packages/@n8n/nodes-langchain
pnpm validate-metadata
```

This will check for:
- All required fields present
- Valid `intelligenceLevel` enum value
- Positive pricing values
- Valid JSON syntax
- ID matches filename

### 4. Build the package

```bash
pnpm build
```

The metadata files will be copied to `dist/model-metadata/` automatically.

### 5. Test in the UI

1. Start n8n in development mode: `pnpm dev`
2. Add a Chat Model node (OpenAI, Anthropic, Google Gemini, or Mistral)
3. Click the model selector dropdown
4. Search for your model
5. Verify the metadata displays correctly (pricing, capabilities, context length)

## Development Workflow

### During Development

When you use a model that doesn't have a metadata file:
- The model will appear in the dropdown without metadata (name and value only)
- No metadata will be displayed in the UI (capabilities, pricing, etc.)
- To add metadata: Create a JSON file for the model following the schema above

### Validation on Build

The `validate-metadata` script runs automatically during:
- `pnpm build` (via `copy-model-metadata`)
- `pnpm test` (via `pretest` hook)

Validation errors will fail the build and must be fixed before proceeding.

## Architecture

### Loading Flow

```
1. Build Time
   â””â”€> validate-metadata checks all JSON files
   â””â”€> copy-model-metadata copies to dist/

2. Runtime (Backend)
   Node listSearch methods:
   â””â”€> Call provider API to get available models
   â””â”€> For each model, load metadata via loadModelMetadata()
       â”œâ”€ Check in-memory cache first
       â”œâ”€ Load from model-metadata/{provider}/{modelId}.json
       â”œâ”€ Support alias resolution via _aliases.json
       â””â”€ Return undefined if not found
   â””â”€> Return results with complete metadata to frontend

3. Frontend
   â””â”€> Receives model list with metadata included
   â””â”€> Displays metadata in ResourceLocator dropdown
   â””â”€> No additional API calls needed
```

### Performance

- **In-memory cache**: Metadata cached after first load
- **Node.js caching**: File reads cached by Node.js
- **Load time**: ~1ms first load, <0.01ms cached
- **Memory footprint**: ~100KB for all 176 models
- **Network efficiency**: Single API call (no metadata waterfall)

### Alias Resolution

The `_aliases.json` file maps model aliases to canonical IDs:

```json
{
  "chatgpt-4o-latest": "gpt-4o",
  "gpt-4-turbo-preview": "gpt-4-turbo"
}
```

This allows users to use common aliases while maintaining a single metadata file.

## Maintenance

### Updating Model Prices

When model prices change:

1. Update the JSON file: `model-metadata/{provider}/{model-id}.json`
2. Commit the change
3. Rebuild: `pnpm build`

### Adding New Providers

To add a new provider (e.g., Google):

1. Create directory: `mkdir model-metadata/google`
2. Add model files: `touch model-metadata/google/gemini-pro.json`
3. Update validation script if needed (usually not required)
4. Build and test

## Testing

### Manual Testing

```bash
# Validate all metadata
pnpm validate-metadata

# Build and copy
pnpm build

# Test in development
pnpm dev
# Open a model node in the UI and verify metadata displays in the dropdown
```

### Unit Tests

Model metadata loading is tested as part of:
- Node integration tests
- listSearch method tests

## Contributing

When adding new model metadata:

1. Use the exact model ID from the provider's API
2. Fill all required fields (id, name, provider, pricing, contextLength, intelligenceLevel, capabilities)
3. Validate before committing: `pnpm validate-metadata`
4. Test in the UI model selection dropdown
